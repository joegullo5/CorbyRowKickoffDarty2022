{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joegullo5/CorbyRowKickoffDarty2022/blob/main/MLALG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "VT5JVjwPNYjw"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import tensorflow as tf\n",
        "from sklearn import model_selection\n",
        "import torch\n",
        "\n",
        "#data filepath\n",
        "DATA_PATH = \"/content/drive/MyDrive/MLPROJECT/notpie.csv\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biY73KyxOKmL",
        "outputId": "7ed2f3ee-b367-44e3-a3e2-b669f78ec846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "nPO3tkg1NYj0"
      },
      "outputs": [],
      "source": [
        "def load_data(data_path):\n",
        "    \"\"\"Loads data dataset from csv file.\n",
        "\n",
        "        :param data_path (str): Path to json file containing data\n",
        "        :return X (ndarray): Inputs\n",
        "        :return y (ndarray): Targets\n",
        "    \"\"\"\n",
        "    #data filepath\n",
        "    filepath = data_path\n",
        "\n",
        "    #import csv using pandas to format into dataframe\n",
        "    with open(filepath, 'r') as f:\n",
        "        data = pandas.read_csv(f, header=None)\n",
        "\n",
        "    #print(data.head())\n",
        "    #randomize te rows of the dataframe\n",
        "    data = normalize_data(data)\n",
        "    data = data.sample(frac=1)\n",
        "    #load all columns except the last one into x\n",
        "    x = np.array(data.iloc[:, :-1])\n",
        "    y = np.array(data.iloc[:, -1])\n",
        "    \n",
        "    #print(\"Data succesfully loaded!\")\n",
        "\n",
        "    return  x,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "PxZTXwi2NYj3"
      },
      "outputs": [],
      "source": [
        "def normalize_data(data):\n",
        "    for i in range((len(data.columns)-1)):\n",
        "        num = data[i].max()\n",
        "        if num < 1:\n",
        "            continue\n",
        "        if num < 10:\n",
        "            data[i] = (data[i]) / (10)\n",
        "        elif num < 100:\n",
        "            data[i] = (data[i]) / (100)\n",
        "        elif num < 1000:\n",
        "            data[i] = (data[i]) / (1000)\n",
        "        else:\n",
        "            print(\"Error in normalization! Please check!\")\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = load_data(\"/content/drive/MyDrive/MLPROJECT/fd3.csv\")\n",
        "train_ratio = 0.60\n",
        "validation_ratio = 0.20\n",
        "test_ratio = 0.20\n",
        "\n",
        "# train is now 75% of the entire data set\n",
        "# the _junk suffix means that we drop that variable completely\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2)\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Net, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.fc2 = torch.nn.Linear(self.hidden_size, 1)\n",
        "        self.sigmoid = torch.nn.Sigmoid() \n",
        "    def forward(self, x):\n",
        "        hidden = self.fc1(x)\n",
        "        relu = self.relu(hidden)\n",
        "        output = self.fc2(relu)\n",
        "        output = self.sigmoid(output)\n",
        "        return output\n",
        "\n",
        "#convert to tensors\n",
        "training_input = torch.FloatTensor(x_train)\n",
        "training_output = torch.FloatTensor(y_train)\n",
        "test_input = torch.FloatTensor(x_test)\n",
        "test_output = torch.FloatTensor(y_test)\n",
        "\n",
        "input_size = training_input.size()[1] # number of features selected\n",
        "hidden_size = 20 # number of nodes/neurons in the hidden layer\n",
        "model = Net(input_size, hidden_size) # create the model\n",
        "criterion = torch.nn.BCELoss() # works for binary classification\n",
        "# without momentum parameter\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.9) \n",
        "#with momentum parameter\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.9, momentum=0.25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DudyuSUr26QY",
        "outputId": "08811257-013d-4f44-d090-865cd0dc3f4b"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.09245 0.3325  0.786   ... 0.069   0.095   0.094  ]\n",
            " [0.0972  0.349   0.8255  ... 0.0955  0.103   0.1    ]\n",
            " [0.09725 0.3515  0.8295  ... 0.098   0.11    0.108  ]\n",
            " ...\n",
            " [0.1102  0.372   0.8205  ... 0.125   0.126   0.121  ]\n",
            " [0.1115  0.3745  0.835   ... 0.1465  0.116   0.118  ]\n",
            " [0.1126  0.4165  0.889   ... 0.142   0.116   0.131  ]]\n",
            "[1 1 1 ... 1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/MLPROJECT/best2.pth'))\n",
        "model.train()\n",
        "epochs = 5000\n",
        "errors = []\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    y_pred = model(training_input)\n",
        "    # Compute Loss\n",
        "    loss = criterion(y_pred.squeeze(), training_output)\n",
        "    errors.append(loss.item())\n",
        "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "def accuracy(model, inp, out):\n",
        "  # ds is a iterable Dataset of Tensors\n",
        "  # one item at a time version\n",
        "  model.load_state_dict(torch.load('/content/drive/MyDrive/MLPROJECT/best2.pth'))\n",
        "  guesses = []\n",
        "  guess = ()\n",
        "  n_correct = 0; n_wrong = 0\n",
        "\n",
        "  for i in range(len(inp)):\n",
        "    inpts = inp[i]\n",
        "    target = out[i]   # float32  [0.0] or [1.0]\n",
        "    with torch.no_grad():\n",
        "      oupt = model(inpts)\n",
        "    guess = (target, oupt.round())\n",
        "\n",
        "    # avoid 'target == 1.0'\n",
        "    if guess[0]==guess[1]:\n",
        "      n_correct += 1\n",
        "    else:\n",
        "      n_wrong += 1\n",
        "\n",
        "  return (n_correct * 1.0) / (n_correct + n_wrong)\n",
        "\n",
        "acc = accuracy(model,test_input,test_output)\n",
        "print(f'Accuracy: {acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5sXyn7d35c2",
        "outputId": "b948b73b-35f4-41d0-c68f-966a4590d0a6"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 1: train loss: 0.44279131293296814\n",
            "Epoch 2: train loss: 0.3764791786670685\n",
            "Epoch 3: train loss: 0.3958619236946106\n",
            "Epoch 4: train loss: 0.3709210455417633\n",
            "Epoch 5: train loss: 0.4124215543270111\n",
            "Epoch 6: train loss: 0.39766156673431396\n",
            "Epoch 7: train loss: 0.46247637271881104\n",
            "Epoch 8: train loss: 0.42699089646339417\n",
            "Epoch 9: train loss: 0.4900936782360077\n",
            "Epoch 10: train loss: 0.41747549176216125\n",
            "Epoch 11: train loss: 0.4540849030017853\n",
            "Epoch 12: train loss: 0.3909754157066345\n",
            "Epoch 13: train loss: 0.4193398952484131\n",
            "Epoch 14: train loss: 0.3841320276260376\n",
            "Epoch 15: train loss: 0.4252902567386627\n",
            "Epoch 16: train loss: 0.3972679078578949\n",
            "Epoch 17: train loss: 0.45151084661483765\n",
            "Epoch 18: train loss: 0.41479283571243286\n",
            "Epoch 19: train loss: 0.4715404808521271\n",
            "Epoch 20: train loss: 0.4145137667655945\n",
            "Epoch 21: train loss: 0.4582235813140869\n",
            "Epoch 22: train loss: 0.3993871212005615\n",
            "Epoch 23: train loss: 0.43507471680641174\n",
            "Epoch 24: train loss: 0.39036521315574646\n",
            "Epoch 25: train loss: 0.43057602643966675\n",
            "Epoch 26: train loss: 0.39683812856674194\n",
            "Epoch 27: train loss: 0.4482913613319397\n",
            "Epoch 28: train loss: 0.4108593463897705\n",
            "Epoch 29: train loss: 0.4654991328716278\n",
            "Epoch 30: train loss: 0.4131124019622803\n",
            "Epoch 31: train loss: 0.45995816588401794\n",
            "Epoch 32: train loss: 0.4029196798801422\n",
            "Epoch 33: train loss: 0.4416058659553528\n",
            "Epoch 34: train loss: 0.39394575357437134\n",
            "Epoch 35: train loss: 0.43514004349708557\n",
            "Epoch 36: train loss: 0.39772483706474304\n",
            "Epoch 37: train loss: 0.4470684826374054\n",
            "Epoch 38: train loss: 0.40854501724243164\n",
            "Epoch 39: train loss: 0.4622100293636322\n",
            "Epoch 40: train loss: 0.41158509254455566\n",
            "Epoch 41: train loss: 0.4591996371746063\n",
            "Epoch 42: train loss: 0.4042399525642395\n",
            "Epoch 43: train loss: 0.4456741213798523\n",
            "Epoch 44: train loss: 0.3965727686882019\n",
            "Epoch 45: train loss: 0.4383561611175537\n",
            "Epoch 46: train loss: 0.3989724814891815\n",
            "Epoch 47: train loss: 0.44811293482780457\n",
            "Epoch 48: train loss: 0.4072580635547638\n",
            "Epoch 49: train loss: 0.4590608775615692\n",
            "Epoch 50: train loss: 0.4094987213611603\n",
            "Epoch 51: train loss: 0.4577491581439972\n",
            "Epoch 52: train loss: 0.40503063797950745\n",
            "Epoch 53: train loss: 0.4485284388065338\n",
            "Epoch 54: train loss: 0.3993038237094879\n",
            "Epoch 55: train loss: 0.4427706301212311\n",
            "Epoch 56: train loss: 0.3999752700328827\n",
            "Epoch 57: train loss: 0.44746723771095276\n",
            "Epoch 58: train loss: 0.4052391052246094\n",
            "Epoch 59: train loss: 0.4558044970035553\n",
            "Epoch 60: train loss: 0.4087332785129547\n",
            "Epoch 61: train loss: 0.45775091648101807\n",
            "Epoch 62: train loss: 0.4061225950717926\n",
            "Epoch 63: train loss: 0.451224684715271\n",
            "Epoch 64: train loss: 0.4010930359363556\n",
            "Epoch 65: train loss: 0.444407194852829\n",
            "Epoch 66: train loss: 0.4002143442630768\n",
            "Epoch 67: train loss: 0.4473573863506317\n",
            "Epoch 68: train loss: 0.4042179584503174\n",
            "Epoch 69: train loss: 0.45365121960639954\n",
            "Epoch 70: train loss: 0.40797391533851624\n",
            "Epoch 71: train loss: 0.457606703042984\n",
            "Epoch 72: train loss: 0.40662458539009094\n",
            "Epoch 73: train loss: 0.45220181345939636\n",
            "Epoch 74: train loss: 0.40217071771621704\n",
            "Epoch 75: train loss: 0.4464172124862671\n",
            "Epoch 76: train loss: 0.40105879306793213\n",
            "Epoch 77: train loss: 0.44738394021987915\n",
            "Epoch 78: train loss: 0.4032876491546631\n",
            "Epoch 79: train loss: 0.45229804515838623\n",
            "Epoch 80: train loss: 0.40686944127082825\n",
            "Epoch 81: train loss: 0.4560468792915344\n",
            "Epoch 82: train loss: 0.40717068314552307\n",
            "Epoch 83: train loss: 0.45433270931243896\n",
            "Epoch 84: train loss: 0.40356287360191345\n",
            "Epoch 85: train loss: 0.4480232894420624\n",
            "Epoch 86: train loss: 0.4006396234035492\n",
            "Epoch 87: train loss: 0.44623640179634094\n",
            "Epoch 88: train loss: 0.402191162109375\n",
            "Epoch 89: train loss: 0.45061731338500977\n",
            "Epoch 90: train loss: 0.406448632478714\n",
            "Epoch 91: train loss: 0.45699989795684814\n",
            "Epoch 92: train loss: 0.4078991711139679\n",
            "Epoch 93: train loss: 0.4553581774234772\n",
            "Epoch 94: train loss: 0.4045061469078064\n",
            "Epoch 95: train loss: 0.4495134651660919\n",
            "Epoch 96: train loss: 0.40084409713745117\n",
            "Epoch 97: train loss: 0.44547128677368164\n",
            "Epoch 98: train loss: 0.40260571241378784\n",
            "Epoch 99: train loss: 0.4512217044830322\n",
            "Epoch 100: train loss: 0.40591961145401\n",
            "Epoch 101: train loss: 0.45512229204177856\n",
            "Epoch 102: train loss: 0.40704548358917236\n",
            "Epoch 103: train loss: 0.4548730254173279\n",
            "Epoch 104: train loss: 0.40451860427856445\n",
            "Epoch 105: train loss: 0.44969311356544495\n",
            "Epoch 106: train loss: 0.4015316963195801\n",
            "Epoch 107: train loss: 0.44707730412483215\n",
            "Epoch 108: train loss: 0.40271663665771484\n",
            "Epoch 109: train loss: 0.45081862807273865\n",
            "Epoch 110: train loss: 0.40554091334342957\n",
            "Epoch 111: train loss: 0.4548191726207733\n",
            "Epoch 112: train loss: 0.4068760573863983\n",
            "Epoch 113: train loss: 0.45455941557884216\n",
            "Epoch 114: train loss: 0.4045337736606598\n",
            "Epoch 115: train loss: 0.4501969516277313\n",
            "Epoch 116: train loss: 0.40186792612075806\n",
            "Epoch 117: train loss: 0.4474475085735321\n",
            "Epoch 118: train loss: 0.40375691652297974\n",
            "Epoch 119: train loss: 0.45274606347084045\n",
            "Epoch 120: train loss: 0.4058575928211212\n",
            "Epoch 121: train loss: 0.45379766821861267\n",
            "Epoch 122: train loss: 0.40519630908966064\n",
            "Epoch 123: train loss: 0.4521118998527527\n",
            "Epoch 124: train loss: 0.40340062975883484\n",
            "Epoch 125: train loss: 0.44947513937950134\n",
            "Epoch 126: train loss: 0.402811735868454\n",
            "Epoch 127: train loss: 0.450295090675354\n",
            "Epoch 128: train loss: 0.4041803479194641\n",
            "Epoch 129: train loss: 0.4521391987800598\n",
            "Epoch 130: train loss: 0.40532875061035156\n",
            "Epoch 131: train loss: 0.4535873830318451\n",
            "Epoch 132: train loss: 0.4053780734539032\n",
            "Epoch 133: train loss: 0.45242342352867126\n",
            "Epoch 134: train loss: 0.4036794602870941\n",
            "Epoch 135: train loss: 0.44995298981666565\n",
            "Epoch 136: train loss: 0.4035739302635193\n",
            "Epoch 137: train loss: 0.4507976472377777\n",
            "Epoch 138: train loss: 0.4037550687789917\n",
            "Epoch 139: train loss: 0.4512576460838318\n",
            "Epoch 140: train loss: 0.40431323647499084\n",
            "Epoch 141: train loss: 0.4521196484565735\n",
            "Epoch 142: train loss: 0.4047934412956238\n",
            "Epoch 143: train loss: 0.4524260461330414\n",
            "Epoch 144: train loss: 0.40454211831092834\n",
            "Epoch 145: train loss: 0.4516660273075104\n",
            "Epoch 146: train loss: 0.40375128388404846\n",
            "Epoch 147: train loss: 0.45064565539360046\n",
            "Epoch 148: train loss: 0.40426862239837646\n",
            "Epoch 149: train loss: 0.4518536329269409\n",
            "Epoch 150: train loss: 0.40490326285362244\n",
            "Epoch 151: train loss: 0.45236170291900635\n",
            "Epoch 152: train loss: 0.4048634171485901\n",
            "Epoch 153: train loss: 0.45180773735046387\n",
            "Epoch 154: train loss: 0.40399593114852905\n",
            "Epoch 155: train loss: 0.4505544602870941\n",
            "Epoch 156: train loss: 0.40366747975349426\n",
            "Epoch 157: train loss: 0.4506611227989197\n",
            "Epoch 158: train loss: 0.40415218472480774\n",
            "Epoch 159: train loss: 0.45165473222732544\n",
            "Epoch 160: train loss: 0.4049247205257416\n",
            "Epoch 161: train loss: 0.4524640440940857\n",
            "Epoch 162: train loss: 0.4048667550086975\n",
            "Epoch 163: train loss: 0.4518316686153412\n",
            "Epoch 164: train loss: 0.40397870540618896\n",
            "Epoch 165: train loss: 0.45053672790527344\n",
            "Epoch 166: train loss: 0.40440496802330017\n",
            "Epoch 167: train loss: 0.45165592432022095\n",
            "Epoch 168: train loss: 0.404182106256485\n",
            "Epoch 169: train loss: 0.45108988881111145\n",
            "Epoch 170: train loss: 0.4048123359680176\n",
            "Epoch 171: train loss: 0.4521559774875641\n",
            "Epoch 172: train loss: 0.4051794111728668\n",
            "Epoch 173: train loss: 0.45214325189590454\n",
            "Epoch 174: train loss: 0.4046722948551178\n",
            "Epoch 175: train loss: 0.4510754942893982\n",
            "Epoch 176: train loss: 0.40387725830078125\n",
            "Epoch 177: train loss: 0.45026257634162903\n",
            "Epoch 178: train loss: 0.40461623668670654\n",
            "Epoch 179: train loss: 0.4518391788005829\n",
            "Epoch 180: train loss: 0.4048168957233429\n",
            "Epoch 181: train loss: 0.45164281129837036\n",
            "Epoch 182: train loss: 0.4044526517391205\n",
            "Epoch 183: train loss: 0.4510713517665863\n",
            "Epoch 184: train loss: 0.40408095717430115\n",
            "Epoch 185: train loss: 0.4507942497730255\n",
            "Epoch 186: train loss: 0.40501007437705994\n",
            "Epoch 187: train loss: 0.4524194598197937\n",
            "Epoch 188: train loss: 0.40496203303337097\n",
            "Epoch 189: train loss: 0.4516719877719879\n",
            "Epoch 190: train loss: 0.4042854607105255\n",
            "Epoch 191: train loss: 0.45069557428359985\n",
            "Epoch 192: train loss: 0.404517263174057\n",
            "Epoch 193: train loss: 0.45150500535964966\n",
            "Epoch 194: train loss: 0.4044305682182312\n",
            "Epoch 195: train loss: 0.45124542713165283\n",
            "Epoch 196: train loss: 0.40514546632766724\n",
            "Epoch 197: train loss: 0.45241260528564453\n",
            "Epoch 198: train loss: 0.4046661853790283\n",
            "Epoch 199: train loss: 0.451178640127182\n",
            "Epoch 200: train loss: 0.4046459197998047\n",
            "Epoch 201: train loss: 0.45144978165626526\n",
            "Epoch 202: train loss: 0.40484878420829773\n",
            "Epoch 203: train loss: 0.45166659355163574\n",
            "Epoch 204: train loss: 0.40487632155418396\n",
            "Epoch 205: train loss: 0.45156586170196533\n",
            "Epoch 206: train loss: 0.40467387437820435\n",
            "Epoch 207: train loss: 0.45123666524887085\n",
            "Epoch 208: train loss: 0.40453049540519714\n",
            "Epoch 209: train loss: 0.4511825144290924\n",
            "Epoch 210: train loss: 0.40467846393585205\n",
            "Epoch 211: train loss: 0.4515058100223541\n",
            "Epoch 212: train loss: 0.404873788356781\n",
            "Epoch 213: train loss: 0.4516812264919281\n",
            "Epoch 214: train loss: 0.40482139587402344\n",
            "Epoch 215: train loss: 0.45152661204338074\n",
            "Epoch 216: train loss: 0.40466630458831787\n",
            "Epoch 217: train loss: 0.45115429162979126\n",
            "Epoch 218: train loss: 0.4045199155807495\n",
            "Epoch 219: train loss: 0.4513566195964813\n",
            "Epoch 220: train loss: 0.40476980805397034\n",
            "Epoch 221: train loss: 0.45146918296813965\n",
            "Epoch 222: train loss: 0.40477892756462097\n",
            "Epoch 223: train loss: 0.45168548822402954\n",
            "Epoch 224: train loss: 0.40481099486351013\n",
            "Epoch 225: train loss: 0.45136258006095886\n",
            "Epoch 226: train loss: 0.4045860767364502\n",
            "Epoch 227: train loss: 0.4513721764087677\n",
            "Epoch 228: train loss: 0.40470632910728455\n",
            "Epoch 229: train loss: 0.4513314962387085\n",
            "Epoch 230: train loss: 0.40464353561401367\n",
            "Epoch 231: train loss: 0.45153969526290894\n",
            "Epoch 232: train loss: 0.4048698842525482\n",
            "Epoch 233: train loss: 0.4515439569950104\n",
            "Epoch 234: train loss: 0.404717355966568\n",
            "Epoch 235: train loss: 0.4515206813812256\n",
            "Epoch 236: train loss: 0.40465226769447327\n",
            "Epoch 237: train loss: 0.45117542147636414\n",
            "Epoch 238: train loss: 0.4044610261917114\n",
            "Epoch 239: train loss: 0.4513225555419922\n",
            "Epoch 240: train loss: 0.40476304292678833\n",
            "Epoch 241: train loss: 0.4515194892883301\n",
            "Epoch 242: train loss: 0.40394970774650574\n",
            "Epoch 243: train loss: 0.4506424069404602\n",
            "Epoch 244: train loss: 0.40477216243743896\n",
            "Epoch 245: train loss: 0.45200714468955994\n",
            "Epoch 246: train loss: 0.40450143814086914\n",
            "Epoch 247: train loss: 0.45148617029190063\n",
            "Epoch 248: train loss: 0.4042799472808838\n",
            "Epoch 249: train loss: 0.4510676860809326\n",
            "Epoch 250: train loss: 0.4042072296142578\n",
            "Epoch 251: train loss: 0.4514799416065216\n",
            "Epoch 252: train loss: 0.4046062231063843\n",
            "Epoch 253: train loss: 0.4517418444156647\n",
            "Epoch 254: train loss: 0.4045611619949341\n",
            "Epoch 255: train loss: 0.45179715752601624\n",
            "Epoch 256: train loss: 0.4044871926307678\n",
            "Epoch 257: train loss: 0.4513334333896637\n",
            "Epoch 258: train loss: 0.4041992723941803\n",
            "Epoch 259: train loss: 0.4513120949268341\n",
            "Epoch 260: train loss: 0.4043903946876526\n",
            "Epoch 261: train loss: 0.4514102041721344\n",
            "Epoch 262: train loss: 0.404337078332901\n",
            "Epoch 263: train loss: 0.45159557461738586\n",
            "Epoch 264: train loss: 0.4044824242591858\n",
            "Epoch 265: train loss: 0.4514838457107544\n",
            "Epoch 266: train loss: 0.4043322801589966\n",
            "Epoch 267: train loss: 0.4515180289745331\n",
            "Epoch 268: train loss: 0.4043724536895752\n",
            "Epoch 269: train loss: 0.4512958228588104\n",
            "Epoch 270: train loss: 0.4042190611362457\n",
            "Epoch 271: train loss: 0.45140340924263\n",
            "Epoch 272: train loss: 0.4043906331062317\n",
            "Epoch 273: train loss: 0.451389878988266\n",
            "Epoch 274: train loss: 0.4042399525642395\n",
            "Epoch 275: train loss: 0.4514358639717102\n",
            "Epoch 276: train loss: 0.4043681025505066\n",
            "Epoch 277: train loss: 0.4513310194015503\n",
            "Epoch 278: train loss: 0.4041849374771118\n",
            "Epoch 279: train loss: 0.45135536789894104\n",
            "Epoch 280: train loss: 0.40433844923973083\n",
            "Epoch 281: train loss: 0.45132726430892944\n",
            "Epoch 282: train loss: 0.40418747067451477\n",
            "Epoch 283: train loss: 0.4513903856277466\n",
            "Epoch 284: train loss: 0.4043053388595581\n",
            "Epoch 285: train loss: 0.4512854814529419\n",
            "Epoch 286: train loss: 0.4042099714279175\n",
            "Epoch 287: train loss: 0.4514096975326538\n",
            "Epoch 288: train loss: 0.4043475389480591\n",
            "Epoch 289: train loss: 0.45133352279663086\n",
            "Epoch 290: train loss: 0.40422552824020386\n",
            "Epoch 291: train loss: 0.45140692591667175\n",
            "Epoch 292: train loss: 0.4042600691318512\n",
            "Epoch 293: train loss: 0.45114701986312866\n",
            "Epoch 294: train loss: 0.40404531359672546\n",
            "Epoch 295: train loss: 0.451160192489624\n",
            "Epoch 296: train loss: 0.40424102544784546\n",
            "Epoch 297: train loss: 0.45132553577423096\n",
            "Epoch 298: train loss: 0.4042396545410156\n",
            "Epoch 299: train loss: 0.45134761929512024\n",
            "Epoch 300: train loss: 0.4042411744594574\n",
            "Epoch 301: train loss: 0.45126983523368835\n",
            "Epoch 302: train loss: 0.40412718057632446\n",
            "Epoch 303: train loss: 0.4511328339576721\n",
            "Epoch 304: train loss: 0.40412986278533936\n",
            "Epoch 305: train loss: 0.45119139552116394\n",
            "Epoch 306: train loss: 0.40414056181907654\n",
            "Epoch 307: train loss: 0.4512455463409424\n",
            "Epoch 308: train loss: 0.40422946214675903\n",
            "Epoch 309: train loss: 0.451262891292572\n",
            "Epoch 310: train loss: 0.404073029756546\n",
            "Epoch 311: train loss: 0.45114007592201233\n",
            "Epoch 312: train loss: 0.4040730595588684\n",
            "Epoch 313: train loss: 0.450883150100708\n",
            "Epoch 314: train loss: 0.40391474962234497\n",
            "Epoch 315: train loss: 0.45113036036491394\n",
            "Epoch 316: train loss: 0.40421462059020996\n",
            "Epoch 317: train loss: 0.4511982798576355\n",
            "Epoch 318: train loss: 0.40413621068000793\n",
            "Epoch 319: train loss: 0.4513721764087677\n",
            "Epoch 320: train loss: 0.404245525598526\n",
            "Epoch 321: train loss: 0.451144814491272\n",
            "Epoch 322: train loss: 0.4038846790790558\n",
            "Epoch 323: train loss: 0.4509388506412506\n",
            "Epoch 324: train loss: 0.4040215313434601\n",
            "Epoch 325: train loss: 0.45093050599098206\n",
            "Epoch 326: train loss: 0.4039624035358429\n",
            "Epoch 327: train loss: 0.451240211725235\n",
            "Epoch 328: train loss: 0.4042769968509674\n",
            "Epoch 329: train loss: 0.4512505531311035\n",
            "Epoch 330: train loss: 0.4041006565093994\n",
            "Epoch 331: train loss: 0.45124709606170654\n",
            "Epoch 332: train loss: 0.40408021211624146\n",
            "Epoch 333: train loss: 0.4508374035358429\n",
            "Epoch 334: train loss: 0.4037242829799652\n",
            "Epoch 335: train loss: 0.4508274495601654\n",
            "Epoch 336: train loss: 0.40402570366859436\n",
            "Epoch 337: train loss: 0.45103979110717773\n",
            "Epoch 338: train loss: 0.40414002537727356\n",
            "Epoch 339: train loss: 0.4514842927455902\n",
            "Epoch 340: train loss: 0.40430280566215515\n",
            "Epoch 341: train loss: 0.4511463940143585\n",
            "Epoch 342: train loss: 0.40388134121894836\n",
            "Epoch 343: train loss: 0.4508848488330841\n",
            "Epoch 344: train loss: 0.40376296639442444\n",
            "Epoch 345: train loss: 0.4505251348018646\n",
            "Epoch 346: train loss: 0.40369465947151184\n",
            "Epoch 347: train loss: 0.45102494955062866\n",
            "Epoch 348: train loss: 0.40434467792510986\n",
            "Epoch 349: train loss: 0.4515277147293091\n",
            "Epoch 350: train loss: 0.4041851758956909\n",
            "Epoch 351: train loss: 0.4513394832611084\n",
            "Epoch 352: train loss: 0.40405818819999695\n",
            "Epoch 353: train loss: 0.450733482837677\n",
            "Epoch 354: train loss: 0.40358540415763855\n",
            "Epoch 355: train loss: 0.4505232572555542\n",
            "Epoch 356: train loss: 0.4037295877933502\n",
            "Epoch 357: train loss: 0.4506075978279114\n",
            "Epoch 358: train loss: 0.4039366841316223\n",
            "Epoch 359: train loss: 0.4513551592826843\n",
            "Epoch 360: train loss: 0.40436190366744995\n",
            "Epoch 361: train loss: 0.45145365595817566\n",
            "Epoch 362: train loss: 0.40401703119277954\n",
            "Epoch 363: train loss: 0.45084837079048157\n",
            "Epoch 364: train loss: 0.403706431388855\n",
            "Epoch 365: train loss: 0.4504987895488739\n",
            "Epoch 366: train loss: 0.4035557210445404\n",
            "Epoch 367: train loss: 0.4505384862422943\n",
            "Epoch 368: train loss: 0.40400534868240356\n",
            "Epoch 369: train loss: 0.4512951374053955\n",
            "Epoch 370: train loss: 0.40416279435157776\n",
            "Epoch 371: train loss: 0.4512399733066559\n",
            "Epoch 372: train loss: 0.40400463342666626\n",
            "Epoch 373: train loss: 0.45088112354278564\n",
            "Epoch 374: train loss: 0.403642475605011\n",
            "Epoch 375: train loss: 0.450449675321579\n",
            "Epoch 376: train loss: 0.4036310613155365\n",
            "Epoch 377: train loss: 0.450684517621994\n",
            "Epoch 378: train loss: 0.40388184785842896\n",
            "Epoch 379: train loss: 0.4511052370071411\n",
            "Epoch 380: train loss: 0.4041500985622406\n",
            "Epoch 381: train loss: 0.4512757956981659\n",
            "Epoch 382: train loss: 0.403874009847641\n",
            "Epoch 383: train loss: 0.45072752237319946\n",
            "Epoch 384: train loss: 0.4036831557750702\n",
            "Epoch 385: train loss: 0.4506012499332428\n",
            "Epoch 386: train loss: 0.4036117196083069\n",
            "Epoch 387: train loss: 0.4506433308124542\n",
            "Epoch 388: train loss: 0.40389710664749146\n",
            "Epoch 389: train loss: 0.45111602544784546\n",
            "Epoch 390: train loss: 0.4040243327617645\n",
            "Epoch 391: train loss: 0.45111083984375\n",
            "Epoch 392: train loss: 0.4039248824119568\n",
            "Epoch 393: train loss: 0.4508558511734009\n",
            "Epoch 394: train loss: 0.4035557210445404\n",
            "Epoch 395: train loss: 0.4504043757915497\n",
            "Epoch 396: train loss: 0.403572678565979\n",
            "Epoch 397: train loss: 0.4506625235080719\n",
            "Epoch 398: train loss: 0.4037513732910156\n",
            "Epoch 399: train loss: 0.4509865641593933\n",
            "Epoch 400: train loss: 0.4041689932346344\n",
            "Epoch 401: train loss: 0.45138970017433167\n",
            "Epoch 402: train loss: 0.4039185643196106\n",
            "Epoch 403: train loss: 0.45077094435691833\n",
            "Epoch 404: train loss: 0.40347257256507874\n",
            "Epoch 405: train loss: 0.4502487778663635\n",
            "Epoch 406: train loss: 0.40337860584259033\n",
            "Epoch 407: train loss: 0.45047301054000854\n",
            "Epoch 408: train loss: 0.4038265645503998\n",
            "Epoch 409: train loss: 0.4511467516422272\n",
            "Epoch 410: train loss: 0.4040718376636505\n",
            "Epoch 411: train loss: 0.4512578547000885\n",
            "Epoch 412: train loss: 0.4039182960987091\n",
            "Epoch 413: train loss: 0.4508332312107086\n",
            "Epoch 414: train loss: 0.40356555581092834\n",
            "Epoch 415: train loss: 0.4503524899482727\n",
            "Epoch 416: train loss: 0.4032730758190155\n",
            "Epoch 417: train loss: 0.45018938183784485\n",
            "Epoch 418: train loss: 0.40355825424194336\n",
            "Epoch 419: train loss: 0.45093831419944763\n",
            "Epoch 420: train loss: 0.40414589643478394\n",
            "Epoch 421: train loss: 0.45141205191612244\n",
            "Epoch 422: train loss: 0.40398937463760376\n",
            "Epoch 423: train loss: 0.450955331325531\n",
            "Epoch 424: train loss: 0.40347328782081604\n",
            "Epoch 425: train loss: 0.4501584470272064\n",
            "Epoch 426: train loss: 0.40323641896247864\n",
            "Epoch 427: train loss: 0.45025911927223206\n",
            "Epoch 428: train loss: 0.403609037399292\n",
            "Epoch 429: train loss: 0.45088067650794983\n",
            "Epoch 430: train loss: 0.40406545996665955\n",
            "Epoch 431: train loss: 0.4515079855918884\n",
            "Epoch 432: train loss: 0.4040137827396393\n",
            "Epoch 433: train loss: 0.4507185220718384\n",
            "Epoch 434: train loss: 0.4033321440219879\n",
            "Epoch 435: train loss: 0.45020872354507446\n",
            "Epoch 436: train loss: 0.4032648801803589\n",
            "Epoch 437: train loss: 0.4501953721046448\n",
            "Epoch 438: train loss: 0.4036082625389099\n",
            "Epoch 439: train loss: 0.45086583495140076\n",
            "Epoch 440: train loss: 0.4039143919944763\n",
            "Epoch 441: train loss: 0.4511864185333252\n",
            "Epoch 442: train loss: 0.40390902757644653\n",
            "Epoch 443: train loss: 0.45084187388420105\n",
            "Epoch 444: train loss: 0.4034150540828705\n",
            "Epoch 445: train loss: 0.4501868784427643\n",
            "Epoch 446: train loss: 0.4031849801540375\n",
            "Epoch 447: train loss: 0.45009833574295044\n",
            "Epoch 448: train loss: 0.40356653928756714\n",
            "Epoch 449: train loss: 0.45104512572288513\n",
            "Epoch 450: train loss: 0.4040432572364807\n",
            "Epoch 451: train loss: 0.4510990083217621\n",
            "Epoch 452: train loss: 0.40371283888816833\n",
            "Epoch 453: train loss: 0.4507596790790558\n",
            "Epoch 454: train loss: 0.40340566635131836\n",
            "Epoch 455: train loss: 0.45010659098625183\n",
            "Epoch 456: train loss: 0.40312623977661133\n",
            "Epoch 457: train loss: 0.4500887393951416\n",
            "Epoch 458: train loss: 0.4035390019416809\n",
            "Epoch 459: train loss: 0.4508790373802185\n",
            "Epoch 460: train loss: 0.40389105677604675\n",
            "Epoch 461: train loss: 0.4510658383369446\n",
            "Epoch 462: train loss: 0.4039376378059387\n",
            "Epoch 463: train loss: 0.4510549306869507\n",
            "Epoch 464: train loss: 0.40338918566703796\n",
            "Epoch 465: train loss: 0.44981852173805237\n",
            "Epoch 466: train loss: 0.4029514789581299\n",
            "Epoch 467: train loss: 0.449995219707489\n",
            "Epoch 468: train loss: 0.40334659814834595\n",
            "Epoch 469: train loss: 0.45049548149108887\n",
            "Epoch 470: train loss: 0.4039762020111084\n",
            "Epoch 471: train loss: 0.45160552859306335\n",
            "Epoch 472: train loss: 0.40411293506622314\n",
            "Epoch 473: train loss: 0.4508543312549591\n",
            "Epoch 474: train loss: 0.40333032608032227\n",
            "Epoch 475: train loss: 0.4500598907470703\n",
            "Epoch 476: train loss: 0.40286609530448914\n",
            "Epoch 477: train loss: 0.44948434829711914\n",
            "Epoch 478: train loss: 0.4031299352645874\n",
            "Epoch 479: train loss: 0.45037898421287537\n",
            "Epoch 480: train loss: 0.4038727581501007\n",
            "Epoch 481: train loss: 0.45140188932418823\n",
            "Epoch 482: train loss: 0.4043835401535034\n",
            "Epoch 483: train loss: 0.45153629779815674\n",
            "Epoch 484: train loss: 0.4034363925457001\n",
            "Epoch 485: train loss: 0.44970405101776123\n",
            "Epoch 486: train loss: 0.4024912714958191\n",
            "Epoch 487: train loss: 0.4490288496017456\n",
            "Epoch 488: train loss: 0.40276896953582764\n",
            "Epoch 489: train loss: 0.4501085579395294\n",
            "Epoch 490: train loss: 0.4041261076927185\n",
            "Epoch 491: train loss: 0.45197364687919617\n",
            "Epoch 492: train loss: 0.40457847714424133\n",
            "Epoch 493: train loss: 0.4516826868057251\n",
            "Epoch 494: train loss: 0.4035465121269226\n",
            "Epoch 495: train loss: 0.4497505724430084\n",
            "Epoch 496: train loss: 0.4023208022117615\n",
            "Epoch 497: train loss: 0.448705792427063\n",
            "Epoch 498: train loss: 0.40264058113098145\n",
            "Epoch 499: train loss: 0.4500195384025574\n",
            "Epoch 500: train loss: 0.403970867395401\n",
            "Epoch 501: train loss: 0.4518733620643616\n",
            "Epoch 502: train loss: 0.40472152829170227\n",
            "Epoch 503: train loss: 0.4519357979297638\n",
            "Epoch 504: train loss: 0.40366074442863464\n",
            "Epoch 505: train loss: 0.4499462842941284\n",
            "Epoch 506: train loss: 0.40233471989631653\n",
            "Epoch 507: train loss: 0.44853147864341736\n",
            "Epoch 508: train loss: 0.40228021144866943\n",
            "Epoch 509: train loss: 0.449439138174057\n",
            "Epoch 510: train loss: 0.4038621485233307\n",
            "Epoch 511: train loss: 0.45192593336105347\n",
            "Epoch 512: train loss: 0.4047890305519104\n",
            "Epoch 513: train loss: 0.4521891474723816\n",
            "Epoch 514: train loss: 0.40394389629364014\n",
            "Epoch 515: train loss: 0.4501585066318512\n",
            "Epoch 516: train loss: 0.40218132734298706\n",
            "Epoch 517: train loss: 0.448190301656723\n",
            "Epoch 518: train loss: 0.40207919478416443\n",
            "Epoch 519: train loss: 0.44916579127311707\n",
            "Epoch 520: train loss: 0.4036709666252136\n",
            "Epoch 521: train loss: 0.4517432451248169\n",
            "Epoch 522: train loss: 0.4048997759819031\n",
            "Epoch 523: train loss: 0.45245426893234253\n",
            "Epoch 524: train loss: 0.40395963191986084\n",
            "Epoch 525: train loss: 0.45015719532966614\n",
            "Epoch 526: train loss: 0.40235182642936707\n",
            "Epoch 527: train loss: 0.44838592410087585\n",
            "Epoch 528: train loss: 0.401975154876709\n",
            "Epoch 529: train loss: 0.44889140129089355\n",
            "Epoch 530: train loss: 0.4033958911895752\n",
            "Epoch 531: train loss: 0.4513925611972809\n",
            "Epoch 532: train loss: 0.4046357572078705\n",
            "Epoch 533: train loss: 0.4523235857486725\n",
            "Epoch 534: train loss: 0.4042729139328003\n",
            "Epoch 535: train loss: 0.4507596492767334\n",
            "Epoch 536: train loss: 0.40262535214424133\n",
            "Epoch 537: train loss: 0.4485793709754944\n",
            "Epoch 538: train loss: 0.4019313454627991\n",
            "Epoch 539: train loss: 0.4485703706741333\n",
            "Epoch 540: train loss: 0.40282538533210754\n",
            "Epoch 541: train loss: 0.4505394697189331\n",
            "Epoch 542: train loss: 0.40440160036087036\n",
            "Epoch 543: train loss: 0.45246994495391846\n",
            "Epoch 544: train loss: 0.4046112895011902\n",
            "Epoch 545: train loss: 0.45154350996017456\n",
            "Epoch 546: train loss: 0.40308207273483276\n",
            "Epoch 547: train loss: 0.4489043951034546\n",
            "Epoch 548: train loss: 0.40158912539482117\n",
            "Epoch 549: train loss: 0.44784852862358093\n",
            "Epoch 550: train loss: 0.4024710953235626\n",
            "Epoch 551: train loss: 0.45017722249031067\n",
            "Epoch 552: train loss: 0.4042215049266815\n",
            "Epoch 553: train loss: 0.45233628153800964\n",
            "Epoch 554: train loss: 0.4048081040382385\n",
            "Epoch 555: train loss: 0.452078640460968\n",
            "Epoch 556: train loss: 0.4033265709877014\n",
            "Epoch 557: train loss: 0.4491891860961914\n",
            "Epoch 558: train loss: 0.4017132520675659\n",
            "Epoch 559: train loss: 0.4477623999118805\n",
            "Epoch 560: train loss: 0.4020484387874603\n",
            "Epoch 561: train loss: 0.44946858286857605\n",
            "Epoch 562: train loss: 0.40408098697662354\n",
            "Epoch 563: train loss: 0.45240601897239685\n",
            "Epoch 564: train loss: 0.4048175513744354\n",
            "Epoch 565: train loss: 0.45208078622817993\n",
            "Epoch 566: train loss: 0.40348702669143677\n",
            "Epoch 567: train loss: 0.4495130777359009\n",
            "Epoch 568: train loss: 0.4018879234790802\n",
            "Epoch 569: train loss: 0.4478435814380646\n",
            "Epoch 570: train loss: 0.401943564414978\n",
            "Epoch 571: train loss: 0.4491114020347595\n",
            "Epoch 572: train loss: 0.40356528759002686\n",
            "Epoch 573: train loss: 0.45168575644493103\n",
            "Epoch 574: train loss: 0.40478867292404175\n",
            "Epoch 575: train loss: 0.4524601697921753\n",
            "Epoch 576: train loss: 0.40386122465133667\n",
            "Epoch 577: train loss: 0.44998666644096375\n",
            "Epoch 578: train loss: 0.4021299183368683\n",
            "Epoch 579: train loss: 0.4479480981826782\n",
            "Epoch 580: train loss: 0.40156590938568115\n",
            "Epoch 581: train loss: 0.4483915865421295\n",
            "Epoch 582: train loss: 0.40323302149772644\n",
            "Epoch 583: train loss: 0.45132678747177124\n",
            "Epoch 584: train loss: 0.4047083556652069\n",
            "Epoch 585: train loss: 0.452548623085022\n",
            "Epoch 586: train loss: 0.4042242467403412\n",
            "Epoch 587: train loss: 0.45061153173446655\n",
            "Epoch 588: train loss: 0.40220576524734497\n",
            "Epoch 589: train loss: 0.44787847995758057\n",
            "Epoch 590: train loss: 0.4014868438243866\n",
            "Epoch 591: train loss: 0.44814130663871765\n",
            "Epoch 592: train loss: 0.40285035967826843\n",
            "Epoch 593: train loss: 0.45082101225852966\n",
            "Epoch 594: train loss: 0.404527485370636\n",
            "Epoch 595: train loss: 0.45251354575157166\n",
            "Epoch 596: train loss: 0.4042620062828064\n",
            "Epoch 597: train loss: 0.450979620218277\n",
            "Epoch 598: train loss: 0.40261977910995483\n",
            "Epoch 599: train loss: 0.44839590787887573\n",
            "Epoch 600: train loss: 0.4016586244106293\n",
            "Epoch 601: train loss: 0.44815465807914734\n",
            "Epoch 602: train loss: 0.40268048644065857\n",
            "Epoch 603: train loss: 0.45033740997314453\n",
            "Epoch 604: train loss: 0.40394118428230286\n",
            "Epoch 605: train loss: 0.4517807364463806\n",
            "Epoch 606: train loss: 0.4042152166366577\n",
            "Epoch 607: train loss: 0.45126885175704956\n",
            "Epoch 608: train loss: 0.4028826057910919\n",
            "Epoch 609: train loss: 0.4488590359687805\n",
            "Epoch 610: train loss: 0.4018916189670563\n",
            "Epoch 611: train loss: 0.448322594165802\n",
            "Epoch 612: train loss: 0.40245968103408813\n",
            "Epoch 613: train loss: 0.4498387575149536\n",
            "Epoch 614: train loss: 0.4036445617675781\n",
            "Epoch 615: train loss: 0.45139753818511963\n",
            "Epoch 616: train loss: 0.4040660560131073\n",
            "Epoch 617: train loss: 0.4512292146682739\n",
            "Epoch 618: train loss: 0.4031709134578705\n",
            "Epoch 619: train loss: 0.4493809938430786\n",
            "Epoch 620: train loss: 0.4020240008831024\n",
            "Epoch 621: train loss: 0.44833898544311523\n",
            "Epoch 622: train loss: 0.40235844254493713\n",
            "Epoch 623: train loss: 0.44956642389297485\n",
            "Epoch 624: train loss: 0.40329793095588684\n",
            "Epoch 625: train loss: 0.45085379481315613\n",
            "Epoch 626: train loss: 0.40389323234558105\n",
            "Epoch 627: train loss: 0.4511871039867401\n",
            "Epoch 628: train loss: 0.40326640009880066\n",
            "Epoch 629: train loss: 0.44974783062934875\n",
            "Epoch 630: train loss: 0.4023336172103882\n",
            "Epoch 631: train loss: 0.44862544536590576\n",
            "Epoch 632: train loss: 0.4021884799003601\n",
            "Epoch 633: train loss: 0.4492090940475464\n",
            "Epoch 634: train loss: 0.40311649441719055\n",
            "Epoch 635: train loss: 0.45064523816108704\n",
            "Epoch 636: train loss: 0.40369912981987\n",
            "Epoch 637: train loss: 0.45100468397140503\n",
            "Epoch 638: train loss: 0.4034290015697479\n",
            "Epoch 639: train loss: 0.4500594139099121\n",
            "Epoch 640: train loss: 0.40232789516448975\n",
            "Epoch 641: train loss: 0.44861140847206116\n",
            "Epoch 642: train loss: 0.4022292494773865\n",
            "Epoch 643: train loss: 0.4491865634918213\n",
            "Epoch 644: train loss: 0.40287667512893677\n",
            "Epoch 645: train loss: 0.4502985179424286\n",
            "Epoch 646: train loss: 0.403642475605011\n",
            "Epoch 647: train loss: 0.45101648569107056\n",
            "Epoch 648: train loss: 0.40345636010169983\n",
            "Epoch 649: train loss: 0.4502188265323639\n",
            "Epoch 650: train loss: 0.4026530086994171\n",
            "Epoch 651: train loss: 0.44896814227104187\n",
            "Epoch 652: train loss: 0.40199825167655945\n",
            "Epoch 653: train loss: 0.4486202001571655\n",
            "Epoch 654: train loss: 0.40258994698524475\n",
            "Epoch 655: train loss: 0.4500088095664978\n",
            "Epoch 656: train loss: 0.4034944176673889\n",
            "Epoch 657: train loss: 0.4509601593017578\n",
            "Epoch 658: train loss: 0.40363335609436035\n",
            "Epoch 659: train loss: 0.450592964887619\n",
            "Epoch 660: train loss: 0.40285107493400574\n",
            "Epoch 661: train loss: 0.44933927059173584\n",
            "Epoch 662: train loss: 0.40206894278526306\n",
            "Epoch 663: train loss: 0.44830599427223206\n",
            "Epoch 664: train loss: 0.4022158980369568\n",
            "Epoch 665: train loss: 0.4496268332004547\n",
            "Epoch 666: train loss: 0.4033716917037964\n",
            "Epoch 667: train loss: 0.4506947100162506\n",
            "Epoch 668: train loss: 0.4036562740802765\n",
            "Epoch 669: train loss: 0.450966477394104\n",
            "Epoch 670: train loss: 0.40301090478897095\n",
            "Epoch 671: train loss: 0.44907745718955994\n",
            "Epoch 672: train loss: 0.40186917781829834\n",
            "Epoch 673: train loss: 0.4484303891658783\n",
            "Epoch 674: train loss: 0.4022037088871002\n",
            "Epoch 675: train loss: 0.4491824209690094\n",
            "Epoch 676: train loss: 0.403181791305542\n",
            "Epoch 677: train loss: 0.4510952830314636\n",
            "Epoch 678: train loss: 0.40384456515312195\n",
            "Epoch 679: train loss: 0.4509276747703552\n",
            "Epoch 680: train loss: 0.4031197130680084\n",
            "Epoch 681: train loss: 0.44969820976257324\n",
            "Epoch 682: train loss: 0.40203991532325745\n",
            "Epoch 683: train loss: 0.44797948002815247\n",
            "Epoch 684: train loss: 0.40180468559265137\n",
            "Epoch 685: train loss: 0.4490368962287903\n",
            "Epoch 686: train loss: 0.4029649794101715\n",
            "Epoch 687: train loss: 0.45028141140937805\n",
            "Epoch 688: train loss: 0.40365293622016907\n",
            "Epoch 689: train loss: 0.4514302611351013\n",
            "Epoch 690: train loss: 0.40342944860458374\n",
            "Epoch 691: train loss: 0.44962969422340393\n",
            "Epoch 692: train loss: 0.40203675627708435\n",
            "Epoch 693: train loss: 0.44858571887016296\n",
            "Epoch 694: train loss: 0.4019193947315216\n",
            "Epoch 695: train loss: 0.4482967257499695\n",
            "Epoch 696: train loss: 0.40249407291412354\n",
            "Epoch 697: train loss: 0.4504198729991913\n",
            "Epoch 698: train loss: 0.40379732847213745\n",
            "Epoch 699: train loss: 0.45097634196281433\n",
            "Epoch 700: train loss: 0.4033418893814087\n",
            "Epoch 701: train loss: 0.4503593146800995\n",
            "Epoch 702: train loss: 0.40235063433647156\n",
            "Epoch 703: train loss: 0.4480961263179779\n",
            "Epoch 704: train loss: 0.40155985951423645\n",
            "Epoch 705: train loss: 0.4486143887042999\n",
            "Epoch 706: train loss: 0.40262413024902344\n",
            "Epoch 707: train loss: 0.4498034417629242\n",
            "Epoch 708: train loss: 0.40339595079421997\n",
            "Epoch 709: train loss: 0.4513031542301178\n",
            "Epoch 710: train loss: 0.40353265404701233\n",
            "Epoch 711: train loss: 0.44988468289375305\n",
            "Epoch 712: train loss: 0.40220603346824646\n",
            "Epoch 713: train loss: 0.4488580822944641\n",
            "Epoch 714: train loss: 0.4019213318824768\n",
            "Epoch 715: train loss: 0.4481486678123474\n",
            "Epoch 716: train loss: 0.40220388770103455\n",
            "Epoch 717: train loss: 0.44993463158607483\n",
            "Epoch 718: train loss: 0.40341970324516296\n",
            "Epoch 719: train loss: 0.4505442678928375\n",
            "Epoch 720: train loss: 0.4032999873161316\n",
            "Epoch 721: train loss: 0.45059821009635925\n",
            "Epoch 722: train loss: 0.4027341902256012\n",
            "Epoch 723: train loss: 0.44864773750305176\n",
            "Epoch 724: train loss: 0.401642382144928\n",
            "Epoch 725: train loss: 0.44846224784851074\n",
            "Epoch 726: train loss: 0.40217792987823486\n",
            "Epoch 727: train loss: 0.4489564597606659\n",
            "Epoch 728: train loss: 0.40286508202552795\n",
            "Epoch 729: train loss: 0.4508698284626007\n",
            "Epoch 730: train loss: 0.403685599565506\n",
            "Epoch 731: train loss: 0.45049458742141724\n",
            "Epoch 732: train loss: 0.40272417664527893\n",
            "Epoch 733: train loss: 0.4494793713092804\n",
            "Epoch 734: train loss: 0.401974618434906\n",
            "Epoch 735: train loss: 0.44782423973083496\n",
            "Epoch 736: train loss: 0.4016757607460022\n",
            "Epoch 737: train loss: 0.4490746855735779\n",
            "Epoch 738: train loss: 0.40286996960639954\n",
            "Epoch 739: train loss: 0.45006197690963745\n",
            "Epoch 740: train loss: 0.4033091366291046\n",
            "Epoch 741: train loss: 0.45100030303001404\n",
            "Epoch 742: train loss: 0.40318772196769714\n",
            "Epoch 743: train loss: 0.449383944272995\n",
            "Epoch 744: train loss: 0.401885449886322\n",
            "Epoch 745: train loss: 0.44848719239234924\n",
            "Epoch 746: train loss: 0.4017907679080963\n",
            "Epoch 747: train loss: 0.44816964864730835\n",
            "Epoch 748: train loss: 0.4023033082485199\n",
            "Epoch 749: train loss: 0.4501747190952301\n",
            "Epoch 750: train loss: 0.403502881526947\n",
            "Epoch 751: train loss: 0.45063188672065735\n",
            "Epoch 752: train loss: 0.4031229019165039\n",
            "Epoch 753: train loss: 0.45019540190696716\n",
            "Epoch 754: train loss: 0.4022223949432373\n",
            "Epoch 755: train loss: 0.4479946196079254\n",
            "Epoch 756: train loss: 0.4014167785644531\n",
            "Epoch 757: train loss: 0.448457807302475\n",
            "Epoch 758: train loss: 0.4024120271205902\n",
            "Epoch 759: train loss: 0.44943714141845703\n",
            "Epoch 760: train loss: 0.40305429697036743\n",
            "Epoch 761: train loss: 0.4509522318840027\n",
            "Epoch 762: train loss: 0.40336543321609497\n",
            "Epoch 763: train loss: 0.44990959763526917\n",
            "Epoch 764: train loss: 0.4021761417388916\n",
            "Epoch 765: train loss: 0.44878247380256653\n",
            "Epoch 766: train loss: 0.40177905559539795\n",
            "Epoch 767: train loss: 0.44791755080223083\n",
            "Epoch 768: train loss: 0.4018578827381134\n",
            "Epoch 769: train loss: 0.4494839608669281\n",
            "Epoch 770: train loss: 0.40307238698005676\n",
            "Epoch 771: train loss: 0.4502169191837311\n",
            "Epoch 772: train loss: 0.40316328406333923\n",
            "Epoch 773: train loss: 0.45058679580688477\n",
            "Epoch 774: train loss: 0.40267178416252136\n",
            "Epoch 775: train loss: 0.4485943615436554\n",
            "Epoch 776: train loss: 0.40154826641082764\n",
            "Epoch 777: train loss: 0.4483402371406555\n",
            "Epoch 778: train loss: 0.4018748700618744\n",
            "Epoch 779: train loss: 0.4485149681568146\n",
            "Epoch 780: train loss: 0.40255793929100037\n",
            "Epoch 781: train loss: 0.45048704743385315\n",
            "Epoch 782: train loss: 0.4034729301929474\n",
            "Epoch 783: train loss: 0.4504367709159851\n",
            "Epoch 784: train loss: 0.4027191698551178\n",
            "Epoch 785: train loss: 0.449540913105011\n",
            "Epoch 786: train loss: 0.4018327593803406\n",
            "Epoch 787: train loss: 0.4475441873073578\n",
            "Epoch 788: train loss: 0.4013012945652008\n",
            "Epoch 789: train loss: 0.4485880732536316\n",
            "Epoch 790: train loss: 0.40259188413619995\n",
            "Epoch 791: train loss: 0.4497641921043396\n",
            "Epoch 792: train loss: 0.40314725041389465\n",
            "Epoch 793: train loss: 0.4508568048477173\n",
            "Epoch 794: train loss: 0.40307626128196716\n",
            "Epoch 795: train loss: 0.44938895106315613\n",
            "Epoch 796: train loss: 0.4017825722694397\n",
            "Epoch 797: train loss: 0.4483424723148346\n",
            "Epoch 798: train loss: 0.4014955461025238\n",
            "Epoch 799: train loss: 0.44775813817977905\n",
            "Epoch 800: train loss: 0.40196090936660767\n",
            "Epoch 801: train loss: 0.4498278796672821\n",
            "Epoch 802: train loss: 0.40327468514442444\n",
            "Epoch 803: train loss: 0.45056989789009094\n",
            "Epoch 804: train loss: 0.403107225894928\n",
            "Epoch 805: train loss: 0.45042502880096436\n",
            "Epoch 806: train loss: 0.4023122191429138\n",
            "Epoch 807: train loss: 0.44792214035987854\n",
            "Epoch 808: train loss: 0.40101367235183716\n",
            "Epoch 809: train loss: 0.4477952718734741\n",
            "Epoch 810: train loss: 0.40178799629211426\n",
            "Epoch 811: train loss: 0.44873982667922974\n",
            "Epoch 812: train loss: 0.40281733870506287\n",
            "Epoch 813: train loss: 0.45092669129371643\n",
            "Epoch 814: train loss: 0.40359416604042053\n",
            "Epoch 815: train loss: 0.4504022002220154\n",
            "Epoch 816: train loss: 0.40236297249794006\n",
            "Epoch 817: train loss: 0.4487941861152649\n",
            "Epoch 818: train loss: 0.4012765884399414\n",
            "Epoch 819: train loss: 0.44692593812942505\n",
            "Epoch 820: train loss: 0.4011150598526001\n",
            "Epoch 821: train loss: 0.4487594664096832\n",
            "Epoch 822: train loss: 0.402927964925766\n",
            "Epoch 823: train loss: 0.4503248333930969\n",
            "Epoch 824: train loss: 0.40338435769081116\n",
            "Epoch 825: train loss: 0.45118221640586853\n",
            "Epoch 826: train loss: 0.4028730094432831\n",
            "Epoch 827: train loss: 0.4487950801849365\n",
            "Epoch 828: train loss: 0.40119078755378723\n",
            "Epoch 829: train loss: 0.4475136399269104\n",
            "Epoch 830: train loss: 0.40107962489128113\n",
            "Epoch 831: train loss: 0.447491854429245\n",
            "Epoch 832: train loss: 0.402108371257782\n",
            "Epoch 833: train loss: 0.4503445029258728\n",
            "Epoch 834: train loss: 0.4036751687526703\n",
            "Epoch 835: train loss: 0.4510282278060913\n",
            "Epoch 836: train loss: 0.4030503034591675\n",
            "Epoch 837: train loss: 0.45001882314682007\n",
            "Epoch 838: train loss: 0.4016810357570648\n",
            "Epoch 839: train loss: 0.4469064772129059\n",
            "Epoch 840: train loss: 0.40054669976234436\n",
            "Epoch 841: train loss: 0.4476920962333679\n",
            "Epoch 842: train loss: 0.4021753668785095\n",
            "Epoch 843: train loss: 0.4495541751384735\n",
            "Epoch 844: train loss: 0.40325483679771423\n",
            "Epoch 845: train loss: 0.4512886106967926\n",
            "Epoch 846: train loss: 0.4032290577888489\n",
            "Epoch 847: train loss: 0.4494176208972931\n",
            "Epoch 848: train loss: 0.4015722870826721\n",
            "Epoch 849: train loss: 0.4480574429035187\n",
            "Epoch 850: train loss: 0.4011012315750122\n",
            "Epoch 851: train loss: 0.4470738172531128\n",
            "Epoch 852: train loss: 0.401405930519104\n",
            "Epoch 853: train loss: 0.4494273364543915\n",
            "Epoch 854: train loss: 0.4032626152038574\n",
            "Epoch 855: train loss: 0.4507816433906555\n",
            "Epoch 856: train loss: 0.4032212793827057\n",
            "Epoch 857: train loss: 0.45047664642333984\n",
            "Epoch 858: train loss: 0.40207234025001526\n",
            "Epoch 859: train loss: 0.4474727213382721\n",
            "Epoch 860: train loss: 0.40058526396751404\n",
            "Epoch 861: train loss: 0.4475509822368622\n",
            "Epoch 862: train loss: 0.40177294611930847\n",
            "Epoch 863: train loss: 0.4488682746887207\n",
            "Epoch 864: train loss: 0.40277260541915894\n",
            "Epoch 865: train loss: 0.45075222849845886\n",
            "Epoch 866: train loss: 0.4032023251056671\n",
            "Epoch 867: train loss: 0.4497360289096832\n",
            "Epoch 868: train loss: 0.4019179344177246\n",
            "Epoch 869: train loss: 0.4485882520675659\n",
            "Epoch 870: train loss: 0.4012957215309143\n",
            "Epoch 871: train loss: 0.4471926987171173\n",
            "Epoch 872: train loss: 0.4011402726173401\n",
            "Epoch 873: train loss: 0.4486619830131531\n",
            "Epoch 874: train loss: 0.402656227350235\n",
            "Epoch 875: train loss: 0.450032114982605\n",
            "Epoch 876: train loss: 0.4030081033706665\n",
            "Epoch 877: train loss: 0.4505455493927002\n",
            "Epoch 878: train loss: 0.4024621248245239\n",
            "Epoch 879: train loss: 0.44831568002700806\n",
            "Epoch 880: train loss: 0.4010201394557953\n",
            "Epoch 881: train loss: 0.4476175606250763\n",
            "Epoch 882: train loss: 0.4012613594532013\n",
            "Epoch 883: train loss: 0.44795161485671997\n",
            "Epoch 884: train loss: 0.4021513760089874\n",
            "Epoch 885: train loss: 0.4499189555644989\n",
            "Epoch 886: train loss: 0.40310752391815186\n",
            "Epoch 887: train loss: 0.4504307508468628\n",
            "Epoch 888: train loss: 0.40253326296806335\n",
            "Epoch 889: train loss: 0.4489912688732147\n",
            "Epoch 890: train loss: 0.40140601992607117\n",
            "Epoch 891: train loss: 0.4474773108959198\n",
            "Epoch 892: train loss: 0.4008370339870453\n",
            "Epoch 893: train loss: 0.44750434160232544\n",
            "Epoch 894: train loss: 0.4018102288246155\n",
            "Epoch 895: train loss: 0.4494096636772156\n",
            "Epoch 896: train loss: 0.4030422866344452\n",
            "Epoch 897: train loss: 0.45071858167648315\n",
            "Epoch 898: train loss: 0.4028816521167755\n",
            "Epoch 899: train loss: 0.4495312571525574\n",
            "Epoch 900: train loss: 0.4015202820301056\n",
            "Epoch 901: train loss: 0.4474220871925354\n",
            "Epoch 902: train loss: 0.40059709548950195\n",
            "Epoch 903: train loss: 0.4470643401145935\n",
            "Epoch 904: train loss: 0.40138933062553406\n",
            "Epoch 905: train loss: 0.44896841049194336\n",
            "Epoch 906: train loss: 0.402864933013916\n",
            "Epoch 907: train loss: 0.4507002532482147\n",
            "Epoch 908: train loss: 0.40322938561439514\n",
            "Epoch 909: train loss: 0.45020973682403564\n",
            "Epoch 910: train loss: 0.40195125341415405\n",
            "Epoch 911: train loss: 0.44785162806510925\n",
            "Epoch 912: train loss: 0.40045762062072754\n",
            "Epoch 913: train loss: 0.4465414583683014\n",
            "Epoch 914: train loss: 0.4007699191570282\n",
            "Epoch 915: train loss: 0.4480946660041809\n",
            "Epoch 916: train loss: 0.40266355872154236\n",
            "Epoch 917: train loss: 0.45080065727233887\n",
            "Epoch 918: train loss: 0.4035676419734955\n",
            "Epoch 919: train loss: 0.4508371651172638\n",
            "Epoch 920: train loss: 0.40222302079200745\n",
            "Epoch 921: train loss: 0.4480345547199249\n",
            "Epoch 922: train loss: 0.40027421712875366\n",
            "Epoch 923: train loss: 0.4460810720920563\n",
            "Epoch 924: train loss: 0.4004689157009125\n",
            "Epoch 925: train loss: 0.44768843054771423\n",
            "Epoch 926: train loss: 0.40248772501945496\n",
            "Epoch 927: train loss: 0.4507301449775696\n",
            "Epoch 928: train loss: 0.40369805693626404\n",
            "Epoch 929: train loss: 0.45113274455070496\n",
            "Epoch 930: train loss: 0.402578204870224\n",
            "Epoch 931: train loss: 0.4486173987388611\n",
            "Epoch 932: train loss: 0.40063947439193726\n",
            "Epoch 933: train loss: 0.4461185038089752\n",
            "Epoch 934: train loss: 0.39987730979919434\n",
            "Epoch 935: train loss: 0.44660264253616333\n",
            "Epoch 936: train loss: 0.40169742703437805\n",
            "Epoch 937: train loss: 0.44989094138145447\n",
            "Epoch 938: train loss: 0.4037686586380005\n",
            "Epoch 939: train loss: 0.4518958032131195\n",
            "Epoch 940: train loss: 0.40334537625312805\n",
            "Epoch 941: train loss: 0.4496614634990692\n",
            "Epoch 942: train loss: 0.40086114406585693\n",
            "Epoch 943: train loss: 0.44611743092536926\n",
            "Epoch 944: train loss: 0.39945846796035767\n",
            "Epoch 945: train loss: 0.44563987851142883\n",
            "Epoch 946: train loss: 0.4009627103805542\n",
            "Epoch 947: train loss: 0.4491657018661499\n",
            "Epoch 948: train loss: 0.40364402532577515\n",
            "Epoch 949: train loss: 0.4521460235118866\n",
            "Epoch 950: train loss: 0.40384742617607117\n",
            "Epoch 951: train loss: 0.4507211744785309\n",
            "Epoch 952: train loss: 0.4014231264591217\n",
            "Epoch 953: train loss: 0.4465125501155853\n",
            "Epoch 954: train loss: 0.3992197513580322\n",
            "Epoch 955: train loss: 0.4449472725391388\n",
            "Epoch 956: train loss: 0.4002251923084259\n",
            "Epoch 957: train loss: 0.44802168011665344\n",
            "Epoch 958: train loss: 0.40321266651153564\n",
            "Epoch 959: train loss: 0.4521162807941437\n",
            "Epoch 960: train loss: 0.40440264344215393\n",
            "Epoch 961: train loss: 0.451655775308609\n",
            "Epoch 962: train loss: 0.4021143615245819\n",
            "Epoch 963: train loss: 0.44738879799842834\n",
            "Epoch 964: train loss: 0.3992518484592438\n",
            "Epoch 965: train loss: 0.4442184865474701\n",
            "Epoch 966: train loss: 0.39931726455688477\n",
            "Epoch 967: train loss: 0.4468976557254791\n",
            "Epoch 968: train loss: 0.4027016758918762\n",
            "Epoch 969: train loss: 0.4516729712486267\n",
            "Epoch 970: train loss: 0.4047577977180481\n",
            "Epoch 971: train loss: 0.4529563784599304\n",
            "Epoch 972: train loss: 0.4029422998428345\n",
            "Epoch 973: train loss: 0.44799351692199707\n",
            "Epoch 974: train loss: 0.39921504259109497\n",
            "Epoch 975: train loss: 0.4440036714076996\n",
            "Epoch 976: train loss: 0.39870715141296387\n",
            "Epoch 977: train loss: 0.44539177417755127\n",
            "Epoch 978: train loss: 0.4018455445766449\n",
            "Epoch 979: train loss: 0.45136943459510803\n",
            "Epoch 980: train loss: 0.4051055312156677\n",
            "Epoch 981: train loss: 0.4534794092178345\n",
            "Epoch 982: train loss: 0.4035407602787018\n",
            "Epoch 983: train loss: 0.44935229420661926\n",
            "Epoch 984: train loss: 0.3997809886932373\n",
            "Epoch 985: train loss: 0.4439318776130676\n",
            "Epoch 986: train loss: 0.3981412351131439\n",
            "Epoch 987: train loss: 0.4445626139640808\n",
            "Epoch 988: train loss: 0.4009053111076355\n",
            "Epoch 989: train loss: 0.4496721029281616\n",
            "Epoch 990: train loss: 0.4046372175216675\n",
            "Epoch 991: train loss: 0.454097718000412\n",
            "Epoch 992: train loss: 0.40443652868270874\n",
            "Epoch 993: train loss: 0.45043742656707764\n",
            "Epoch 994: train loss: 0.4002916216850281\n",
            "Epoch 995: train loss: 0.444623202085495\n",
            "Epoch 996: train loss: 0.3979550898075104\n",
            "Epoch 997: train loss: 0.44357243180274963\n",
            "Epoch 998: train loss: 0.4001111090183258\n",
            "Epoch 999: train loss: 0.4488556683063507\n",
            "Epoch 1000: train loss: 0.404407799243927\n",
            "Epoch 1001: train loss: 0.45394954085350037\n",
            "Epoch 1002: train loss: 0.40483003854751587\n",
            "Epoch 1003: train loss: 0.4515857994556427\n",
            "Epoch 1004: train loss: 0.40123870968818665\n",
            "Epoch 1005: train loss: 0.445474237203598\n",
            "Epoch 1006: train loss: 0.39771217107772827\n",
            "Epoch 1007: train loss: 0.4425981640815735\n",
            "Epoch 1008: train loss: 0.3990471661090851\n",
            "Epoch 1009: train loss: 0.4472636878490448\n",
            "Epoch 1010: train loss: 0.4035697877407074\n",
            "Epoch 1011: train loss: 0.45348164439201355\n",
            "Epoch 1012: train loss: 0.40542393922805786\n",
            "Epoch 1013: train loss: 0.4531043469905853\n",
            "Epoch 1014: train loss: 0.4023662805557251\n",
            "Epoch 1015: train loss: 0.446941614151001\n",
            "Epoch 1016: train loss: 0.39809122681617737\n",
            "Epoch 1017: train loss: 0.4422041177749634\n",
            "Epoch 1018: train loss: 0.3980576694011688\n",
            "Epoch 1019: train loss: 0.44567909836769104\n",
            "Epoch 1020: train loss: 0.4026045501232147\n",
            "Epoch 1021: train loss: 0.4522815942764282\n",
            "Epoch 1022: train loss: 0.4054073989391327\n",
            "Epoch 1023: train loss: 0.4542057514190674\n",
            "Epoch 1024: train loss: 0.4033360183238983\n",
            "Epoch 1025: train loss: 0.4480634033679962\n",
            "Epoch 1026: train loss: 0.398637592792511\n",
            "Epoch 1027: train loss: 0.44274550676345825\n",
            "Epoch 1028: train loss: 0.3977028727531433\n",
            "Epoch 1029: train loss: 0.44435280561447144\n",
            "Epoch 1030: train loss: 0.40146178007125854\n",
            "Epoch 1031: train loss: 0.45099180936813354\n",
            "Epoch 1032: train loss: 0.4052494764328003\n",
            "Epoch 1033: train loss: 0.45441094040870667\n",
            "Epoch 1034: train loss: 0.40404221415519714\n",
            "Epoch 1035: train loss: 0.4495414197444916\n",
            "Epoch 1036: train loss: 0.3993397355079651\n",
            "Epoch 1037: train loss: 0.4434119462966919\n",
            "Epoch 1038: train loss: 0.3974721431732178\n",
            "Epoch 1039: train loss: 0.4432102143764496\n",
            "Epoch 1040: train loss: 0.40024933218955994\n",
            "Epoch 1041: train loss: 0.4496701657772064\n",
            "Epoch 1042: train loss: 0.4049859642982483\n",
            "Epoch 1043: train loss: 0.45449209213256836\n",
            "Epoch 1044: train loss: 0.40478697419166565\n",
            "Epoch 1045: train loss: 0.4513104557991028\n",
            "Epoch 1046: train loss: 0.4005354344844818\n",
            "Epoch 1047: train loss: 0.4441547095775604\n",
            "Epoch 1048: train loss: 0.3970135748386383\n",
            "Epoch 1049: train loss: 0.4421486556529999\n",
            "Epoch 1050: train loss: 0.39905020594596863\n",
            "Epoch 1051: train loss: 0.4474392831325531\n",
            "Epoch 1052: train loss: 0.4041132628917694\n",
            "Epoch 1053: train loss: 0.45472830533981323\n",
            "Epoch 1054: train loss: 0.4057638645172119\n",
            "Epoch 1055: train loss: 0.4525682330131531\n",
            "Epoch 1056: train loss: 0.40128809213638306\n",
            "Epoch 1057: train loss: 0.44557294249534607\n",
            "Epoch 1058: train loss: 0.39735305309295654\n",
            "Epoch 1059: train loss: 0.4412308931350708\n",
            "Epoch 1060: train loss: 0.39790210127830505\n",
            "Epoch 1061: train loss: 0.44630491733551025\n",
            "Epoch 1062: train loss: 0.4034123718738556\n",
            "Epoch 1063: train loss: 0.45361459255218506\n",
            "Epoch 1064: train loss: 0.40583959221839905\n",
            "Epoch 1065: train loss: 0.45405837893486023\n",
            "Epoch 1066: train loss: 0.40253394842147827\n",
            "Epoch 1067: train loss: 0.4466931223869324\n",
            "Epoch 1068: train loss: 0.3975442349910736\n",
            "Epoch 1069: train loss: 0.44133856892585754\n",
            "Epoch 1070: train loss: 0.3972785770893097\n",
            "Epoch 1071: train loss: 0.4444407522678375\n",
            "Epoch 1072: train loss: 0.4021340012550354\n",
            "Epoch 1073: train loss: 0.4523959159851074\n",
            "Epoch 1074: train loss: 0.4058130383491516\n",
            "Epoch 1075: train loss: 0.4547414779663086\n",
            "Epoch 1076: train loss: 0.40357786417007446\n",
            "Epoch 1077: train loss: 0.44855210185050964\n",
            "Epoch 1078: train loss: 0.39847585558891296\n",
            "Epoch 1079: train loss: 0.4419630467891693\n",
            "Epoch 1080: train loss: 0.3967891335487366\n",
            "Epoch 1081: train loss: 0.44306832551956177\n",
            "Epoch 1082: train loss: 0.4007512927055359\n",
            "Epoch 1083: train loss: 0.45062580704689026\n",
            "Epoch 1084: train loss: 0.4054849147796631\n",
            "Epoch 1085: train loss: 0.4550425708293915\n",
            "Epoch 1086: train loss: 0.4044557809829712\n",
            "Epoch 1087: train loss: 0.45023396611213684\n",
            "Epoch 1088: train loss: 0.3993888199329376\n",
            "Epoch 1089: train loss: 0.44276341795921326\n",
            "Epoch 1090: train loss: 0.39654913544654846\n",
            "Epoch 1091: train loss: 0.4420764744281769\n",
            "Epoch 1092: train loss: 0.39948025345802307\n",
            "Epoch 1093: train loss: 0.4486805200576782\n",
            "Epoch 1094: train loss: 0.40487679839134216\n",
            "Epoch 1095: train loss: 0.45510029792785645\n",
            "Epoch 1096: train loss: 0.40536054968833923\n",
            "Epoch 1097: train loss: 0.4519925117492676\n",
            "Epoch 1098: train loss: 0.4005524814128876\n",
            "Epoch 1099: train loss: 0.4439455568790436\n",
            "Epoch 1100: train loss: 0.39638274908065796\n",
            "Epoch 1101: train loss: 0.44088509678840637\n",
            "Epoch 1102: train loss: 0.39819666743278503\n",
            "Epoch 1103: train loss: 0.44674089550971985\n",
            "Epoch 1104: train loss: 0.40380051732063293\n",
            "Epoch 1105: train loss: 0.45442402362823486\n",
            "Epoch 1106: train loss: 0.4060263931751251\n",
            "Epoch 1107: train loss: 0.45377251505851746\n",
            "Epoch 1108: train loss: 0.40193188190460205\n",
            "Epoch 1109: train loss: 0.44577738642692566\n",
            "Epoch 1110: train loss: 0.3966827094554901\n",
            "Epoch 1111: train loss: 0.44019922614097595\n",
            "Epoch 1112: train loss: 0.39775869250297546\n",
            "Epoch 1113: train loss: 0.4457991421222687\n",
            "Epoch 1114: train loss: 0.40287578105926514\n",
            "Epoch 1115: train loss: 0.4531457722187042\n",
            "Epoch 1116: train loss: 0.4055577516555786\n",
            "Epoch 1117: train loss: 0.45365381240844727\n",
            "Epoch 1118: train loss: 0.40242817997932434\n",
            "Epoch 1119: train loss: 0.4469633996486664\n",
            "Epoch 1120: train loss: 0.39775657653808594\n",
            "Epoch 1121: train loss: 0.4414553642272949\n",
            "Epoch 1122: train loss: 0.39701199531555176\n",
            "Epoch 1123: train loss: 0.4440120756626129\n",
            "Epoch 1124: train loss: 0.40167802572250366\n",
            "Epoch 1125: train loss: 0.45182937383651733\n",
            "Epoch 1126: train loss: 0.4055115282535553\n",
            "Epoch 1127: train loss: 0.45447632670402527\n",
            "Epoch 1128: train loss: 0.403571218252182\n",
            "Epoch 1129: train loss: 0.4486386477947235\n",
            "Epoch 1130: train loss: 0.3983041048049927\n",
            "Epoch 1131: train loss: 0.441697359085083\n",
            "Epoch 1132: train loss: 0.3964909613132477\n",
            "Epoch 1133: train loss: 0.4427644908428192\n",
            "Epoch 1134: train loss: 0.40055447816848755\n",
            "Epoch 1135: train loss: 0.45040470361709595\n",
            "Epoch 1136: train loss: 0.4053102731704712\n",
            "Epoch 1137: train loss: 0.4549267590045929\n",
            "Epoch 1138: train loss: 0.40443307161331177\n",
            "Epoch 1139: train loss: 0.4502653479576111\n",
            "Epoch 1140: train loss: 0.3991777300834656\n",
            "Epoch 1141: train loss: 0.4423445165157318\n",
            "Epoch 1142: train loss: 0.39608511328697205\n",
            "Epoch 1143: train loss: 0.44150635600090027\n",
            "Epoch 1144: train loss: 0.39920151233673096\n",
            "Epoch 1145: train loss: 0.4484870731830597\n",
            "Epoch 1146: train loss: 0.40490061044692993\n",
            "Epoch 1147: train loss: 0.4553639888763428\n",
            "Epoch 1148: train loss: 0.4053511321544647\n",
            "Epoch 1149: train loss: 0.4518479108810425\n",
            "Epoch 1150: train loss: 0.40022265911102295\n",
            "Epoch 1151: train loss: 0.4435533285140991\n",
            "Epoch 1152: train loss: 0.39601847529411316\n",
            "Epoch 1153: train loss: 0.4403775930404663\n",
            "Epoch 1154: train loss: 0.39783889055252075\n",
            "Epoch 1155: train loss: 0.4464886784553528\n",
            "Epoch 1156: train loss: 0.4037943482398987\n",
            "Epoch 1157: train loss: 0.4546644985675812\n",
            "Epoch 1158: train loss: 0.40601423382759094\n",
            "Epoch 1159: train loss: 0.4536997973918915\n",
            "Epoch 1160: train loss: 0.4016263484954834\n",
            "Epoch 1161: train loss: 0.44529518485069275\n",
            "Epoch 1162: train loss: 0.3963963985443115\n",
            "Epoch 1163: train loss: 0.4398907721042633\n",
            "Epoch 1164: train loss: 0.39750802516937256\n",
            "Epoch 1165: train loss: 0.4455861747264862\n",
            "Epoch 1166: train loss: 0.4027487337589264\n",
            "Epoch 1167: train loss: 0.4531063139438629\n",
            "Epoch 1168: train loss: 0.40544095635414124\n",
            "Epoch 1169: train loss: 0.4535626769065857\n",
            "Epoch 1170: train loss: 0.40214550495147705\n",
            "Epoch 1171: train loss: 0.4464828670024872\n",
            "Epoch 1172: train loss: 0.39737802743911743\n",
            "Epoch 1173: train loss: 0.4409995973110199\n",
            "Epoch 1174: train loss: 0.3975253999233246\n",
            "Epoch 1175: train loss: 0.44489338994026184\n",
            "Epoch 1176: train loss: 0.40166035294532776\n",
            "Epoch 1177: train loss: 0.451290100812912\n",
            "Epoch 1178: train loss: 0.4046333134174347\n",
            "Epoch 1179: train loss: 0.45304393768310547\n",
            "Epoch 1180: train loss: 0.4026622474193573\n",
            "Epoch 1181: train loss: 0.447904497385025\n",
            "Epoch 1182: train loss: 0.3985103964805603\n",
            "Epoch 1183: train loss: 0.4424392580986023\n",
            "Epoch 1184: train loss: 0.3970819115638733\n",
            "Epoch 1185: train loss: 0.4435391426086426\n",
            "Epoch 1186: train loss: 0.40071338415145874\n",
            "Epoch 1187: train loss: 0.45006418228149414\n",
            "Epoch 1188: train loss: 0.4044867753982544\n",
            "Epoch 1189: train loss: 0.4535233676433563\n",
            "Epoch 1190: train loss: 0.40348801016807556\n",
            "Epoch 1191: train loss: 0.44918516278266907\n",
            "Epoch 1192: train loss: 0.3989756107330322\n",
            "Epoch 1193: train loss: 0.44273027777671814\n",
            "Epoch 1194: train loss: 0.39662274718284607\n",
            "Epoch 1195: train loss: 0.4423784017562866\n",
            "Epoch 1196: train loss: 0.3996151387691498\n",
            "Epoch 1197: train loss: 0.44869035482406616\n",
            "Epoch 1198: train loss: 0.4043648838996887\n",
            "Epoch 1199: train loss: 0.4541857838630676\n",
            "Epoch 1200: train loss: 0.40442565083503723\n",
            "Epoch 1201: train loss: 0.4506470859050751\n",
            "Epoch 1202: train loss: 0.3998611271381378\n",
            "Epoch 1203: train loss: 0.4435286521911621\n",
            "Epoch 1204: train loss: 0.39627206325531006\n",
            "Epoch 1205: train loss: 0.44108909368515015\n",
            "Epoch 1206: train loss: 0.3982413113117218\n",
            "Epoch 1207: train loss: 0.44683945178985596\n",
            "Epoch 1208: train loss: 0.4035985469818115\n",
            "Epoch 1209: train loss: 0.4539986550807953\n",
            "Epoch 1210: train loss: 0.4053371548652649\n",
            "Epoch 1211: train loss: 0.45274460315704346\n",
            "Epoch 1212: train loss: 0.4010886251926422\n",
            "Epoch 1213: train loss: 0.44481992721557617\n",
            "Epoch 1214: train loss: 0.39643803238868713\n",
            "Epoch 1215: train loss: 0.4403691291809082\n",
            "Epoch 1216: train loss: 0.3976982533931732\n",
            "Epoch 1217: train loss: 0.4457281529903412\n",
            "Epoch 1218: train loss: 0.4025077223777771\n",
            "Epoch 1219: train loss: 0.4525074362754822\n",
            "Epoch 1220: train loss: 0.4048824608325958\n",
            "Epoch 1221: train loss: 0.45278486609458923\n",
            "Epoch 1222: train loss: 0.40184158086776733\n",
            "Epoch 1223: train loss: 0.4463229775428772\n",
            "Epoch 1224: train loss: 0.3974420726299286\n",
            "Epoch 1225: train loss: 0.44143712520599365\n",
            "Epoch 1226: train loss: 0.3976318836212158\n",
            "Epoch 1227: train loss: 0.4448171555995941\n",
            "Epoch 1228: train loss: 0.4013708233833313\n",
            "Epoch 1229: train loss: 0.4506116509437561\n",
            "Epoch 1230: train loss: 0.40407484769821167\n",
            "Epoch 1231: train loss: 0.4524213373661041\n",
            "Epoch 1232: train loss: 0.40240755677223206\n",
            "Epoch 1233: train loss: 0.447801411151886\n",
            "Epoch 1234: train loss: 0.3985562026500702\n",
            "Epoch 1235: train loss: 0.4427703619003296\n",
            "Epoch 1236: train loss: 0.397126168012619\n",
            "Epoch 1237: train loss: 0.4434020221233368\n",
            "Epoch 1238: train loss: 0.40032875537872314\n",
            "Epoch 1239: train loss: 0.44946470856666565\n",
            "Epoch 1240: train loss: 0.40405136346817017\n",
            "Epoch 1241: train loss: 0.45309942960739136\n",
            "Epoch 1242: train loss: 0.40330496430397034\n",
            "Epoch 1243: train loss: 0.4491451680660248\n",
            "Epoch 1244: train loss: 0.3989953398704529\n",
            "Epoch 1245: train loss: 0.4428129196166992\n",
            "Epoch 1246: train loss: 0.3965899646282196\n",
            "Epoch 1247: train loss: 0.44225770235061646\n",
            "Epoch 1248: train loss: 0.39927661418914795\n",
            "Epoch 1249: train loss: 0.4481867849826813\n",
            "Epoch 1250: train loss: 0.4038921594619751\n",
            "Epoch 1251: train loss: 0.45366185903549194\n",
            "Epoch 1252: train loss: 0.40424424409866333\n",
            "Epoch 1253: train loss: 0.45068565011024475\n",
            "Epoch 1254: train loss: 0.3998330235481262\n",
            "Epoch 1255: train loss: 0.4434660077095032\n",
            "Epoch 1256: train loss: 0.3961922228336334\n",
            "Epoch 1257: train loss: 0.44104135036468506\n",
            "Epoch 1258: train loss: 0.39800912141799927\n",
            "Epoch 1259: train loss: 0.44646862149238586\n",
            "Epoch 1260: train loss: 0.4032376706600189\n",
            "Epoch 1261: train loss: 0.45358094573020935\n",
            "Epoch 1262: train loss: 0.40502095222473145\n",
            "Epoch 1263: train loss: 0.4523758888244629\n",
            "Epoch 1264: train loss: 0.4009883999824524\n",
            "Epoch 1265: train loss: 0.44482386112213135\n",
            "Epoch 1266: train loss: 0.39630499482154846\n",
            "Epoch 1267: train loss: 0.44030997157096863\n",
            "Epoch 1268: train loss: 0.39761778712272644\n",
            "Epoch 1269: train loss: 0.44560354948043823\n",
            "Epoch 1270: train loss: 0.4022116959095001\n",
            "Epoch 1271: train loss: 0.45195600390434265\n",
            "Epoch 1272: train loss: 0.404440701007843\n",
            "Epoch 1273: train loss: 0.45227956771850586\n",
            "Epoch 1274: train loss: 0.40172266960144043\n",
            "Epoch 1275: train loss: 0.4465058147907257\n",
            "Epoch 1276: train loss: 0.3976236581802368\n",
            "Epoch 1277: train loss: 0.4415545165538788\n",
            "Epoch 1278: train loss: 0.39746370911598206\n",
            "Epoch 1279: train loss: 0.4445495307445526\n",
            "Epoch 1280: train loss: 0.4010162353515625\n",
            "Epoch 1281: train loss: 0.4500584602355957\n",
            "Epoch 1282: train loss: 0.403646856546402\n",
            "Epoch 1283: train loss: 0.4519985020160675\n",
            "Epoch 1284: train loss: 0.40233632922172546\n",
            "Epoch 1285: train loss: 0.44788259267807007\n",
            "Epoch 1286: train loss: 0.39846983551979065\n",
            "Epoch 1287: train loss: 0.44270917773246765\n",
            "Epoch 1288: train loss: 0.3970690369606018\n",
            "Epoch 1289: train loss: 0.44322484731674194\n",
            "Epoch 1290: train loss: 0.39988258481025696\n",
            "Epoch 1291: train loss: 0.4488365650177002\n",
            "Epoch 1292: train loss: 0.40363574028015137\n",
            "Epoch 1293: train loss: 0.45276400446891785\n",
            "Epoch 1294: train loss: 0.4031699299812317\n",
            "Epoch 1295: train loss: 0.44912996888160706\n",
            "Epoch 1296: train loss: 0.39910590648651123\n",
            "Epoch 1297: train loss: 0.4431367516517639\n",
            "Epoch 1298: train loss: 0.3966764211654663\n",
            "Epoch 1299: train loss: 0.4422065317630768\n",
            "Epoch 1300: train loss: 0.3988982141017914\n",
            "Epoch 1301: train loss: 0.4474639296531677\n",
            "Epoch 1302: train loss: 0.40331852436065674\n",
            "Epoch 1303: train loss: 0.45288002490997314\n",
            "Epoch 1304: train loss: 0.40375420451164246\n",
            "Epoch 1305: train loss: 0.4502870440483093\n",
            "Epoch 1306: train loss: 0.40006425976753235\n",
            "Epoch 1307: train loss: 0.44417935609817505\n",
            "Epoch 1308: train loss: 0.39659324288368225\n",
            "Epoch 1309: train loss: 0.4413449168205261\n",
            "Epoch 1310: train loss: 0.39855989813804626\n",
            "Epoch 1311: train loss: 0.4468081593513489\n",
            "Epoch 1312: train loss: 0.4024542272090912\n",
            "Epoch 1313: train loss: 0.4516281187534332\n",
            "Epoch 1314: train loss: 0.4034343957901001\n",
            "Epoch 1315: train loss: 0.4504717290401459\n",
            "Epoch 1316: train loss: 0.40065327286720276\n",
            "Epoch 1317: train loss: 0.445399671792984\n",
            "Epoch 1318: train loss: 0.3974587917327881\n",
            "Epoch 1319: train loss: 0.44218313694000244\n",
            "Epoch 1320: train loss: 0.3976612389087677\n",
            "Epoch 1321: train loss: 0.444969117641449\n",
            "Epoch 1322: train loss: 0.4012698233127594\n",
            "Epoch 1323: train loss: 0.45048022270202637\n",
            "Epoch 1324: train loss: 0.403727650642395\n",
            "Epoch 1325: train loss: 0.45182105898857117\n",
            "Epoch 1326: train loss: 0.40189749002456665\n",
            "Epoch 1327: train loss: 0.44712597131729126\n",
            "Epoch 1328: train loss: 0.39796921610832214\n",
            "Epoch 1329: train loss: 0.4420132040977478\n",
            "Epoch 1330: train loss: 0.3975646197795868\n",
            "Epoch 1331: train loss: 0.444370299577713\n",
            "Epoch 1332: train loss: 0.4003947377204895\n",
            "Epoch 1333: train loss: 0.44902750849723816\n",
            "Epoch 1334: train loss: 0.40295881032943726\n",
            "Epoch 1335: train loss: 0.4510297179222107\n",
            "Epoch 1336: train loss: 0.40203219652175903\n",
            "Epoch 1337: train loss: 0.44804471731185913\n",
            "Epoch 1338: train loss: 0.3988425135612488\n",
            "Epoch 1339: train loss: 0.4432338774204254\n",
            "Epoch 1340: train loss: 0.397192120552063\n",
            "Epoch 1341: train loss: 0.4433673620223999\n",
            "Epoch 1342: train loss: 0.3996541202068329\n",
            "Epoch 1343: train loss: 0.4482055604457855\n",
            "Epoch 1344: train loss: 0.4029548764228821\n",
            "Epoch 1345: train loss: 0.45163431763648987\n",
            "Epoch 1346: train loss: 0.4026724100112915\n",
            "Epoch 1347: train loss: 0.4489760100841522\n",
            "Epoch 1348: train loss: 0.39912453293800354\n",
            "Epoch 1349: train loss: 0.4432982802391052\n",
            "Epoch 1350: train loss: 0.39687231183052063\n",
            "Epoch 1351: train loss: 0.4426339268684387\n",
            "Epoch 1352: train loss: 0.39890363812446594\n",
            "Epoch 1353: train loss: 0.4472436010837555\n",
            "Epoch 1354: train loss: 0.4027738869190216\n",
            "Epoch 1355: train loss: 0.45192545652389526\n",
            "Epoch 1356: train loss: 0.4031333923339844\n",
            "Epoch 1357: train loss: 0.44966408610343933\n",
            "Epoch 1358: train loss: 0.39991068840026855\n",
            "Epoch 1359: train loss: 0.44438230991363525\n",
            "Epoch 1360: train loss: 0.3968668282032013\n",
            "Epoch 1361: train loss: 0.44184213876724243\n",
            "Epoch 1362: train loss: 0.39788827300071716\n",
            "Epoch 1363: train loss: 0.4456605017185211\n",
            "Epoch 1364: train loss: 0.40171507000923157\n",
            "Epoch 1365: train loss: 0.45111083984375\n",
            "Epoch 1366: train loss: 0.4036282002925873\n",
            "Epoch 1367: train loss: 0.45129159092903137\n",
            "Epoch 1368: train loss: 0.4011409282684326\n",
            "Epoch 1369: train loss: 0.4457610845565796\n",
            "Epoch 1370: train loss: 0.39731743931770325\n",
            "Epoch 1371: train loss: 0.44181862473487854\n",
            "Epoch 1372: train loss: 0.39778950810432434\n",
            "Epoch 1373: train loss: 0.4448520839214325\n",
            "Epoch 1374: train loss: 0.40073180198669434\n",
            "Epoch 1375: train loss: 0.44954854249954224\n",
            "Epoch 1376: train loss: 0.40293994545936584\n",
            "Epoch 1377: train loss: 0.4507419466972351\n",
            "Epoch 1378: train loss: 0.4014771580696106\n",
            "Epoch 1379: train loss: 0.4469873905181885\n",
            "Epoch 1380: train loss: 0.39820703864097595\n",
            "Epoch 1381: train loss: 0.44264715909957886\n",
            "Epoch 1382: train loss: 0.39792564511299133\n",
            "Epoch 1383: train loss: 0.44471871852874756\n",
            "Epoch 1384: train loss: 0.40009835362434387\n",
            "Epoch 1385: train loss: 0.4482572078704834\n",
            "Epoch 1386: train loss: 0.4022236168384552\n",
            "Epoch 1387: train loss: 0.45006826519966125\n",
            "Epoch 1388: train loss: 0.4015311002731323\n",
            "Epoch 1389: train loss: 0.4473140835762024\n",
            "Epoch 1390: train loss: 0.39867866039276123\n",
            "Epoch 1391: train loss: 0.4436981678009033\n",
            "Epoch 1392: train loss: 0.397721529006958\n",
            "Epoch 1393: train loss: 0.44394776225090027\n",
            "Epoch 1394: train loss: 0.3997097909450531\n",
            "Epoch 1395: train loss: 0.4480387270450592\n",
            "Epoch 1396: train loss: 0.4023616909980774\n",
            "Epoch 1397: train loss: 0.4503946900367737\n",
            "Epoch 1398: train loss: 0.40186160802841187\n",
            "Epoch 1399: train loss: 0.4480612576007843\n",
            "Epoch 1400: train loss: 0.3989046514034271\n",
            "Epoch 1401: train loss: 0.4436296820640564\n",
            "Epoch 1402: train loss: 0.3973709046840668\n",
            "Epoch 1403: train loss: 0.44334909319877625\n",
            "Epoch 1404: train loss: 0.399228036403656\n",
            "Epoch 1405: train loss: 0.4474201202392578\n",
            "Epoch 1406: train loss: 0.40229934453964233\n",
            "Epoch 1407: train loss: 0.4507993161678314\n",
            "Epoch 1408: train loss: 0.40222594141960144\n",
            "Epoch 1409: train loss: 0.4484156668186188\n",
            "Epoch 1410: train loss: 0.39904484152793884\n",
            "Epoch 1411: train loss: 0.44375190138816833\n",
            "Epoch 1412: train loss: 0.3972579538822174\n",
            "Epoch 1413: train loss: 0.4429880380630493\n",
            "Epoch 1414: train loss: 0.39885804057121277\n",
            "Epoch 1415: train loss: 0.44698670506477356\n",
            "Epoch 1416: train loss: 0.4020253121852875\n",
            "Epoch 1417: train loss: 0.4506790339946747\n",
            "Epoch 1418: train loss: 0.40233170986175537\n",
            "Epoch 1419: train loss: 0.44875630736351013\n",
            "Epoch 1420: train loss: 0.3995816707611084\n",
            "Epoch 1421: train loss: 0.44462326169013977\n",
            "Epoch 1422: train loss: 0.39748096466064453\n",
            "Epoch 1423: train loss: 0.44283580780029297\n",
            "Epoch 1424: train loss: 0.3982601761817932\n",
            "Epoch 1425: train loss: 0.44590330123901367\n",
            "Epoch 1426: train loss: 0.40117523074150085\n",
            "Epoch 1427: train loss: 0.4497828483581543\n",
            "Epoch 1428: train loss: 0.40249544382095337\n",
            "Epoch 1429: train loss: 0.44971463084220886\n",
            "Epoch 1430: train loss: 0.4005833864212036\n",
            "Epoch 1431: train loss: 0.44563597440719604\n",
            "Epoch 1432: train loss: 0.39764225482940674\n",
            "Epoch 1433: train loss: 0.44269976019859314\n",
            "Epoch 1434: train loss: 0.398478627204895\n",
            "Epoch 1435: train loss: 0.4457162618637085\n",
            "Epoch 1436: train loss: 0.40032759308815\n",
            "Epoch 1437: train loss: 0.44814684987068176\n",
            "Epoch 1438: train loss: 0.40174123644828796\n",
            "Epoch 1439: train loss: 0.44925302267074585\n",
            "Epoch 1440: train loss: 0.40086886286735535\n",
            "Epoch 1441: train loss: 0.4467032849788666\n",
            "Epoch 1442: train loss: 0.3984925448894501\n",
            "Epoch 1443: train loss: 0.4435242712497711\n",
            "Epoch 1444: train loss: 0.39776039123535156\n",
            "Epoch 1445: train loss: 0.44439488649368286\n",
            "Epoch 1446: train loss: 0.39990174770355225\n",
            "Epoch 1447: train loss: 0.4480333626270294\n",
            "Epoch 1448: train loss: 0.4019782245159149\n",
            "Epoch 1449: train loss: 0.44971704483032227\n",
            "Epoch 1450: train loss: 0.4013198912143707\n",
            "Epoch 1451: train loss: 0.4473370313644409\n",
            "Epoch 1452: train loss: 0.39853599667549133\n",
            "Epoch 1453: train loss: 0.4432772696018219\n",
            "Epoch 1454: train loss: 0.3973354399204254\n",
            "Epoch 1455: train loss: 0.44369152188301086\n",
            "Epoch 1456: train loss: 0.39940977096557617\n",
            "Epoch 1457: train loss: 0.4476286768913269\n",
            "Epoch 1458: train loss: 0.4021253287792206\n",
            "Epoch 1459: train loss: 0.450277715921402\n",
            "Epoch 1460: train loss: 0.40163952112197876\n",
            "Epoch 1461: train loss: 0.4477716386318207\n",
            "Epoch 1462: train loss: 0.3987346887588501\n",
            "Epoch 1463: train loss: 0.44340312480926514\n",
            "Epoch 1464: train loss: 0.3978440463542938\n",
            "Epoch 1465: train loss: 0.4441508948802948\n",
            "Epoch 1466: train loss: 0.39913463592529297\n",
            "Epoch 1467: train loss: 0.44668981432914734\n",
            "Epoch 1468: train loss: 0.40091848373413086\n",
            "Epoch 1469: train loss: 0.4487062096595764\n",
            "Epoch 1470: train loss: 0.40143653750419617\n",
            "Epoch 1471: train loss: 0.4482886493206024\n",
            "Epoch 1472: train loss: 0.39973148703575134\n",
            "Epoch 1473: train loss: 0.44519636034965515\n",
            "Epoch 1474: train loss: 0.39798587560653687\n",
            "Epoch 1475: train loss: 0.4435383379459381\n",
            "Epoch 1476: train loss: 0.39836281538009644\n",
            "Epoch 1477: train loss: 0.4456617534160614\n",
            "Epoch 1478: train loss: 0.4005497097969055\n",
            "Epoch 1479: train loss: 0.44867566227912903\n",
            "Epoch 1480: train loss: 0.40177229046821594\n",
            "Epoch 1481: train loss: 0.4488585293292999\n",
            "Epoch 1482: train loss: 0.40042275190353394\n",
            "Epoch 1483: train loss: 0.44614148139953613\n",
            "Epoch 1484: train loss: 0.3980576992034912\n",
            "Epoch 1485: train loss: 0.44310101866722107\n",
            "Epoch 1486: train loss: 0.3983566164970398\n",
            "Epoch 1487: train loss: 0.445399671792984\n",
            "Epoch 1488: train loss: 0.39991676807403564\n",
            "Epoch 1489: train loss: 0.44755157828330994\n",
            "Epoch 1490: train loss: 0.4011201560497284\n",
            "Epoch 1491: train loss: 0.4483528137207031\n",
            "Epoch 1492: train loss: 0.4005588889122009\n",
            "Epoch 1493: train loss: 0.4468521773815155\n",
            "Epoch 1494: train loss: 0.3987571597099304\n",
            "Epoch 1495: train loss: 0.4440782070159912\n",
            "Epoch 1496: train loss: 0.3979671001434326\n",
            "Epoch 1497: train loss: 0.4444769024848938\n",
            "Epoch 1498: train loss: 0.39935746788978577\n",
            "Epoch 1499: train loss: 0.4470549523830414\n",
            "Epoch 1500: train loss: 0.4011651575565338\n",
            "Epoch 1501: train loss: 0.44878968596458435\n",
            "Epoch 1502: train loss: 0.40108540654182434\n",
            "Epoch 1503: train loss: 0.4476717710494995\n",
            "Epoch 1504: train loss: 0.3991054892539978\n",
            "Epoch 1505: train loss: 0.4441460072994232\n",
            "Epoch 1506: train loss: 0.3976067304611206\n",
            "Epoch 1507: train loss: 0.44399958848953247\n",
            "Epoch 1508: train loss: 0.39903056621551514\n",
            "Epoch 1509: train loss: 0.4461761713027954\n",
            "Epoch 1510: train loss: 0.40075191855430603\n",
            "Epoch 1511: train loss: 0.4493768513202667\n",
            "Epoch 1512: train loss: 0.4016912877559662\n",
            "Epoch 1513: train loss: 0.44776076078414917\n",
            "Epoch 1514: train loss: 0.3991805613040924\n",
            "Epoch 1515: train loss: 0.4450291097164154\n",
            "Epoch 1516: train loss: 0.39792147278785706\n",
            "Epoch 1517: train loss: 0.4430335462093353\n",
            "Epoch 1518: train loss: 0.39816486835479736\n",
            "Epoch 1519: train loss: 0.4461267292499542\n",
            "Epoch 1520: train loss: 0.40080925822257996\n",
            "Epoch 1521: train loss: 0.4483373761177063\n",
            "Epoch 1522: train loss: 0.40143677592277527\n",
            "Epoch 1523: train loss: 0.4493087828159332\n",
            "Epoch 1524: train loss: 0.4005010426044464\n",
            "Epoch 1525: train loss: 0.4452756345272064\n",
            "Epoch 1526: train loss: 0.39746031165122986\n",
            "Epoch 1527: train loss: 0.4431585669517517\n",
            "Epoch 1528: train loss: 0.39857736229896545\n",
            "Epoch 1529: train loss: 0.44521287083625793\n",
            "Epoch 1530: train loss: 0.399771124124527\n",
            "Epoch 1531: train loss: 0.4480697214603424\n",
            "Epoch 1532: train loss: 0.4013397693634033\n",
            "Epoch 1533: train loss: 0.4479498863220215\n",
            "Epoch 1534: train loss: 0.40000081062316895\n",
            "Epoch 1535: train loss: 0.44650977849960327\n",
            "Epoch 1536: train loss: 0.3985177278518677\n",
            "Epoch 1537: train loss: 0.4433285593986511\n",
            "Epoch 1538: train loss: 0.3984624445438385\n",
            "Epoch 1539: train loss: 0.44607600569725037\n",
            "Epoch 1540: train loss: 0.3999917209148407\n",
            "Epoch 1541: train loss: 0.44675129652023315\n",
            "Epoch 1542: train loss: 0.40021100640296936\n",
            "Epoch 1543: train loss: 0.4479138255119324\n",
            "Epoch 1544: train loss: 0.4002614915370941\n",
            "Epoch 1545: train loss: 0.44591763615608215\n",
            "Epoch 1546: train loss: 0.398456871509552\n",
            "Epoch 1547: train loss: 0.4449198842048645\n",
            "Epoch 1548: train loss: 0.39864036440849304\n",
            "Epoch 1549: train loss: 0.44462618231773376\n",
            "Epoch 1550: train loss: 0.39913004636764526\n",
            "Epoch 1551: train loss: 0.44721704721450806\n",
            "Epoch 1552: train loss: 0.40099287033081055\n",
            "Epoch 1553: train loss: 0.44790810346603394\n",
            "Epoch 1554: train loss: 0.4002954959869385\n",
            "Epoch 1555: train loss: 0.4471771717071533\n",
            "Epoch 1556: train loss: 0.39889806509017944\n",
            "Epoch 1557: train loss: 0.4437702000141144\n",
            "Epoch 1558: train loss: 0.39766260981559753\n",
            "Epoch 1559: train loss: 0.44460558891296387\n",
            "Epoch 1560: train loss: 0.39921292662620544\n",
            "Epoch 1561: train loss: 0.4461092948913574\n",
            "Epoch 1562: train loss: 0.4004482626914978\n",
            "Epoch 1563: train loss: 0.44886836409568787\n",
            "Epoch 1564: train loss: 0.4011576175689697\n",
            "Epoch 1565: train loss: 0.44710573554039\n",
            "Epoch 1566: train loss: 0.39879298210144043\n",
            "Epoch 1567: train loss: 0.44470787048339844\n",
            "Epoch 1568: train loss: 0.3978278636932373\n",
            "Epoch 1569: train loss: 0.4431615173816681\n",
            "Epoch 1570: train loss: 0.39829114079475403\n",
            "Epoch 1571: train loss: 0.4463486969470978\n",
            "Epoch 1572: train loss: 0.40089336037635803\n",
            "Epoch 1573: train loss: 0.448318213224411\n",
            "Epoch 1574: train loss: 0.4010353684425354\n",
            "Epoch 1575: train loss: 0.44849255681037903\n",
            "Epoch 1576: train loss: 0.39971721172332764\n",
            "Epoch 1577: train loss: 0.4444045126438141\n",
            "Epoch 1578: train loss: 0.3971380889415741\n",
            "Epoch 1579: train loss: 0.4431549906730652\n",
            "Epoch 1580: train loss: 0.3988029360771179\n",
            "Epoch 1581: train loss: 0.44571158289909363\n",
            "Epoch 1582: train loss: 0.4000098407268524\n",
            "Epoch 1583: train loss: 0.448250949382782\n",
            "Epoch 1584: train loss: 0.40099406242370605\n",
            "Epoch 1585: train loss: 0.44717663526535034\n",
            "Epoch 1586: train loss: 0.39918678998947144\n",
            "Epoch 1587: train loss: 0.44546017050743103\n",
            "Epoch 1588: train loss: 0.39809101819992065\n",
            "Epoch 1589: train loss: 0.4432256519794464\n",
            "Epoch 1590: train loss: 0.398701936006546\n",
            "Epoch 1591: train loss: 0.44666165113449097\n",
            "Epoch 1592: train loss: 0.4003964364528656\n",
            "Epoch 1593: train loss: 0.4470209777355194\n",
            "Epoch 1594: train loss: 0.39989304542541504\n",
            "Epoch 1595: train loss: 0.44704267382621765\n",
            "Epoch 1596: train loss: 0.3992057144641876\n",
            "Epoch 1597: train loss: 0.44445884227752686\n",
            "Epoch 1598: train loss: 0.3979286253452301\n",
            "Epoch 1599: train loss: 0.4448423385620117\n",
            "Epoch 1600: train loss: 0.399007648229599\n",
            "Epoch 1601: train loss: 0.44550344347953796\n",
            "Epoch 1602: train loss: 0.39957961440086365\n",
            "Epoch 1603: train loss: 0.4475840628147125\n",
            "Epoch 1604: train loss: 0.40062737464904785\n",
            "Epoch 1605: train loss: 0.44696587324142456\n",
            "Epoch 1606: train loss: 0.39938461780548096\n",
            "Epoch 1607: train loss: 0.446011483669281\n",
            "Epoch 1608: train loss: 0.3984083831310272\n",
            "Epoch 1609: train loss: 0.44351717829704285\n",
            "Epoch 1610: train loss: 0.39864954352378845\n",
            "Epoch 1611: train loss: 0.44628357887268066\n",
            "Epoch 1612: train loss: 0.3998117744922638\n",
            "Epoch 1613: train loss: 0.4462258815765381\n",
            "Epoch 1614: train loss: 0.3995053470134735\n",
            "Epoch 1615: train loss: 0.4468483328819275\n",
            "Epoch 1616: train loss: 0.3995511531829834\n",
            "Epoch 1617: train loss: 0.44520992040634155\n",
            "Epoch 1618: train loss: 0.3982909023761749\n",
            "Epoch 1619: train loss: 0.44507142901420593\n",
            "Epoch 1620: train loss: 0.3987099230289459\n",
            "Epoch 1621: train loss: 0.4448869228363037\n",
            "Epoch 1622: train loss: 0.39923375844955444\n",
            "Epoch 1623: train loss: 0.4471929371356964\n",
            "Epoch 1624: train loss: 0.40049174427986145\n",
            "Epoch 1625: train loss: 0.44692569971084595\n",
            "Epoch 1626: train loss: 0.3994852602481842\n",
            "Epoch 1627: train loss: 0.44622308015823364\n",
            "Epoch 1628: train loss: 0.39846712350845337\n",
            "Epoch 1629: train loss: 0.4435354173183441\n",
            "Epoch 1630: train loss: 0.39853253960609436\n",
            "Epoch 1631: train loss: 0.4460121691226959\n",
            "Epoch 1632: train loss: 0.39943447709083557\n",
            "Epoch 1633: train loss: 0.4456815719604492\n",
            "Epoch 1634: train loss: 0.3992045223712921\n",
            "Epoch 1635: train loss: 0.44667595624923706\n",
            "Epoch 1636: train loss: 0.3996036946773529\n",
            "Epoch 1637: train loss: 0.4455343186855316\n",
            "Epoch 1638: train loss: 0.3987274467945099\n",
            "Epoch 1639: train loss: 0.4456844925880432\n",
            "Epoch 1640: train loss: 0.3988359570503235\n",
            "Epoch 1641: train loss: 0.44467777013778687\n",
            "Epoch 1642: train loss: 0.39864978194236755\n",
            "Epoch 1643: train loss: 0.446170836687088\n",
            "Epoch 1644: train loss: 0.3997921645641327\n",
            "Epoch 1645: train loss: 0.4462318420410156\n",
            "Epoch 1646: train loss: 0.39952823519706726\n",
            "Epoch 1647: train loss: 0.4467897415161133\n",
            "Epoch 1648: train loss: 0.3992288112640381\n",
            "Epoch 1649: train loss: 0.44462573528289795\n",
            "Epoch 1650: train loss: 0.39798295497894287\n",
            "Epoch 1651: train loss: 0.44479498267173767\n",
            "Epoch 1652: train loss: 0.3986334502696991\n",
            "Epoch 1653: train loss: 0.444892019033432\n",
            "Epoch 1654: train loss: 0.3992096781730652\n",
            "Epoch 1655: train loss: 0.4470688998699188\n",
            "Epoch 1656: train loss: 0.4003206193447113\n",
            "Epoch 1657: train loss: 0.44671404361724854\n",
            "Epoch 1658: train loss: 0.39934900403022766\n",
            "Epoch 1659: train loss: 0.4460335373878479\n",
            "Epoch 1660: train loss: 0.39825987815856934\n",
            "Epoch 1661: train loss: 0.44326522946357727\n",
            "Epoch 1662: train loss: 0.3983868360519409\n",
            "Epoch 1663: train loss: 0.44593149423599243\n",
            "Epoch 1664: train loss: 0.39949092268943787\n",
            "Epoch 1665: train loss: 0.4458114206790924\n",
            "Epoch 1666: train loss: 0.39934948086738586\n",
            "Epoch 1667: train loss: 0.4467548429965973\n",
            "Epoch 1668: train loss: 0.39932477474212646\n",
            "Epoch 1669: train loss: 0.444886714220047\n",
            "Epoch 1670: train loss: 0.3981214463710785\n",
            "Epoch 1671: train loss: 0.44494032859802246\n",
            "Epoch 1672: train loss: 0.39853617548942566\n",
            "Epoch 1673: train loss: 0.4446432888507843\n",
            "Epoch 1674: train loss: 0.3989162743091583\n",
            "Epoch 1675: train loss: 0.44668176770210266\n",
            "Epoch 1676: train loss: 0.40009844303131104\n",
            "Epoch 1677: train loss: 0.4465849995613098\n",
            "Epoch 1678: train loss: 0.3994706869125366\n",
            "Epoch 1679: train loss: 0.4463532269001007\n",
            "Epoch 1680: train loss: 0.39855143427848816\n",
            "Epoch 1681: train loss: 0.44360244274139404\n",
            "Epoch 1682: train loss: 0.39824146032333374\n",
            "Epoch 1683: train loss: 0.44543689489364624\n",
            "Epoch 1684: train loss: 0.39892667531967163\n",
            "Epoch 1685: train loss: 0.4450241029262543\n",
            "Epoch 1686: train loss: 0.3988763689994812\n",
            "Epoch 1687: train loss: 0.4463634192943573\n",
            "Epoch 1688: train loss: 0.3995768129825592\n",
            "Epoch 1689: train loss: 0.445689857006073\n",
            "Epoch 1690: train loss: 0.3987899124622345\n",
            "Epoch 1691: train loss: 0.44569817185401917\n",
            "Epoch 1692: train loss: 0.3987392485141754\n",
            "Epoch 1693: train loss: 0.44437816739082336\n",
            "Epoch 1694: train loss: 0.3980695605278015\n",
            "Epoch 1695: train loss: 0.44525840878486633\n",
            "Epoch 1696: train loss: 0.3990722596645355\n",
            "Epoch 1697: train loss: 0.44549211859703064\n",
            "Epoch 1698: train loss: 0.39937981963157654\n",
            "Epoch 1699: train loss: 0.44694048166275024\n",
            "Epoch 1700: train loss: 0.3996506631374359\n",
            "Epoch 1701: train loss: 0.44551947712898254\n",
            "Epoch 1702: train loss: 0.3983307480812073\n",
            "Epoch 1703: train loss: 0.4448736310005188\n",
            "Epoch 1704: train loss: 0.39805611968040466\n",
            "Epoch 1705: train loss: 0.44374343752861023\n",
            "Epoch 1706: train loss: 0.3990131616592407\n",
            "Epoch 1707: train loss: 0.44703125953674316\n",
            "Epoch 1708: train loss: 0.4000546932220459\n",
            "Epoch 1709: train loss: 0.4462166428565979\n",
            "Epoch 1710: train loss: 0.3988664448261261\n",
            "Epoch 1711: train loss: 0.4454241693019867\n",
            "Epoch 1712: train loss: 0.39803755283355713\n",
            "Epoch 1713: train loss: 0.4432801604270935\n",
            "Epoch 1714: train loss: 0.3983880281448364\n",
            "Epoch 1715: train loss: 0.44599470496177673\n",
            "Epoch 1716: train loss: 0.39950138330459595\n",
            "Epoch 1717: train loss: 0.44584354758262634\n",
            "Epoch 1718: train loss: 0.39907777309417725\n",
            "Epoch 1719: train loss: 0.4461420178413391\n",
            "Epoch 1720: train loss: 0.39877480268478394\n",
            "Epoch 1721: train loss: 0.4442104995250702\n",
            "Epoch 1722: train loss: 0.39863985776901245\n",
            "Epoch 1723: train loss: 0.4459436535835266\n",
            "Epoch 1724: train loss: 0.39889585971832275\n",
            "Epoch 1725: train loss: 0.4445737898349762\n",
            "Epoch 1726: train loss: 0.3988901972770691\n",
            "Epoch 1727: train loss: 0.4462527334690094\n",
            "Epoch 1728: train loss: 0.39899417757987976\n",
            "Epoch 1729: train loss: 0.4445551931858063\n",
            "Epoch 1730: train loss: 0.39872705936431885\n",
            "Epoch 1731: train loss: 0.44592636823654175\n",
            "Epoch 1732: train loss: 0.39869266748428345\n",
            "Epoch 1733: train loss: 0.44422638416290283\n",
            "Epoch 1734: train loss: 0.39873287081718445\n",
            "Epoch 1735: train loss: 0.44615331292152405\n",
            "Epoch 1736: train loss: 0.3991013169288635\n",
            "Epoch 1737: train loss: 0.4448103904724121\n",
            "Epoch 1738: train loss: 0.3989541530609131\n",
            "Epoch 1739: train loss: 0.44616997241973877\n",
            "Epoch 1740: train loss: 0.3987514078617096\n",
            "Epoch 1741: train loss: 0.44412344694137573\n",
            "Epoch 1742: train loss: 0.3984891176223755\n",
            "Epoch 1743: train loss: 0.4457196295261383\n",
            "Epoch 1744: train loss: 0.3987003564834595\n",
            "Epoch 1745: train loss: 0.44439607858657837\n",
            "Epoch 1746: train loss: 0.3988669812679291\n",
            "Epoch 1747: train loss: 0.4463615417480469\n",
            "Epoch 1748: train loss: 0.39918506145477295\n",
            "Epoch 1749: train loss: 0.4448416233062744\n",
            "Epoch 1750: train loss: 0.398831307888031\n",
            "Epoch 1751: train loss: 0.445920467376709\n",
            "Epoch 1752: train loss: 0.3984546363353729\n",
            "Epoch 1753: train loss: 0.4437723159790039\n",
            "Epoch 1754: train loss: 0.39830470085144043\n",
            "Epoch 1755: train loss: 0.4456825852394104\n",
            "Epoch 1756: train loss: 0.3989331126213074\n",
            "Epoch 1757: train loss: 0.44487521052360535\n",
            "Epoch 1758: train loss: 0.39916324615478516\n",
            "Epoch 1759: train loss: 0.4466269612312317\n",
            "Epoch 1760: train loss: 0.39905986189842224\n",
            "Epoch 1761: train loss: 0.44443437457084656\n",
            "Epoch 1762: train loss: 0.3983463943004608\n",
            "Epoch 1763: train loss: 0.4452812671661377\n",
            "Epoch 1764: train loss: 0.39892318844795227\n",
            "Epoch 1765: train loss: 0.44467252492904663\n",
            "Epoch 1766: train loss: 0.39870068430900574\n",
            "Epoch 1767: train loss: 0.44584593176841736\n",
            "Epoch 1768: train loss: 0.39855071902275085\n",
            "Epoch 1769: train loss: 0.4440097510814667\n",
            "Epoch 1770: train loss: 0.39844128489494324\n",
            "Epoch 1771: train loss: 0.4458020329475403\n",
            "Epoch 1772: train loss: 0.39887142181396484\n",
            "Epoch 1773: train loss: 0.44467413425445557\n",
            "Epoch 1774: train loss: 0.3989391624927521\n",
            "Epoch 1775: train loss: 0.446321964263916\n",
            "Epoch 1776: train loss: 0.398927241563797\n",
            "Epoch 1777: train loss: 0.4443659782409668\n",
            "Epoch 1778: train loss: 0.39834967255592346\n",
            "Epoch 1779: train loss: 0.4453701078891754\n",
            "Epoch 1780: train loss: 0.39901867508888245\n",
            "Epoch 1781: train loss: 0.44479256868362427\n",
            "Epoch 1782: train loss: 0.3986669182777405\n",
            "Epoch 1783: train loss: 0.44574612379074097\n",
            "Epoch 1784: train loss: 0.399192214012146\n",
            "Epoch 1785: train loss: 0.44479551911354065\n",
            "Epoch 1786: train loss: 0.39832237362861633\n",
            "Epoch 1787: train loss: 0.4451015293598175\n",
            "Epoch 1788: train loss: 0.39874592423439026\n",
            "Epoch 1789: train loss: 0.4443569481372833\n",
            "Epoch 1790: train loss: 0.3983492851257324\n",
            "Epoch 1791: train loss: 0.44546619057655334\n",
            "Epoch 1792: train loss: 0.39928996562957764\n",
            "Epoch 1793: train loss: 0.4452382028102875\n",
            "Epoch 1794: train loss: 0.39884766936302185\n",
            "Epoch 1795: train loss: 0.445759654045105\n",
            "Epoch 1796: train loss: 0.39892056584358215\n",
            "Epoch 1797: train loss: 0.4442318081855774\n",
            "Epoch 1798: train loss: 0.39783984422683716\n",
            "Epoch 1799: train loss: 0.4445609152317047\n",
            "Epoch 1800: train loss: 0.3986547291278839\n",
            "Epoch 1801: train loss: 0.4446045756340027\n",
            "Epoch 1802: train loss: 0.39872777462005615\n",
            "Epoch 1803: train loss: 0.44600099325180054\n",
            "Epoch 1804: train loss: 0.3996299207210541\n",
            "Epoch 1805: train loss: 0.4456530511379242\n",
            "Epoch 1806: train loss: 0.3987334966659546\n",
            "Epoch 1807: train loss: 0.4452906548976898\n",
            "Epoch 1808: train loss: 0.39821523427963257\n",
            "Epoch 1809: train loss: 0.4432951509952545\n",
            "Epoch 1810: train loss: 0.39763930439949036\n",
            "Epoch 1811: train loss: 0.44474557042121887\n",
            "Epoch 1812: train loss: 0.39907246828079224\n",
            "Epoch 1813: train loss: 0.4453953206539154\n",
            "Epoch 1814: train loss: 0.3993344008922577\n",
            "Epoch 1815: train loss: 0.4465804100036621\n",
            "Epoch 1816: train loss: 0.3986637592315674\n",
            "Epoch 1817: train loss: 0.443766713142395\n",
            "Epoch 1818: train loss: 0.3976966440677643\n",
            "Epoch 1819: train loss: 0.4445887804031372\n",
            "Epoch 1820: train loss: 0.3987281620502472\n",
            "Epoch 1821: train loss: 0.44479724764823914\n",
            "Epoch 1822: train loss: 0.39896678924560547\n",
            "Epoch 1823: train loss: 0.4462672770023346\n",
            "Epoch 1824: train loss: 0.3995003402233124\n",
            "Epoch 1825: train loss: 0.44518956542015076\n",
            "Epoch 1826: train loss: 0.39820507168769836\n",
            "Epoch 1827: train loss: 0.4446128010749817\n",
            "Epoch 1828: train loss: 0.398019939661026\n",
            "Epoch 1829: train loss: 0.44330817461013794\n",
            "Epoch 1830: train loss: 0.39785510301589966\n",
            "Epoch 1831: train loss: 0.4452676475048065\n",
            "Epoch 1832: train loss: 0.3995510935783386\n",
            "Epoch 1833: train loss: 0.4459742605686188\n",
            "Epoch 1834: train loss: 0.3993299901485443\n",
            "Epoch 1835: train loss: 0.44620007276535034\n",
            "Epoch 1836: train loss: 0.39876091480255127\n",
            "Epoch 1837: train loss: 0.4437105059623718\n",
            "Epoch 1838: train loss: 0.39718979597091675\n",
            "Epoch 1839: train loss: 0.4435737431049347\n",
            "Epoch 1840: train loss: 0.39806196093559265\n",
            "Epoch 1841: train loss: 0.44419151544570923\n",
            "Epoch 1842: train loss: 0.39891794323921204\n",
            "Epoch 1843: train loss: 0.44670283794403076\n",
            "Epoch 1844: train loss: 0.3992376923561096\n",
            "Epoch 1845: train loss: 0.4449087679386139\n",
            "Epoch 1846: train loss: 0.39851275086402893\n",
            "Epoch 1847: train loss: 0.44531938433647156\n",
            "Epoch 1848: train loss: 0.3986455798149109\n",
            "Epoch 1849: train loss: 0.4440646767616272\n",
            "Epoch 1850: train loss: 0.39777615666389465\n",
            "Epoch 1851: train loss: 0.44447192549705505\n",
            "Epoch 1852: train loss: 0.39846089482307434\n",
            "Epoch 1853: train loss: 0.4444591701030731\n",
            "Epoch 1854: train loss: 0.3988250195980072\n",
            "Epoch 1855: train loss: 0.44632700085639954\n",
            "Epoch 1856: train loss: 0.3994244337081909\n",
            "Epoch 1857: train loss: 0.44499942660331726\n",
            "Epoch 1858: train loss: 0.39814960956573486\n",
            "Epoch 1859: train loss: 0.44459718465805054\n",
            "Epoch 1860: train loss: 0.39790406823158264\n",
            "Epoch 1861: train loss: 0.44319450855255127\n",
            "Epoch 1862: train loss: 0.39768537878990173\n",
            "Epoch 1863: train loss: 0.4449361562728882\n",
            "Epoch 1864: train loss: 0.39930757880210876\n",
            "Epoch 1865: train loss: 0.44570258259773254\n",
            "Epoch 1866: train loss: 0.3992014229297638\n",
            "Epoch 1867: train loss: 0.44617313146591187\n",
            "Epoch 1868: train loss: 0.39873838424682617\n",
            "Epoch 1869: train loss: 0.44372257590293884\n",
            "Epoch 1870: train loss: 0.3971737325191498\n",
            "Epoch 1871: train loss: 0.44355419278144836\n",
            "Epoch 1872: train loss: 0.3979725241661072\n",
            "Epoch 1873: train loss: 0.4440397322177887\n",
            "Epoch 1874: train loss: 0.39876264333724976\n",
            "Epoch 1875: train loss: 0.44652241468429565\n",
            "Epoch 1876: train loss: 0.39989781379699707\n",
            "Epoch 1877: train loss: 0.44583699107170105\n",
            "Epoch 1878: train loss: 0.3984829783439636\n",
            "Epoch 1879: train loss: 0.444652259349823\n",
            "Epoch 1880: train loss: 0.3975394666194916\n",
            "Epoch 1881: train loss: 0.442413330078125\n",
            "Epoch 1882: train loss: 0.39694449305534363\n",
            "Epoch 1883: train loss: 0.4440297484397888\n",
            "Epoch 1884: train loss: 0.398930162191391\n",
            "Epoch 1885: train loss: 0.4457061290740967\n",
            "Epoch 1886: train loss: 0.39970991015434265\n",
            "Epoch 1887: train loss: 0.44724318385124207\n",
            "Epoch 1888: train loss: 0.39954352378845215\n",
            "Epoch 1889: train loss: 0.4445846378803253\n",
            "Epoch 1890: train loss: 0.3970595598220825\n",
            "Epoch 1891: train loss: 0.4427300989627838\n",
            "Epoch 1892: train loss: 0.39679691195487976\n",
            "Epoch 1893: train loss: 0.4423721134662628\n",
            "Epoch 1894: train loss: 0.3980000615119934\n",
            "Epoch 1895: train loss: 0.4461703300476074\n",
            "Epoch 1896: train loss: 0.4004458785057068\n",
            "Epoch 1897: train loss: 0.4472396671772003\n",
            "Epoch 1898: train loss: 0.39878761768341064\n",
            "Epoch 1899: train loss: 0.44482308626174927\n",
            "Epoch 1900: train loss: 0.3973878026008606\n",
            "Epoch 1901: train loss: 0.44210875034332275\n",
            "Epoch 1902: train loss: 0.39675188064575195\n",
            "Epoch 1903: train loss: 0.44381219148635864\n",
            "Epoch 1904: train loss: 0.3987937271595001\n",
            "Epoch 1905: train loss: 0.4455951154232025\n",
            "Epoch 1906: train loss: 0.39972102642059326\n",
            "Epoch 1907: train loss: 0.44731637835502625\n",
            "Epoch 1908: train loss: 0.3994712829589844\n",
            "Epoch 1909: train loss: 0.44447246193885803\n",
            "Epoch 1910: train loss: 0.39696285128593445\n",
            "Epoch 1911: train loss: 0.44264137744903564\n",
            "Epoch 1912: train loss: 0.39676445722579956\n",
            "Epoch 1913: train loss: 0.44230973720550537\n",
            "Epoch 1914: train loss: 0.39784371852874756\n",
            "Epoch 1915: train loss: 0.44595515727996826\n",
            "Epoch 1916: train loss: 0.4003453850746155\n",
            "Epoch 1917: train loss: 0.44717955589294434\n",
            "Epoch 1918: train loss: 0.39951905608177185\n",
            "Epoch 1919: train loss: 0.44590651988983154\n",
            "Epoch 1920: train loss: 0.3977276086807251\n",
            "Epoch 1921: train loss: 0.4418821930885315\n",
            "Epoch 1922: train loss: 0.3958193063735962\n",
            "Epoch 1923: train loss: 0.44209933280944824\n",
            "Epoch 1924: train loss: 0.3975914418697357\n",
            "Epoch 1925: train loss: 0.44443148374557495\n",
            "Epoch 1926: train loss: 0.3996916711330414\n",
            "Epoch 1927: train loss: 0.4482022821903229\n",
            "Epoch 1928: train loss: 0.3996884524822235\n",
            "Epoch 1929: train loss: 0.4449545443058014\n",
            "Epoch 1930: train loss: 0.39764803647994995\n",
            "Epoch 1931: train loss: 0.44367560744285583\n",
            "Epoch 1932: train loss: 0.3971482813358307\n",
            "Epoch 1933: train loss: 0.4423258602619171\n",
            "Epoch 1934: train loss: 0.3972533345222473\n",
            "Epoch 1935: train loss: 0.4447031021118164\n",
            "Epoch 1936: train loss: 0.399269700050354\n",
            "Epoch 1937: train loss: 0.4459022879600525\n",
            "Epoch 1938: train loss: 0.3993622660636902\n",
            "Epoch 1939: train loss: 0.44638827443122864\n",
            "Epoch 1940: train loss: 0.39853131771087646\n",
            "Epoch 1941: train loss: 0.44319823384284973\n",
            "Epoch 1942: train loss: 0.39654502272605896\n",
            "Epoch 1943: train loss: 0.44264790415763855\n",
            "Epoch 1944: train loss: 0.3973360061645508\n",
            "Epoch 1945: train loss: 0.44341960549354553\n",
            "Epoch 1946: train loss: 0.39852654933929443\n",
            "Epoch 1947: train loss: 0.44650763273239136\n",
            "Epoch 1948: train loss: 0.39992204308509827\n",
            "Epoch 1949: train loss: 0.4460434019565582\n",
            "Epoch 1950: train loss: 0.39850831031799316\n",
            "Epoch 1951: train loss: 0.444523423910141\n",
            "Epoch 1952: train loss: 0.39699089527130127\n",
            "Epoch 1953: train loss: 0.44153228402137756\n",
            "Epoch 1954: train loss: 0.39628466963768005\n",
            "Epoch 1955: train loss: 0.44335460662841797\n",
            "Epoch 1956: train loss: 0.3986360430717468\n",
            "Epoch 1957: train loss: 0.445639044046402\n",
            "Epoch 1958: train loss: 0.39987629652023315\n",
            "Epoch 1959: train loss: 0.44763514399528503\n",
            "Epoch 1960: train loss: 0.3993493616580963\n",
            "Epoch 1961: train loss: 0.4441019892692566\n",
            "Epoch 1962: train loss: 0.39646634459495544\n",
            "Epoch 1963: train loss: 0.4419858753681183\n",
            "Epoch 1964: train loss: 0.3962787389755249\n",
            "Epoch 1965: train loss: 0.4418957531452179\n",
            "Epoch 1966: train loss: 0.39785152673721313\n",
            "Epoch 1967: train loss: 0.4462355375289917\n",
            "Epoch 1968: train loss: 0.4003959596157074\n",
            "Epoch 1969: train loss: 0.4472172260284424\n",
            "Epoch 1970: train loss: 0.3992750942707062\n",
            "Epoch 1971: train loss: 0.4454061686992645\n",
            "Epoch 1972: train loss: 0.3972480297088623\n",
            "Epoch 1973: train loss: 0.4413309693336487\n",
            "Epoch 1974: train loss: 0.3955705165863037\n",
            "Epoch 1975: train loss: 0.44200021028518677\n",
            "Epoch 1976: train loss: 0.3977501392364502\n",
            "Epoch 1977: train loss: 0.4447837471961975\n",
            "Epoch 1978: train loss: 0.3998470902442932\n",
            "Epoch 1979: train loss: 0.4484897553920746\n",
            "Epoch 1980: train loss: 0.3994971215724945\n",
            "Epoch 1981: train loss: 0.4440425932407379\n",
            "Epoch 1982: train loss: 0.3967379033565521\n",
            "Epoch 1983: train loss: 0.44301554560661316\n",
            "Epoch 1984: train loss: 0.396929532289505\n",
            "Epoch 1985: train loss: 0.44201773405075073\n",
            "Epoch 1986: train loss: 0.3975243866443634\n",
            "Epoch 1987: train loss: 0.44590476155281067\n",
            "Epoch 1988: train loss: 0.39991775155067444\n",
            "Epoch 1989: train loss: 0.4460257291793823\n",
            "Epoch 1990: train loss: 0.39883455634117126\n",
            "Epoch 1991: train loss: 0.44578585028648376\n",
            "Epoch 1992: train loss: 0.39783090353012085\n",
            "Epoch 1993: train loss: 0.44179847836494446\n",
            "Epoch 1994: train loss: 0.39577335119247437\n",
            "Epoch 1995: train loss: 0.4426852762699127\n",
            "Epoch 1996: train loss: 0.3977203369140625\n",
            "Epoch 1997: train loss: 0.44390133023262024\n",
            "Epoch 1998: train loss: 0.39910393953323364\n",
            "Epoch 1999: train loss: 0.44774413108825684\n",
            "Epoch 2000: train loss: 0.39996734261512756\n",
            "Epoch 2001: train loss: 0.4449946880340576\n",
            "Epoch 2002: train loss: 0.3973008096218109\n",
            "Epoch 2003: train loss: 0.44336098432540894\n",
            "Epoch 2004: train loss: 0.3964831531047821\n",
            "Epoch 2005: train loss: 0.4409152567386627\n",
            "Epoch 2006: train loss: 0.39644408226013184\n",
            "Epoch 2007: train loss: 0.44463175535202026\n",
            "Epoch 2008: train loss: 0.3995172381401062\n",
            "Epoch 2009: train loss: 0.44613781571388245\n",
            "Epoch 2010: train loss: 0.3995416462421417\n",
            "Epoch 2011: train loss: 0.44712042808532715\n",
            "Epoch 2012: train loss: 0.39857688546180725\n",
            "Epoch 2013: train loss: 0.4424602687358856\n",
            "Epoch 2014: train loss: 0.3957144618034363\n",
            "Epoch 2015: train loss: 0.4419071078300476\n",
            "Epoch 2016: train loss: 0.39673975110054016\n",
            "Epoch 2017: train loss: 0.44244498014450073\n",
            "Epoch 2018: train loss: 0.39815959334373474\n",
            "Epoch 2019: train loss: 0.44704604148864746\n",
            "Epoch 2020: train loss: 0.4003199636936188\n",
            "Epoch 2021: train loss: 0.4461541771888733\n",
            "Epoch 2022: train loss: 0.39844971895217896\n",
            "Epoch 2023: train loss: 0.44490599632263184\n",
            "Epoch 2024: train loss: 0.39688387513160706\n",
            "Epoch 2025: train loss: 0.440612256526947\n",
            "Epoch 2026: train loss: 0.3954060971736908\n",
            "Epoch 2027: train loss: 0.4427255094051361\n",
            "Epoch 2028: train loss: 0.39822015166282654\n",
            "Epoch 2029: train loss: 0.4448990225791931\n",
            "Epoch 2030: train loss: 0.3996243476867676\n",
            "Epoch 2031: train loss: 0.4481637477874756\n",
            "Epoch 2032: train loss: 0.3998230993747711\n",
            "Epoch 2033: train loss: 0.4443370997905731\n",
            "Epoch 2034: train loss: 0.39642533659935\n",
            "Epoch 2035: train loss: 0.4421005845069885\n",
            "Epoch 2036: train loss: 0.39597728848457336\n",
            "Epoch 2037: train loss: 0.4407092034816742\n",
            "Epoch 2038: train loss: 0.39688724279403687\n",
            "Epoch 2039: train loss: 0.4456265866756439\n",
            "Epoch 2040: train loss: 0.4001602828502655\n",
            "Epoch 2041: train loss: 0.44676655530929565\n",
            "Epoch 2042: train loss: 0.39928343892097473\n",
            "Epoch 2043: train loss: 0.4462873935699463\n",
            "Epoch 2044: train loss: 0.3976205885410309\n",
            "Epoch 2045: train loss: 0.4411071240901947\n",
            "Epoch 2046: train loss: 0.3950347900390625\n",
            "Epoch 2047: train loss: 0.4415571689605713\n",
            "Epoch 2048: train loss: 0.39711838960647583\n",
            "Epoch 2049: train loss: 0.4435109496116638\n",
            "Epoch 2050: train loss: 0.3991324305534363\n",
            "Epoch 2051: train loss: 0.44830024242401123\n",
            "Epoch 2052: train loss: 0.3996901214122772\n",
            "Epoch 2053: train loss: 0.44450387358665466\n",
            "Epoch 2054: train loss: 0.39700064063072205\n",
            "Epoch 2055: train loss: 0.4430600106716156\n",
            "Epoch 2056: train loss: 0.3964345455169678\n",
            "Epoch 2057: train loss: 0.4411192834377289\n",
            "Epoch 2058: train loss: 0.3967035710811615\n",
            "Epoch 2059: train loss: 0.4451926648616791\n",
            "Epoch 2060: train loss: 0.3996047079563141\n",
            "Epoch 2061: train loss: 0.44577401876449585\n",
            "Epoch 2062: train loss: 0.3987388014793396\n",
            "Epoch 2063: train loss: 0.44553685188293457\n",
            "Epoch 2064: train loss: 0.39757534861564636\n",
            "Epoch 2065: train loss: 0.4418635070323944\n",
            "Epoch 2066: train loss: 0.3957658112049103\n",
            "Epoch 2067: train loss: 0.44234153628349304\n",
            "Epoch 2068: train loss: 0.3971969485282898\n",
            "Epoch 2069: train loss: 0.4432063698768616\n",
            "Epoch 2070: train loss: 0.39851170778274536\n",
            "Epoch 2071: train loss: 0.4472852647304535\n",
            "Epoch 2072: train loss: 0.39995649456977844\n",
            "Epoch 2073: train loss: 0.44509586691856384\n",
            "Epoch 2074: train loss: 0.39711883664131165\n",
            "Epoch 2075: train loss: 0.4429398775100708\n",
            "Epoch 2076: train loss: 0.39604297280311584\n",
            "Epoch 2077: train loss: 0.44038474559783936\n",
            "Epoch 2078: train loss: 0.3961712419986725\n",
            "Epoch 2079: train loss: 0.4444534480571747\n",
            "Epoch 2080: train loss: 0.39932021498680115\n",
            "Epoch 2081: train loss: 0.4458988904953003\n",
            "Epoch 2082: train loss: 0.3992515802383423\n",
            "Epoch 2083: train loss: 0.44682031869888306\n",
            "Epoch 2084: train loss: 0.3982556164264679\n",
            "Epoch 2085: train loss: 0.4420086145401001\n",
            "Epoch 2086: train loss: 0.39520078897476196\n",
            "Epoch 2087: train loss: 0.44117534160614014\n",
            "Epoch 2088: train loss: 0.3964397609233856\n",
            "Epoch 2089: train loss: 0.4424189329147339\n",
            "Epoch 2090: train loss: 0.3982943594455719\n",
            "Epoch 2091: train loss: 0.4475718140602112\n",
            "Epoch 2092: train loss: 0.4005174934864044\n",
            "Epoch 2093: train loss: 0.4460623264312744\n",
            "Epoch 2094: train loss: 0.3975606858730316\n",
            "Epoch 2095: train loss: 0.44298022985458374\n",
            "Epoch 2096: train loss: 0.3956036865711212\n",
            "Epoch 2097: train loss: 0.4397875964641571\n",
            "Epoch 2098: train loss: 0.3953975737094879\n",
            "Epoch 2099: train loss: 0.44312143325805664\n",
            "Epoch 2100: train loss: 0.39887648820877075\n",
            "Epoch 2101: train loss: 0.4460585117340088\n",
            "Epoch 2102: train loss: 0.40000760555267334\n",
            "Epoch 2103: train loss: 0.44837334752082825\n",
            "Epoch 2104: train loss: 0.3990856111049652\n",
            "Epoch 2105: train loss: 0.44260790944099426\n",
            "Epoch 2106: train loss: 0.3947524130344391\n",
            "Epoch 2107: train loss: 0.43973129987716675\n",
            "Epoch 2108: train loss: 0.3950927257537842\n",
            "Epoch 2109: train loss: 0.44103795289993286\n",
            "Epoch 2110: train loss: 0.3977968692779541\n",
            "Epoch 2111: train loss: 0.4471498429775238\n",
            "Epoch 2112: train loss: 0.4009396433830261\n",
            "Epoch 2113: train loss: 0.4475131034851074\n",
            "Epoch 2114: train loss: 0.3990141749382019\n",
            "Epoch 2115: train loss: 0.44519394636154175\n",
            "Epoch 2116: train loss: 0.3960379660129547\n",
            "Epoch 2117: train loss: 0.4387374520301819\n",
            "Epoch 2118: train loss: 0.3936592936515808\n",
            "Epoch 2119: train loss: 0.4405254125595093\n",
            "Epoch 2120: train loss: 0.3974224627017975\n",
            "Epoch 2121: train loss: 0.44499483704566956\n",
            "Epoch 2122: train loss: 0.40056124329566956\n",
            "Epoch 2123: train loss: 0.4502951502799988\n",
            "Epoch 2124: train loss: 0.3999025821685791\n",
            "Epoch 2125: train loss: 0.4436446726322174\n",
            "Epoch 2126: train loss: 0.3952237367630005\n",
            "Epoch 2127: train loss: 0.43996191024780273\n",
            "Epoch 2128: train loss: 0.39460134506225586\n",
            "Epoch 2129: train loss: 0.4399487376213074\n",
            "Epoch 2130: train loss: 0.39702409505844116\n",
            "Epoch 2131: train loss: 0.4462176263332367\n",
            "Epoch 2132: train loss: 0.4006805717945099\n",
            "Epoch 2133: train loss: 0.4475668668746948\n",
            "Epoch 2134: train loss: 0.39926180243492126\n",
            "Epoch 2135: train loss: 0.4458162784576416\n",
            "Epoch 2136: train loss: 0.3965649902820587\n",
            "Epoch 2137: train loss: 0.43931835889816284\n",
            "Epoch 2138: train loss: 0.39361992478370667\n",
            "Epoch 2139: train loss: 0.4401753842830658\n",
            "Epoch 2140: train loss: 0.3969365954399109\n",
            "Epoch 2141: train loss: 0.44410440325737\n",
            "Epoch 2142: train loss: 0.39990508556365967\n",
            "Epoch 2143: train loss: 0.44952213764190674\n",
            "Epoch 2144: train loss: 0.399909108877182\n",
            "Epoch 2145: train loss: 0.44417715072631836\n",
            "Epoch 2146: train loss: 0.39587289094924927\n",
            "Epoch 2147: train loss: 0.441074013710022\n",
            "Epoch 2148: train loss: 0.39502084255218506\n",
            "Epoch 2149: train loss: 0.43967747688293457\n",
            "Epoch 2150: train loss: 0.39638999104499817\n",
            "Epoch 2151: train loss: 0.44558411836624146\n",
            "Epoch 2152: train loss: 0.40026769042015076\n",
            "Epoch 2153: train loss: 0.44686150550842285\n",
            "Epoch 2154: train loss: 0.39905664324760437\n",
            "Epoch 2155: train loss: 0.44572389125823975\n",
            "Epoch 2156: train loss: 0.3966953456401825\n",
            "Epoch 2157: train loss: 0.43986544013023376\n",
            "Epoch 2158: train loss: 0.3941807448863983\n",
            "Epoch 2159: train loss: 0.44090092182159424\n",
            "Epoch 2160: train loss: 0.39684948325157166\n",
            "Epoch 2161: train loss: 0.4434192478656769\n",
            "Epoch 2162: train loss: 0.39894670248031616\n",
            "Epoch 2163: train loss: 0.4479946792125702\n",
            "Epoch 2164: train loss: 0.4001340866088867\n",
            "Epoch 2165: train loss: 0.44517388939857483\n",
            "Epoch 2166: train loss: 0.3965166211128235\n",
            "Epoch 2167: train loss: 0.44198939204216003\n",
            "Epoch 2168: train loss: 0.39509207010269165\n",
            "Epoch 2169: train loss: 0.43905213475227356\n",
            "Epoch 2170: train loss: 0.3952818512916565\n",
            "Epoch 2171: train loss: 0.44367605447769165\n",
            "Epoch 2172: train loss: 0.3992331624031067\n",
            "Epoch 2173: train loss: 0.4464215636253357\n",
            "Epoch 2174: train loss: 0.39971745014190674\n",
            "Epoch 2175: train loss: 0.4476318955421448\n",
            "Epoch 2176: train loss: 0.39831826090812683\n",
            "Epoch 2177: train loss: 0.4415439963340759\n",
            "Epoch 2178: train loss: 0.39412692189216614\n",
            "Epoch 2179: train loss: 0.43924033641815186\n",
            "Epoch 2180: train loss: 0.3949892818927765\n",
            "Epoch 2181: train loss: 0.4412679672241211\n",
            "Epoch 2182: train loss: 0.3981287181377411\n",
            "Epoch 2183: train loss: 0.44764983654022217\n",
            "Epoch 2184: train loss: 0.4009133577346802\n",
            "Epoch 2185: train loss: 0.44701600074768066\n",
            "Epoch 2186: train loss: 0.39809614419937134\n",
            "Epoch 2187: train loss: 0.443815678358078\n",
            "Epoch 2188: train loss: 0.39532899856567383\n",
            "Epoch 2189: train loss: 0.4382225275039673\n",
            "Epoch 2190: train loss: 0.3936358392238617\n",
            "Epoch 2191: train loss: 0.4408603608608246\n",
            "Epoch 2192: train loss: 0.39768967032432556\n",
            "Epoch 2193: train loss: 0.4453265368938446\n",
            "Epoch 2194: train loss: 0.40046054124832153\n",
            "Epoch 2195: train loss: 0.4500752091407776\n",
            "Epoch 2196: train loss: 0.39936161041259766\n",
            "Epoch 2197: train loss: 0.442685067653656\n",
            "Epoch 2198: train loss: 0.39416295289993286\n",
            "Epoch 2199: train loss: 0.43780866265296936\n",
            "Epoch 2200: train loss: 0.39359480142593384\n",
            "Epoch 2201: train loss: 0.4403342604637146\n",
            "Epoch 2202: train loss: 0.3979053795337677\n",
            "Epoch 2203: train loss: 0.44705039262771606\n",
            "Epoch 2204: train loss: 0.4010884463787079\n",
            "Epoch 2205: train loss: 0.4488435387611389\n",
            "Epoch 2206: train loss: 0.3990878760814667\n",
            "Epoch 2207: train loss: 0.4434422254562378\n",
            "Epoch 2208: train loss: 0.3945161998271942\n",
            "Epoch 2209: train loss: 0.4379405975341797\n",
            "Epoch 2210: train loss: 0.3931180536746979\n",
            "Epoch 2211: train loss: 0.4390828311443329\n",
            "Epoch 2212: train loss: 0.39677461981773376\n",
            "Epoch 2213: train loss: 0.4457779824733734\n",
            "Epoch 2214: train loss: 0.40109771490097046\n",
            "Epoch 2215: train loss: 0.44951340556144714\n",
            "Epoch 2216: train loss: 0.3999030590057373\n",
            "Epoch 2217: train loss: 0.4447351098060608\n",
            "Epoch 2218: train loss: 0.39501869678497314\n",
            "Epoch 2219: train loss: 0.4381270706653595\n",
            "Epoch 2220: train loss: 0.3927012085914612\n",
            "Epoch 2221: train loss: 0.43812012672424316\n",
            "Epoch 2222: train loss: 0.395958811044693\n",
            "Epoch 2223: train loss: 0.44470858573913574\n",
            "Epoch 2224: train loss: 0.40078145265579224\n",
            "Epoch 2225: train loss: 0.4497891366481781\n",
            "Epoch 2226: train loss: 0.3997848331928253\n",
            "Epoch 2227: train loss: 0.44478681683540344\n",
            "Epoch 2228: train loss: 0.3955371677875519\n",
            "Epoch 2229: train loss: 0.4392351806163788\n",
            "Epoch 2230: train loss: 0.3934991657733917\n",
            "Epoch 2231: train loss: 0.4388453960418701\n",
            "Epoch 2232: train loss: 0.3957516551017761\n",
            "Epoch 2233: train loss: 0.4439059793949127\n",
            "Epoch 2234: train loss: 0.39999261498451233\n",
            "Epoch 2235: train loss: 0.44854289293289185\n",
            "Epoch 2236: train loss: 0.3999883532524109\n",
            "Epoch 2237: train loss: 0.44556477665901184\n",
            "Epoch 2238: train loss: 0.3959856927394867\n",
            "Epoch 2239: train loss: 0.4395658075809479\n",
            "Epoch 2240: train loss: 0.3933318555355072\n",
            "Epoch 2241: train loss: 0.4383212924003601\n",
            "Epoch 2242: train loss: 0.395345002412796\n",
            "Epoch 2243: train loss: 0.44334349036216736\n",
            "Epoch 2244: train loss: 0.39954298734664917\n",
            "Epoch 2245: train loss: 0.4482358396053314\n",
            "Epoch 2246: train loss: 0.4003181755542755\n",
            "Epoch 2247: train loss: 0.4463649392127991\n",
            "Epoch 2248: train loss: 0.39678284525871277\n",
            "Epoch 2249: train loss: 0.4404447078704834\n",
            "Epoch 2250: train loss: 0.3933226764202118\n",
            "Epoch 2251: train loss: 0.437725305557251\n",
            "Epoch 2252: train loss: 0.3942793309688568\n",
            "Epoch 2253: train loss: 0.4417737126350403\n",
            "Epoch 2254: train loss: 0.3986985385417938\n",
            "Epoch 2255: train loss: 0.44778549671173096\n",
            "Epoch 2256: train loss: 0.40089717507362366\n",
            "Epoch 2257: train loss: 0.44795408844947815\n",
            "Epoch 2258: train loss: 0.3979305922985077\n",
            "Epoch 2259: train loss: 0.4417608976364136\n",
            "Epoch 2260: train loss: 0.3935995399951935\n",
            "Epoch 2261: train loss: 0.43732979893684387\n",
            "Epoch 2262: train loss: 0.3941150903701782\n",
            "Epoch 2263: train loss: 0.4410545229911804\n",
            "Epoch 2264: train loss: 0.3978213667869568\n",
            "Epoch 2265: train loss: 0.4464857280254364\n",
            "Epoch 2266: train loss: 0.4002905488014221\n",
            "Epoch 2267: train loss: 0.44750741124153137\n",
            "Epoch 2268: train loss: 0.398298054933548\n",
            "Epoch 2269: train loss: 0.4428573548793793\n",
            "Epoch 2270: train loss: 0.3945177495479584\n",
            "Epoch 2271: train loss: 0.43833619356155396\n",
            "Epoch 2272: train loss: 0.3935234248638153\n",
            "Epoch 2273: train loss: 0.4398097097873688\n",
            "Epoch 2274: train loss: 0.39704176783561707\n",
            "Epoch 2275: train loss: 0.4457084536552429\n",
            "Epoch 2276: train loss: 0.40034836530685425\n",
            "Epoch 2277: train loss: 0.4482214152812958\n",
            "Epoch 2278: train loss: 0.3990338146686554\n",
            "Epoch 2279: train loss: 0.44384580850601196\n",
            "Epoch 2280: train loss: 0.39486491680145264\n",
            "Epoch 2281: train loss: 0.4384475648403168\n",
            "Epoch 2282: train loss: 0.39374813437461853\n",
            "Epoch 2283: train loss: 0.439597487449646\n",
            "Epoch 2284: train loss: 0.3962382674217224\n",
            "Epoch 2285: train loss: 0.4443266987800598\n",
            "Epoch 2286: train loss: 0.39958930015563965\n",
            "Epoch 2287: train loss: 0.44754758477211\n",
            "Epoch 2288: train loss: 0.3992014229297638\n",
            "Epoch 2289: train loss: 0.44473615288734436\n",
            "Epoch 2290: train loss: 0.3957117795944214\n",
            "Epoch 2291: train loss: 0.439596563577652\n",
            "Epoch 2292: train loss: 0.39356672763824463\n",
            "Epoch 2293: train loss: 0.4388089179992676\n",
            "Epoch 2294: train loss: 0.39535704255104065\n",
            "Epoch 2295: train loss: 0.4431766867637634\n",
            "Epoch 2296: train loss: 0.39922744035720825\n",
            "Epoch 2297: train loss: 0.4477978050708771\n",
            "Epoch 2298: train loss: 0.39986130595207214\n",
            "Epoch 2299: train loss: 0.44577300548553467\n",
            "Epoch 2300: train loss: 0.3964477777481079\n",
            "Epoch 2301: train loss: 0.4403724670410156\n",
            "Epoch 2302: train loss: 0.39343392848968506\n",
            "Epoch 2303: train loss: 0.43805623054504395\n",
            "Epoch 2304: train loss: 0.3945141136646271\n",
            "Epoch 2305: train loss: 0.4419228136539459\n",
            "Epoch 2306: train loss: 0.39851877093315125\n",
            "Epoch 2307: train loss: 0.4473249018192291\n",
            "Epoch 2308: train loss: 0.4002284109592438\n",
            "Epoch 2309: train loss: 0.44687211513519287\n",
            "Epoch 2310: train loss: 0.3974844813346863\n",
            "Epoch 2311: train loss: 0.44161638617515564\n",
            "Epoch 2312: train loss: 0.39373302459716797\n",
            "Epoch 2313: train loss: 0.43771612644195557\n",
            "Epoch 2314: train loss: 0.3943142592906952\n",
            "Epoch 2315: train loss: 0.4413047432899475\n",
            "Epoch 2316: train loss: 0.397630512714386\n",
            "Epoch 2317: train loss: 0.4459523558616638\n",
            "Epoch 2318: train loss: 0.39973485469818115\n",
            "Epoch 2319: train loss: 0.4467891454696655\n",
            "Epoch 2320: train loss: 0.3978307843208313\n",
            "Epoch 2321: train loss: 0.44238775968551636\n",
            "Epoch 2322: train loss: 0.3943740427494049\n",
            "Epoch 2323: train loss: 0.4385199546813965\n",
            "Epoch 2324: train loss: 0.3937462568283081\n",
            "Epoch 2325: train loss: 0.44004473090171814\n",
            "Epoch 2326: train loss: 0.3969312906265259\n",
            "Epoch 2327: train loss: 0.44538694620132446\n",
            "Epoch 2328: train loss: 0.3998792767524719\n",
            "Epoch 2329: train loss: 0.44754093885421753\n",
            "Epoch 2330: train loss: 0.3986268639564514\n",
            "Epoch 2331: train loss: 0.44347721338272095\n",
            "Epoch 2332: train loss: 0.39466699957847595\n",
            "Epoch 2333: train loss: 0.4384007751941681\n",
            "Epoch 2334: train loss: 0.39391905069351196\n",
            "Epoch 2335: train loss: 0.43991249799728394\n",
            "Epoch 2336: train loss: 0.3962050974369049\n",
            "Epoch 2337: train loss: 0.444038450717926\n",
            "Epoch 2338: train loss: 0.3990522623062134\n",
            "Epoch 2339: train loss: 0.446805477142334\n",
            "Epoch 2340: train loss: 0.39867618680000305\n",
            "Epoch 2341: train loss: 0.44417259097099304\n",
            "Epoch 2342: train loss: 0.3955281972885132\n",
            "Epoch 2343: train loss: 0.43965816497802734\n",
            "Epoch 2344: train loss: 0.39360511302948\n",
            "Epoch 2345: train loss: 0.43900105357170105\n",
            "Epoch 2346: train loss: 0.39552193880081177\n",
            "Epoch 2347: train loss: 0.4433201551437378\n",
            "Epoch 2348: train loss: 0.3989288806915283\n",
            "Epoch 2349: train loss: 0.44715434312820435\n",
            "Epoch 2350: train loss: 0.399288147687912\n",
            "Epoch 2351: train loss: 0.4450874924659729\n",
            "Epoch 2352: train loss: 0.3960306942462921\n",
            "Epoch 2353: train loss: 0.440071165561676\n",
            "Epoch 2354: train loss: 0.3934326469898224\n",
            "Epoch 2355: train loss: 0.4382737874984741\n",
            "Epoch 2356: train loss: 0.3945983052253723\n",
            "Epoch 2357: train loss: 0.4421154260635376\n",
            "Epoch 2358: train loss: 0.39846745133399963\n",
            "Epoch 2359: train loss: 0.44705626368522644\n",
            "Epoch 2360: train loss: 0.39981022477149963\n",
            "Epoch 2361: train loss: 0.44627079367637634\n",
            "Epoch 2362: train loss: 0.3970015347003937\n",
            "Epoch 2363: train loss: 0.4411071836948395\n",
            "Epoch 2364: train loss: 0.39355790615081787\n",
            "Epoch 2365: train loss: 0.43772467970848083\n",
            "Epoch 2366: train loss: 0.39438939094543457\n",
            "Epoch 2367: train loss: 0.44149112701416016\n",
            "Epoch 2368: train loss: 0.39762988686561584\n",
            "Epoch 2369: train loss: 0.4458281993865967\n",
            "Epoch 2370: train loss: 0.39935600757598877\n",
            "Epoch 2371: train loss: 0.44619548320770264\n",
            "Epoch 2372: train loss: 0.3973877727985382\n",
            "Epoch 2373: train loss: 0.44190558791160583\n",
            "Epoch 2374: train loss: 0.39418861269950867\n",
            "Epoch 2375: train loss: 0.438547283411026\n",
            "Epoch 2376: train loss: 0.39456382393836975\n",
            "Epoch 2377: train loss: 0.44117727875709534\n",
            "Epoch 2378: train loss: 0.39689409732818604\n",
            "Epoch 2379: train loss: 0.4445918798446655\n",
            "Epoch 2380: train loss: 0.3987264633178711\n",
            "Epoch 2381: train loss: 0.44575417041778564\n",
            "Epoch 2382: train loss: 0.39762163162231445\n",
            "Epoch 2383: train loss: 0.4427037239074707\n",
            "Epoch 2384: train loss: 0.39489200711250305\n",
            "Epoch 2385: train loss: 0.4394649863243103\n",
            "Epoch 2386: train loss: 0.3940763771533966\n",
            "Epoch 2387: train loss: 0.4400557279586792\n",
            "Epoch 2388: train loss: 0.3963261544704437\n",
            "Epoch 2389: train loss: 0.4442181885242462\n",
            "Epoch 2390: train loss: 0.39890602231025696\n",
            "Epoch 2391: train loss: 0.4463386833667755\n",
            "Epoch 2392: train loss: 0.39818036556243896\n",
            "Epoch 2393: train loss: 0.4434818923473358\n",
            "Epoch 2394: train loss: 0.3950389325618744\n",
            "Epoch 2395: train loss: 0.4391380846500397\n",
            "Epoch 2396: train loss: 0.3942699432373047\n",
            "Epoch 2397: train loss: 0.44027966260910034\n",
            "Epoch 2398: train loss: 0.3960452079772949\n",
            "Epoch 2399: train loss: 0.443468302488327\n",
            "Epoch 2400: train loss: 0.39812490344047546\n",
            "Epoch 2401: train loss: 0.4454081356525421\n",
            "Epoch 2402: train loss: 0.3979259729385376\n",
            "Epoch 2403: train loss: 0.4436277151107788\n",
            "Epoch 2404: train loss: 0.3956087529659271\n",
            "Epoch 2405: train loss: 0.4402226507663727\n",
            "Epoch 2406: train loss: 0.3942182958126068\n",
            "Epoch 2407: train loss: 0.4398692548274994\n",
            "Epoch 2408: train loss: 0.3955587148666382\n",
            "Epoch 2409: train loss: 0.44278252124786377\n",
            "Epoch 2410: train loss: 0.3979467451572418\n",
            "Epoch 2411: train loss: 0.445596843957901\n",
            "Epoch 2412: train loss: 0.39829644560813904\n",
            "Epoch 2413: train loss: 0.44423896074295044\n",
            "Epoch 2414: train loss: 0.3958874046802521\n",
            "Epoch 2415: train loss: 0.44049155712127686\n",
            "Epoch 2416: train loss: 0.3940790295600891\n",
            "Epoch 2417: train loss: 0.4392978250980377\n",
            "Epoch 2418: train loss: 0.3950876295566559\n",
            "Epoch 2419: train loss: 0.44235196709632874\n",
            "Epoch 2420: train loss: 0.39776769280433655\n",
            "Epoch 2421: train loss: 0.44562581181526184\n",
            "Epoch 2422: train loss: 0.3986469507217407\n",
            "Epoch 2423: train loss: 0.4449227750301361\n",
            "Epoch 2424: train loss: 0.3963930308818817\n",
            "Epoch 2425: train loss: 0.4408629834651947\n",
            "Epoch 2426: train loss: 0.39385244250297546\n",
            "Epoch 2427: train loss: 0.43871742486953735\n",
            "Epoch 2428: train loss: 0.39508700370788574\n",
            "Epoch 2429: train loss: 0.4421564042568207\n",
            "Epoch 2430: train loss: 0.39737436175346375\n",
            "Epoch 2431: train loss: 0.4449997544288635\n",
            "Epoch 2432: train loss: 0.3982625901699066\n",
            "Epoch 2433: train loss: 0.44462937116622925\n",
            "Epoch 2434: train loss: 0.39654630422592163\n",
            "Epoch 2435: train loss: 0.4413929581642151\n",
            "Epoch 2436: train loss: 0.39438945055007935\n",
            "Epoch 2437: train loss: 0.43929383158683777\n",
            "Epoch 2438: train loss: 0.3945063352584839\n",
            "Epoch 2439: train loss: 0.4411717653274536\n",
            "Epoch 2440: train loss: 0.3969772458076477\n",
            "Epoch 2441: train loss: 0.44479265809059143\n",
            "Epoch 2442: train loss: 0.3984825015068054\n",
            "Epoch 2443: train loss: 0.44518646597862244\n",
            "Epoch 2444: train loss: 0.39704933762550354\n",
            "Epoch 2445: train loss: 0.4420381784439087\n",
            "Epoch 2446: train loss: 0.39447757601737976\n",
            "Epoch 2447: train loss: 0.43900367617607117\n",
            "Epoch 2448: train loss: 0.3946223258972168\n",
            "Epoch 2449: train loss: 0.4411609470844269\n",
            "Epoch 2450: train loss: 0.3965736925601959\n",
            "Epoch 2451: train loss: 0.44398626685142517\n",
            "Epoch 2452: train loss: 0.3979633152484894\n",
            "Epoch 2453: train loss: 0.44470861554145813\n",
            "Epoch 2454: train loss: 0.3970378637313843\n",
            "Epoch 2455: train loss: 0.4422755539417267\n",
            "Epoch 2456: train loss: 0.3949369192123413\n",
            "Epoch 2457: train loss: 0.439905047416687\n",
            "Epoch 2458: train loss: 0.3943849503993988\n",
            "Epoch 2459: train loss: 0.44049257040023804\n",
            "Epoch 2460: train loss: 0.3960491418838501\n",
            "Epoch 2461: train loss: 0.4433618187904358\n",
            "Epoch 2462: train loss: 0.39795294404029846\n",
            "Epoch 2463: train loss: 0.44521772861480713\n",
            "Epoch 2464: train loss: 0.39757633209228516\n",
            "Epoch 2465: train loss: 0.4430902302265167\n",
            "Epoch 2466: train loss: 0.3951377868652344\n",
            "Epoch 2467: train loss: 0.4396913945674896\n",
            "Epoch 2468: train loss: 0.39383938908576965\n",
            "Epoch 2469: train loss: 0.4396217167377472\n",
            "Epoch 2470: train loss: 0.3955944776535034\n",
            "Epoch 2471: train loss: 0.4431474804878235\n",
            "Epoch 2472: train loss: 0.39818114042282104\n",
            "Epoch 2473: train loss: 0.4457153379917145\n",
            "Epoch 2474: train loss: 0.39791569113731384\n",
            "Epoch 2475: train loss: 0.4434453547000885\n",
            "Epoch 2476: train loss: 0.3951491117477417\n",
            "Epoch 2477: train loss: 0.4396250545978546\n",
            "Epoch 2478: train loss: 0.3943788409233093\n",
            "Epoch 2479: train loss: 0.4401504397392273\n",
            "Epoch 2480: train loss: 0.3953261375427246\n",
            "Epoch 2481: train loss: 0.4421372711658478\n",
            "Epoch 2482: train loss: 0.3969501256942749\n",
            "Epoch 2483: train loss: 0.44422394037246704\n",
            "Epoch 2484: train loss: 0.39769676327705383\n",
            "Epoch 2485: train loss: 0.4440059959888458\n",
            "Epoch 2486: train loss: 0.3962297737598419\n",
            "Epoch 2487: train loss: 0.44120994210243225\n",
            "Epoch 2488: train loss: 0.3944135010242462\n",
            "Epoch 2489: train loss: 0.4396824240684509\n",
            "Epoch 2490: train loss: 0.394659161567688\n",
            "Epoch 2491: train loss: 0.4412815570831299\n",
            "Epoch 2492: train loss: 0.39667099714279175\n",
            "Epoch 2493: train loss: 0.4440809190273285\n",
            "Epoch 2494: train loss: 0.3979412913322449\n",
            "Epoch 2495: train loss: 0.4447013735771179\n",
            "Epoch 2496: train loss: 0.39676934480667114\n",
            "Epoch 2497: train loss: 0.44180577993392944\n",
            "Epoch 2498: train loss: 0.39456048607826233\n",
            "Epoch 2499: train loss: 0.43946149945259094\n",
            "Epoch 2500: train loss: 0.39483940601348877\n",
            "Epoch 2501: train loss: 0.4412413537502289\n",
            "Epoch 2502: train loss: 0.3961748778820038\n",
            "Epoch 2503: train loss: 0.44305887818336487\n",
            "Epoch 2504: train loss: 0.39709988236427307\n",
            "Epoch 2505: train loss: 0.4437922537326813\n",
            "Epoch 2506: train loss: 0.3967403173446655\n",
            "Epoch 2507: train loss: 0.44234856963157654\n",
            "Epoch 2508: train loss: 0.39520785212516785\n",
            "Epoch 2509: train loss: 0.4404876232147217\n",
            "Epoch 2510: train loss: 0.3947142958641052\n",
            "Epoch 2511: train loss: 0.44073253870010376\n",
            "Epoch 2512: train loss: 0.3958263099193573\n",
            "Epoch 2513: train loss: 0.4426828622817993\n",
            "Epoch 2514: train loss: 0.39706358313560486\n",
            "Epoch 2515: train loss: 0.4439472258090973\n",
            "Epoch 2516: train loss: 0.39695167541503906\n",
            "Epoch 2517: train loss: 0.44268494844436646\n",
            "Epoch 2518: train loss: 0.39529862999916077\n",
            "Epoch 2519: train loss: 0.4404661953449249\n",
            "Epoch 2520: train loss: 0.39445140957832336\n",
            "Epoch 2521: train loss: 0.4402807354927063\n",
            "Epoch 2522: train loss: 0.39557549357414246\n",
            "Epoch 2523: train loss: 0.4424986243247986\n",
            "Epoch 2524: train loss: 0.3971157670021057\n",
            "Epoch 2525: train loss: 0.44419991970062256\n",
            "Epoch 2526: train loss: 0.3971662223339081\n",
            "Epoch 2527: train loss: 0.4430496394634247\n",
            "Epoch 2528: train loss: 0.3953261077404022\n",
            "Epoch 2529: train loss: 0.4401964843273163\n",
            "Epoch 2530: train loss: 0.3941430151462555\n",
            "Epoch 2531: train loss: 0.4398793578147888\n",
            "Epoch 2532: train loss: 0.3953331410884857\n",
            "Epoch 2533: train loss: 0.442426860332489\n",
            "Epoch 2534: train loss: 0.3971952199935913\n",
            "Epoch 2535: train loss: 0.4443000555038452\n",
            "Epoch 2536: train loss: 0.3973454535007477\n",
            "Epoch 2537: train loss: 0.44335511326789856\n",
            "Epoch 2538: train loss: 0.39557141065597534\n",
            "Epoch 2539: train loss: 0.4403407871723175\n",
            "Epoch 2540: train loss: 0.3938739001750946\n",
            "Epoch 2541: train loss: 0.4392726421356201\n",
            "Epoch 2542: train loss: 0.3954283595085144\n",
            "Epoch 2543: train loss: 0.4426155686378479\n",
            "Epoch 2544: train loss: 0.3971730172634125\n",
            "Epoch 2545: train loss: 0.4441194534301758\n",
            "Epoch 2546: train loss: 0.3970763385295868\n",
            "Epoch 2547: train loss: 0.4429568648338318\n",
            "Epoch 2548: train loss: 0.39528483152389526\n",
            "Epoch 2549: train loss: 0.4402334988117218\n",
            "Epoch 2550: train loss: 0.39404717087745667\n",
            "Epoch 2551: train loss: 0.43955177068710327\n",
            "Epoch 2552: train loss: 0.39501601457595825\n",
            "Epoch 2553: train loss: 0.44209545850753784\n",
            "Epoch 2554: train loss: 0.3971516489982605\n",
            "Epoch 2555: train loss: 0.4445071518421173\n",
            "Epoch 2556: train loss: 0.3974081873893738\n",
            "Epoch 2557: train loss: 0.443309485912323\n",
            "Epoch 2558: train loss: 0.3955593407154083\n",
            "Epoch 2559: train loss: 0.4404672086238861\n",
            "Epoch 2560: train loss: 0.39397016167640686\n",
            "Epoch 2561: train loss: 0.4392773509025574\n",
            "Epoch 2562: train loss: 0.3952755928039551\n",
            "Epoch 2563: train loss: 0.4422799348831177\n",
            "Epoch 2564: train loss: 0.39681848883628845\n",
            "Epoch 2565: train loss: 0.4436660706996918\n",
            "Epoch 2566: train loss: 0.3968659043312073\n",
            "Epoch 2567: train loss: 0.4428733289241791\n",
            "Epoch 2568: train loss: 0.3953746557235718\n",
            "Epoch 2569: train loss: 0.440514475107193\n",
            "Epoch 2570: train loss: 0.394402414560318\n",
            "Epoch 2571: train loss: 0.43998223543167114\n",
            "Epoch 2572: train loss: 0.395106703042984\n",
            "Epoch 2573: train loss: 0.4419060945510864\n",
            "Epoch 2574: train loss: 0.39652153849601746\n",
            "Epoch 2575: train loss: 0.44349682331085205\n",
            "Epoch 2576: train loss: 0.3969539701938629\n",
            "Epoch 2577: train loss: 0.4430539309978485\n",
            "Epoch 2578: train loss: 0.39572861790657043\n",
            "Epoch 2579: train loss: 0.44102415442466736\n",
            "Epoch 2580: train loss: 0.3945617973804474\n",
            "Epoch 2581: train loss: 0.440007746219635\n",
            "Epoch 2582: train loss: 0.3947674632072449\n",
            "Epoch 2583: train loss: 0.4410938322544098\n",
            "Epoch 2584: train loss: 0.39595410227775574\n",
            "Epoch 2585: train loss: 0.44299980998039246\n",
            "Epoch 2586: train loss: 0.3969084620475769\n",
            "Epoch 2587: train loss: 0.4432887136936188\n",
            "Epoch 2588: train loss: 0.3962402641773224\n",
            "Epoch 2589: train loss: 0.44202250242233276\n",
            "Epoch 2590: train loss: 0.39476507902145386\n",
            "Epoch 2591: train loss: 0.4395943284034729\n",
            "Epoch 2592: train loss: 0.39487946033477783\n",
            "Epoch 2593: train loss: 0.4415823221206665\n",
            "Epoch 2594: train loss: 0.39581921696662903\n",
            "Epoch 2595: train loss: 0.44177865982055664\n",
            "Epoch 2596: train loss: 0.3960494101047516\n",
            "Epoch 2597: train loss: 0.4434446394443512\n",
            "Epoch 2598: train loss: 0.39651384949684143\n",
            "Epoch 2599: train loss: 0.4413377344608307\n",
            "Epoch 2600: train loss: 0.3949419856071472\n",
            "Epoch 2601: train loss: 0.4418794512748718\n",
            "Epoch 2602: train loss: 0.39542722702026367\n",
            "Epoch 2603: train loss: 0.440105676651001\n",
            "Epoch 2604: train loss: 0.3949168026447296\n",
            "Epoch 2605: train loss: 0.44227296113967896\n",
            "Epoch 2606: train loss: 0.39637649059295654\n",
            "Epoch 2607: train loss: 0.4420161843299866\n",
            "Epoch 2608: train loss: 0.3959867060184479\n",
            "Epoch 2609: train loss: 0.4434637129306793\n",
            "Epoch 2610: train loss: 0.39633792638778687\n",
            "Epoch 2611: train loss: 0.4407740533351898\n",
            "Epoch 2612: train loss: 0.3945062756538391\n",
            "Epoch 2613: train loss: 0.44115376472473145\n",
            "Epoch 2614: train loss: 0.3952440023422241\n",
            "Epoch 2615: train loss: 0.44036024808883667\n",
            "Epoch 2616: train loss: 0.39532914757728577\n",
            "Epoch 2617: train loss: 0.44327619671821594\n",
            "Epoch 2618: train loss: 0.39703547954559326\n",
            "Epoch 2619: train loss: 0.4423167109489441\n",
            "Epoch 2620: train loss: 0.3954852819442749\n",
            "Epoch 2621: train loss: 0.44217583537101746\n",
            "Epoch 2622: train loss: 0.3952777683734894\n",
            "Epoch 2623: train loss: 0.43964287638664246\n",
            "Epoch 2624: train loss: 0.39511966705322266\n",
            "Epoch 2625: train loss: 0.44279971718788147\n",
            "Epoch 2626: train loss: 0.3964204490184784\n",
            "Epoch 2627: train loss: 0.4413609802722931\n",
            "Epoch 2628: train loss: 0.39505401253700256\n",
            "Epoch 2629: train loss: 0.44193994998931885\n",
            "Epoch 2630: train loss: 0.3955150246620178\n",
            "Epoch 2631: train loss: 0.44038063287734985\n",
            "Epoch 2632: train loss: 0.39495036005973816\n",
            "Epoch 2633: train loss: 0.44239944219589233\n",
            "Epoch 2634: train loss: 0.3962785005569458\n",
            "Epoch 2635: train loss: 0.4414699077606201\n",
            "Epoch 2636: train loss: 0.39538466930389404\n",
            "Epoch 2637: train loss: 0.44251808524131775\n",
            "Epoch 2638: train loss: 0.3958687484264374\n",
            "Epoch 2639: train loss: 0.4406161904335022\n",
            "Epoch 2640: train loss: 0.3947896957397461\n",
            "Epoch 2641: train loss: 0.4419318437576294\n",
            "Epoch 2642: train loss: 0.39569100737571716\n",
            "Epoch 2643: train loss: 0.44061774015426636\n",
            "Epoch 2644: train loss: 0.3949752449989319\n",
            "Epoch 2645: train loss: 0.44206199049949646\n",
            "Epoch 2646: train loss: 0.3959929049015045\n",
            "Epoch 2647: train loss: 0.44130203127861023\n",
            "Epoch 2648: train loss: 0.39550459384918213\n",
            "Epoch 2649: train loss: 0.443185031414032\n",
            "Epoch 2650: train loss: 0.3963269889354706\n",
            "Epoch 2651: train loss: 0.4408695101737976\n",
            "Epoch 2652: train loss: 0.3942679166793823\n",
            "Epoch 2653: train loss: 0.4398248791694641\n",
            "Epoch 2654: train loss: 0.3952656686306\n",
            "Epoch 2655: train loss: 0.4416052997112274\n",
            "Epoch 2656: train loss: 0.3956761360168457\n",
            "Epoch 2657: train loss: 0.4424014389514923\n",
            "Epoch 2658: train loss: 0.3959944546222687\n",
            "Epoch 2659: train loss: 0.4414725601673126\n",
            "Epoch 2660: train loss: 0.39512181282043457\n",
            "Epoch 2661: train loss: 0.44176551699638367\n",
            "Epoch 2662: train loss: 0.3951742351055145\n",
            "Epoch 2663: train loss: 0.4399106800556183\n",
            "Epoch 2664: train loss: 0.39544677734375\n",
            "Epoch 2665: train loss: 0.44338271021842957\n",
            "Epoch 2666: train loss: 0.3964925706386566\n",
            "Epoch 2667: train loss: 0.44107604026794434\n",
            "Epoch 2668: train loss: 0.39447757601737976\n",
            "Epoch 2669: train loss: 0.4407110810279846\n",
            "Epoch 2670: train loss: 0.39551934599876404\n",
            "Epoch 2671: train loss: 0.4407370090484619\n",
            "Epoch 2672: train loss: 0.3951004147529602\n",
            "Epoch 2673: train loss: 0.4426540732383728\n",
            "Epoch 2674: train loss: 0.3961494565010071\n",
            "Epoch 2675: train loss: 0.44094666838645935\n",
            "Epoch 2676: train loss: 0.3945545554161072\n",
            "Epoch 2677: train loss: 0.44036948680877686\n",
            "Epoch 2678: train loss: 0.3954901695251465\n",
            "Epoch 2679: train loss: 0.44169098138809204\n",
            "Epoch 2680: train loss: 0.3955356180667877\n",
            "Epoch 2681: train loss: 0.44203388690948486\n",
            "Epoch 2682: train loss: 0.39545464515686035\n",
            "Epoch 2683: train loss: 0.44075503945350647\n",
            "Epoch 2684: train loss: 0.3947850465774536\n",
            "Epoch 2685: train loss: 0.4414440393447876\n",
            "Epoch 2686: train loss: 0.39528319239616394\n",
            "Epoch 2687: train loss: 0.440388947725296\n",
            "Epoch 2688: train loss: 0.3959202170372009\n",
            "Epoch 2689: train loss: 0.44413021206855774\n",
            "Epoch 2690: train loss: 0.39667031168937683\n",
            "Epoch 2691: train loss: 0.44097721576690674\n",
            "Epoch 2692: train loss: 0.3945269286632538\n",
            "Epoch 2693: train loss: 0.4398882985115051\n",
            "Epoch 2694: train loss: 0.39460113644599915\n",
            "Epoch 2695: train loss: 0.44011053442955017\n",
            "Epoch 2696: train loss: 0.39526885747909546\n",
            "Epoch 2697: train loss: 0.44213026762008667\n",
            "Epoch 2698: train loss: 0.3957481384277344\n",
            "Epoch 2699: train loss: 0.44131773710250854\n",
            "Epoch 2700: train loss: 0.3950914144515991\n",
            "Epoch 2701: train loss: 0.4417691230773926\n",
            "Epoch 2702: train loss: 0.39513590931892395\n",
            "Epoch 2703: train loss: 0.43991947174072266\n",
            "Epoch 2704: train loss: 0.3953426778316498\n",
            "Epoch 2705: train loss: 0.44330859184265137\n",
            "Epoch 2706: train loss: 0.39635440707206726\n",
            "Epoch 2707: train loss: 0.44094982743263245\n",
            "Epoch 2708: train loss: 0.3949719965457916\n",
            "Epoch 2709: train loss: 0.44094690680503845\n",
            "Epoch 2710: train loss: 0.3952273428440094\n",
            "Epoch 2711: train loss: 0.44064199924468994\n",
            "Epoch 2712: train loss: 0.39516520500183105\n",
            "Epoch 2713: train loss: 0.44157037138938904\n",
            "Epoch 2714: train loss: 0.3949294090270996\n",
            "Epoch 2715: train loss: 0.44007766246795654\n",
            "Epoch 2716: train loss: 0.3952683210372925\n",
            "Epoch 2717: train loss: 0.4424912929534912\n",
            "Epoch 2718: train loss: 0.3958280682563782\n",
            "Epoch 2719: train loss: 0.44068169593811035\n",
            "Epoch 2720: train loss: 0.3954010307788849\n",
            "Epoch 2721: train loss: 0.44288063049316406\n",
            "Epoch 2722: train loss: 0.39565378427505493\n",
            "Epoch 2723: train loss: 0.4398043751716614\n",
            "Epoch 2724: train loss: 0.39426088333129883\n",
            "Epoch 2725: train loss: 0.4400779604911804\n",
            "Epoch 2726: train loss: 0.39503318071365356\n",
            "Epoch 2727: train loss: 0.4409918189048767\n",
            "Epoch 2728: train loss: 0.3959404528141022\n",
            "Epoch 2729: train loss: 0.44290128350257874\n",
            "Epoch 2730: train loss: 0.39573585987091064\n",
            "Epoch 2731: train loss: 0.440723180770874\n",
            "Epoch 2732: train loss: 0.3949628174304962\n",
            "Epoch 2733: train loss: 0.441387802362442\n",
            "Epoch 2734: train loss: 0.3953820765018463\n",
            "Epoch 2735: train loss: 0.4401058256626129\n",
            "Epoch 2736: train loss: 0.39487943053245544\n",
            "Epoch 2737: train loss: 0.4420495629310608\n",
            "Epoch 2738: train loss: 0.39523300528526306\n",
            "Epoch 2739: train loss: 0.439727246761322\n",
            "Epoch 2740: train loss: 0.39481353759765625\n",
            "Epoch 2741: train loss: 0.4418463706970215\n",
            "Epoch 2742: train loss: 0.3954025208950043\n",
            "Epoch 2743: train loss: 0.44053366780281067\n",
            "Epoch 2744: train loss: 0.3955595791339874\n",
            "Epoch 2745: train loss: 0.44312289357185364\n",
            "Epoch 2746: train loss: 0.39585229754447937\n",
            "Epoch 2747: train loss: 0.4401909410953522\n",
            "Epoch 2748: train loss: 0.39460524916648865\n",
            "Epoch 2749: train loss: 0.44122281670570374\n",
            "Epoch 2750: train loss: 0.3953987658023834\n",
            "Epoch 2751: train loss: 0.4402739405632019\n",
            "Epoch 2752: train loss: 0.3950323760509491\n",
            "Epoch 2753: train loss: 0.4421793818473816\n",
            "Epoch 2754: train loss: 0.395244836807251\n",
            "Epoch 2755: train loss: 0.4397223889827728\n",
            "Epoch 2756: train loss: 0.39478787779808044\n",
            "Epoch 2757: train loss: 0.44201797246932983\n",
            "Epoch 2758: train loss: 0.3954673409461975\n",
            "Epoch 2759: train loss: 0.4403727054595947\n",
            "Epoch 2760: train loss: 0.39524465799331665\n",
            "Epoch 2761: train loss: 0.4425108730792999\n",
            "Epoch 2762: train loss: 0.3955041766166687\n",
            "Epoch 2763: train loss: 0.4399776756763458\n",
            "Epoch 2764: train loss: 0.3947319984436035\n",
            "Epoch 2765: train loss: 0.4417867362499237\n",
            "Epoch 2766: train loss: 0.39581018686294556\n",
            "Epoch 2767: train loss: 0.44057410955429077\n",
            "Epoch 2768: train loss: 0.394912451505661\n",
            "Epoch 2769: train loss: 0.4418029189109802\n",
            "Epoch 2770: train loss: 0.39562299847602844\n",
            "Epoch 2771: train loss: 0.4401559829711914\n",
            "Epoch 2772: train loss: 0.39462295174598694\n",
            "Epoch 2773: train loss: 0.44147971272468567\n",
            "Epoch 2774: train loss: 0.3954789340496063\n",
            "Epoch 2775: train loss: 0.44016093015670776\n",
            "Epoch 2776: train loss: 0.3948195278644562\n",
            "Epoch 2777: train loss: 0.44190868735313416\n",
            "Epoch 2778: train loss: 0.39582711458206177\n",
            "Epoch 2779: train loss: 0.44054436683654785\n",
            "Epoch 2780: train loss: 0.39479684829711914\n",
            "Epoch 2781: train loss: 0.4415070116519928\n",
            "Epoch 2782: train loss: 0.3953721523284912\n",
            "Epoch 2783: train loss: 0.44005778431892395\n",
            "Epoch 2784: train loss: 0.394694983959198\n",
            "Epoch 2785: train loss: 0.4416688084602356\n",
            "Epoch 2786: train loss: 0.39567938446998596\n",
            "Epoch 2787: train loss: 0.44042202830314636\n",
            "Epoch 2788: train loss: 0.3947788178920746\n",
            "Epoch 2789: train loss: 0.44159385561943054\n",
            "Epoch 2790: train loss: 0.3955024182796478\n",
            "Epoch 2791: train loss: 0.4402613341808319\n",
            "Epoch 2792: train loss: 0.3947293758392334\n",
            "Epoch 2793: train loss: 0.44164350628852844\n",
            "Epoch 2794: train loss: 0.3955088257789612\n",
            "Epoch 2795: train loss: 0.4401572048664093\n",
            "Epoch 2796: train loss: 0.3946846127510071\n",
            "Epoch 2797: train loss: 0.44154980778694153\n",
            "Epoch 2798: train loss: 0.39552807807922363\n",
            "Epoch 2799: train loss: 0.44032371044158936\n",
            "Epoch 2800: train loss: 0.3947336971759796\n",
            "Epoch 2801: train loss: 0.44172951579093933\n",
            "Epoch 2802: train loss: 0.39555150270462036\n",
            "Epoch 2803: train loss: 0.44014397263526917\n",
            "Epoch 2804: train loss: 0.39533522725105286\n",
            "Epoch 2805: train loss: 0.4422314763069153\n",
            "Epoch 2806: train loss: 0.3954465091228485\n",
            "Epoch 2807: train loss: 0.4397345185279846\n",
            "Epoch 2808: train loss: 0.3949519097805023\n",
            "Epoch 2809: train loss: 0.44200778007507324\n",
            "Epoch 2810: train loss: 0.3952435851097107\n",
            "Epoch 2811: train loss: 0.43929144740104675\n",
            "Epoch 2812: train loss: 0.3943912386894226\n",
            "Epoch 2813: train loss: 0.4401286840438843\n",
            "Epoch 2814: train loss: 0.3955608904361725\n",
            "Epoch 2815: train loss: 0.44184646010398865\n",
            "Epoch 2816: train loss: 0.39530086517333984\n",
            "Epoch 2817: train loss: 0.4403708875179291\n",
            "Epoch 2818: train loss: 0.3951159417629242\n",
            "Epoch 2819: train loss: 0.44117334485054016\n",
            "Epoch 2820: train loss: 0.39461401104927063\n",
            "Epoch 2821: train loss: 0.43930500745773315\n",
            "Epoch 2822: train loss: 0.39493343234062195\n",
            "Epoch 2823: train loss: 0.44180572032928467\n",
            "Epoch 2824: train loss: 0.3954935669898987\n",
            "Epoch 2825: train loss: 0.440288782119751\n",
            "Epoch 2826: train loss: 0.3953653872013092\n",
            "Epoch 2827: train loss: 0.44239792227745056\n",
            "Epoch 2828: train loss: 0.39533764123916626\n",
            "Epoch 2829: train loss: 0.4394018352031708\n",
            "Epoch 2830: train loss: 0.39462342858314514\n",
            "Epoch 2831: train loss: 0.44150540232658386\n",
            "Epoch 2832: train loss: 0.39502450823783875\n",
            "Epoch 2833: train loss: 0.4392474591732025\n",
            "Epoch 2834: train loss: 0.39475730061531067\n",
            "Epoch 2835: train loss: 0.44164422154426575\n",
            "Epoch 2836: train loss: 0.3953586518764496\n",
            "Epoch 2837: train loss: 0.4400198459625244\n",
            "Epoch 2838: train loss: 0.3953607678413391\n",
            "Epoch 2839: train loss: 0.44278308749198914\n",
            "Epoch 2840: train loss: 0.39576423168182373\n",
            "Epoch 2841: train loss: 0.4397841691970825\n",
            "Epoch 2842: train loss: 0.39418715238571167\n",
            "Epoch 2843: train loss: 0.4394838213920593\n",
            "Epoch 2844: train loss: 0.39490556716918945\n",
            "Epoch 2845: train loss: 0.44094350934028625\n",
            "Epoch 2846: train loss: 0.39560940861701965\n",
            "Epoch 2847: train loss: 0.4413720667362213\n",
            "Epoch 2848: train loss: 0.394722580909729\n",
            "Epoch 2849: train loss: 0.43999093770980835\n",
            "Epoch 2850: train loss: 0.3948880434036255\n",
            "Epoch 2851: train loss: 0.44041821360588074\n",
            "Epoch 2852: train loss: 0.39534103870391846\n",
            "Epoch 2853: train loss: 0.44163957238197327\n",
            "Epoch 2854: train loss: 0.39483705163002014\n",
            "Epoch 2855: train loss: 0.4395062327384949\n",
            "Epoch 2856: train loss: 0.3948369026184082\n",
            "Epoch 2857: train loss: 0.44150057435035706\n",
            "Epoch 2858: train loss: 0.3958098590373993\n",
            "Epoch 2859: train loss: 0.44066351652145386\n",
            "Epoch 2860: train loss: 0.39509305357933044\n",
            "Epoch 2861: train loss: 0.4416707754135132\n",
            "Epoch 2862: train loss: 0.3954530656337738\n",
            "Epoch 2863: train loss: 0.4395608901977539\n",
            "Epoch 2864: train loss: 0.3943493664264679\n",
            "Epoch 2865: train loss: 0.44075673818588257\n",
            "Epoch 2866: train loss: 0.395181268453598\n",
            "Epoch 2867: train loss: 0.4395965039730072\n",
            "Epoch 2868: train loss: 0.3947440981864929\n",
            "Epoch 2869: train loss: 0.44141292572021484\n",
            "Epoch 2870: train loss: 0.3957422077655792\n",
            "Epoch 2871: train loss: 0.44050198793411255\n",
            "Epoch 2872: train loss: 0.3950704336166382\n",
            "Epoch 2873: train loss: 0.4418807923793793\n",
            "Epoch 2874: train loss: 0.39552953839302063\n",
            "Epoch 2875: train loss: 0.4395272433757782\n",
            "Epoch 2876: train loss: 0.3941030502319336\n",
            "Epoch 2877: train loss: 0.4399343430995941\n",
            "Epoch 2878: train loss: 0.3948325514793396\n",
            "Epoch 2879: train loss: 0.4397768974304199\n",
            "Epoch 2880: train loss: 0.3952861726284027\n",
            "Epoch 2881: train loss: 0.44280436635017395\n",
            "Epoch 2882: train loss: 0.3957209885120392\n",
            "Epoch 2883: train loss: 0.43980786204338074\n",
            "Epoch 2884: train loss: 0.3943532109260559\n",
            "Epoch 2885: train loss: 0.4402536153793335\n",
            "Epoch 2886: train loss: 0.39495065808296204\n",
            "Epoch 2887: train loss: 0.4398305416107178\n",
            "Epoch 2888: train loss: 0.3950631618499756\n",
            "Epoch 2889: train loss: 0.44214293360710144\n",
            "Epoch 2890: train loss: 0.3960266709327698\n",
            "Epoch 2891: train loss: 0.44045162200927734\n",
            "Epoch 2892: train loss: 0.3946446180343628\n",
            "Epoch 2893: train loss: 0.44090187549591064\n",
            "Epoch 2894: train loss: 0.3948674201965332\n",
            "Epoch 2895: train loss: 0.4389444589614868\n",
            "Epoch 2896: train loss: 0.3941834568977356\n",
            "Epoch 2897: train loss: 0.4408548176288605\n",
            "Epoch 2898: train loss: 0.39551493525505066\n",
            "Epoch 2899: train loss: 0.4402866065502167\n",
            "Epoch 2900: train loss: 0.3951415717601776\n",
            "Epoch 2901: train loss: 0.442070871591568\n",
            "Epoch 2902: train loss: 0.3956969976425171\n",
            "Epoch 2903: train loss: 0.4398241937160492\n",
            "Epoch 2904: train loss: 0.3942914307117462\n",
            "Epoch 2905: train loss: 0.4404599666595459\n",
            "Epoch 2906: train loss: 0.3948105573654175\n",
            "Epoch 2907: train loss: 0.4391513466835022\n",
            "Epoch 2908: train loss: 0.3945246934890747\n",
            "Epoch 2909: train loss: 0.44144922494888306\n",
            "Epoch 2910: train loss: 0.3958970904350281\n",
            "Epoch 2911: train loss: 0.44059574604034424\n",
            "Epoch 2912: train loss: 0.3949365019798279\n",
            "Epoch 2913: train loss: 0.4413490891456604\n",
            "Epoch 2914: train loss: 0.3950948119163513\n",
            "Epoch 2915: train loss: 0.43908607959747314\n",
            "Epoch 2916: train loss: 0.3939777910709381\n",
            "Epoch 2917: train loss: 0.4403908848762512\n",
            "Epoch 2918: train loss: 0.3951442539691925\n",
            "Epoch 2919: train loss: 0.4398912787437439\n",
            "Epoch 2920: train loss: 0.3950364887714386\n",
            "Epoch 2921: train loss: 0.44206202030181885\n",
            "Epoch 2922: train loss: 0.3958318531513214\n",
            "Epoch 2923: train loss: 0.44015738368034363\n",
            "Epoch 2924: train loss: 0.39439713954925537\n",
            "Epoch 2925: train loss: 0.4405270516872406\n",
            "Epoch 2926: train loss: 0.3947148621082306\n",
            "Epoch 2927: train loss: 0.43891018629074097\n",
            "Epoch 2928: train loss: 0.3942049741744995\n",
            "Epoch 2929: train loss: 0.44099801778793335\n",
            "Epoch 2930: train loss: 0.39560356736183167\n",
            "Epoch 2931: train loss: 0.4404336214065552\n",
            "Epoch 2932: train loss: 0.3950973153114319\n",
            "Epoch 2933: train loss: 0.44182494282722473\n",
            "Epoch 2934: train loss: 0.3954184949398041\n",
            "Epoch 2935: train loss: 0.43947935104370117\n",
            "Epoch 2936: train loss: 0.3940534293651581\n",
            "Epoch 2937: train loss: 0.4402162432670593\n",
            "Epoch 2938: train loss: 0.3946785032749176\n",
            "Epoch 2939: train loss: 0.4390108585357666\n",
            "Epoch 2940: train loss: 0.3944399356842041\n",
            "Epoch 2941: train loss: 0.4414115846157074\n",
            "Epoch 2942: train loss: 0.39578819274902344\n",
            "Epoch 2943: train loss: 0.4405279755592346\n",
            "Epoch 2944: train loss: 0.3949650824069977\n",
            "Epoch 2945: train loss: 0.44147562980651855\n",
            "Epoch 2946: train loss: 0.3950876295566559\n",
            "Epoch 2947: train loss: 0.43904435634613037\n",
            "Epoch 2948: train loss: 0.3938291072845459\n",
            "Epoch 2949: train loss: 0.44008883833885193\n",
            "Epoch 2950: train loss: 0.39482083916664124\n",
            "Epoch 2951: train loss: 0.4393286108970642\n",
            "Epoch 2952: train loss: 0.3946225345134735\n",
            "Epoch 2953: train loss: 0.44161826372146606\n",
            "Epoch 2954: train loss: 0.3957599699497223\n",
            "Epoch 2955: train loss: 0.4403502345085144\n",
            "Epoch 2956: train loss: 0.3946724534034729\n",
            "Epoch 2957: train loss: 0.4409666657447815\n",
            "Epoch 2958: train loss: 0.39487630128860474\n",
            "Epoch 2959: train loss: 0.43895819783210754\n",
            "Epoch 2960: train loss: 0.3939281105995178\n",
            "Epoch 2961: train loss: 0.4403674900531769\n",
            "Epoch 2962: train loss: 0.3951526880264282\n",
            "Epoch 2963: train loss: 0.43988654017448425\n",
            "Epoch 2964: train loss: 0.39483994245529175\n",
            "Epoch 2965: train loss: 0.44163864850997925\n",
            "Epoch 2966: train loss: 0.3954218626022339\n",
            "Epoch 2967: train loss: 0.4396193325519562\n",
            "Epoch 2968: train loss: 0.3940559923648834\n",
            "Epoch 2969: train loss: 0.4401596188545227\n",
            "Epoch 2970: train loss: 0.39457058906555176\n",
            "Epoch 2971: train loss: 0.4388347864151001\n",
            "Epoch 2972: train loss: 0.3943278193473816\n",
            "Epoch 2973: train loss: 0.44143059849739075\n",
            "Epoch 2974: train loss: 0.39576926827430725\n",
            "Epoch 2975: train loss: 0.44038125872612\n",
            "Epoch 2976: train loss: 0.3947082757949829\n",
            "Epoch 2977: train loss: 0.4407266676425934\n",
            "Epoch 2978: train loss: 0.3947073817253113\n",
            "Epoch 2979: train loss: 0.4388822317123413\n",
            "Epoch 2980: train loss: 0.3938683271408081\n",
            "Epoch 2981: train loss: 0.4402438700199127\n",
            "Epoch 2982: train loss: 0.39499086141586304\n",
            "Epoch 2983: train loss: 0.439605712890625\n",
            "Epoch 2984: train loss: 0.39470934867858887\n",
            "Epoch 2985: train loss: 0.4417000412940979\n",
            "Epoch 2986: train loss: 0.3955017924308777\n",
            "Epoch 2987: train loss: 0.4396677315235138\n",
            "Epoch 2988: train loss: 0.39398637413978577\n",
            "Epoch 2989: train loss: 0.43991711735725403\n",
            "Epoch 2990: train loss: 0.3944012224674225\n",
            "Epoch 2991: train loss: 0.4386860728263855\n",
            "Epoch 2992: train loss: 0.39423295855522156\n",
            "Epoch 2993: train loss: 0.44136375188827515\n",
            "Epoch 2994: train loss: 0.395741730928421\n",
            "Epoch 2995: train loss: 0.44040507078170776\n",
            "Epoch 2996: train loss: 0.39462828636169434\n",
            "Epoch 2997: train loss: 0.44049617648124695\n",
            "Epoch 2998: train loss: 0.3944847881793976\n",
            "Epoch 2999: train loss: 0.4386594295501709\n",
            "Epoch 3000: train loss: 0.39381271600723267\n",
            "Epoch 3001: train loss: 0.44033023715019226\n",
            "Epoch 3002: train loss: 0.39518630504608154\n",
            "Epoch 3003: train loss: 0.43985387682914734\n",
            "Epoch 3004: train loss: 0.3947102427482605\n",
            "Epoch 3005: train loss: 0.4415235221385956\n",
            "Epoch 3006: train loss: 0.39522862434387207\n",
            "Epoch 3007: train loss: 0.43916839361190796\n",
            "Epoch 3008: train loss: 0.39354225993156433\n",
            "Epoch 3009: train loss: 0.4391532838344574\n",
            "Epoch 3010: train loss: 0.39416393637657166\n",
            "Epoch 3011: train loss: 0.43887507915496826\n",
            "Epoch 3012: train loss: 0.39456039667129517\n",
            "Epoch 3013: train loss: 0.44178369641304016\n",
            "Epoch 3014: train loss: 0.39605221152305603\n",
            "Epoch 3015: train loss: 0.4409114420413971\n",
            "Epoch 3016: train loss: 0.3947776257991791\n",
            "Epoch 3017: train loss: 0.4408290684223175\n",
            "Epoch 3018: train loss: 0.3942815661430359\n",
            "Epoch 3019: train loss: 0.4377060830593109\n",
            "Epoch 3020: train loss: 0.3927888572216034\n",
            "Epoch 3021: train loss: 0.439267098903656\n",
            "Epoch 3022: train loss: 0.39488062262535095\n",
            "Epoch 3023: train loss: 0.439924955368042\n",
            "Epoch 3024: train loss: 0.3953184187412262\n",
            "Epoch 3025: train loss: 0.4424659013748169\n",
            "Epoch 3026: train loss: 0.3959760069847107\n",
            "Epoch 3027: train loss: 0.440340131521225\n",
            "Epoch 3028: train loss: 0.3939739763736725\n",
            "Epoch 3029: train loss: 0.4396137595176697\n",
            "Epoch 3030: train loss: 0.3936489224433899\n",
            "Epoch 3031: train loss: 0.4371432363986969\n",
            "Epoch 3032: train loss: 0.3929018974304199\n",
            "Epoch 3033: train loss: 0.43953776359558105\n",
            "Epoch 3034: train loss: 0.3953483998775482\n",
            "Epoch 3035: train loss: 0.44089993834495544\n",
            "Epoch 3036: train loss: 0.3958781361579895\n",
            "Epoch 3037: train loss: 0.44322723150253296\n",
            "Epoch 3038: train loss: 0.39580658078193665\n",
            "Epoch 3039: train loss: 0.43931588530540466\n",
            "Epoch 3040: train loss: 0.3926834166049957\n",
            "Epoch 3041: train loss: 0.4373164772987366\n",
            "Epoch 3042: train loss: 0.39262089133262634\n",
            "Epoch 3043: train loss: 0.43721261620521545\n",
            "Epoch 3044: train loss: 0.394307017326355\n",
            "Epoch 3045: train loss: 0.4423770010471344\n",
            "Epoch 3046: train loss: 0.3970886170864105\n",
            "Epoch 3047: train loss: 0.44265347719192505\n",
            "Epoch 3048: train loss: 0.3956264555454254\n",
            "Epoch 3049: train loss: 0.44157135486602783\n",
            "Epoch 3050: train loss: 0.3938053846359253\n",
            "Epoch 3051: train loss: 0.43614843487739563\n",
            "Epoch 3052: train loss: 0.3921051621437073\n",
            "Epoch 3053: train loss: 0.4380123019218445\n",
            "Epoch 3054: train loss: 0.394074410200119\n",
            "Epoch 3055: train loss: 0.43938255310058594\n",
            "Epoch 3056: train loss: 0.3953966200351715\n",
            "Epoch 3057: train loss: 0.44308504462242126\n",
            "Epoch 3058: train loss: 0.3964655101299286\n",
            "Epoch 3059: train loss: 0.44093257188796997\n",
            "Epoch 3060: train loss: 0.39412426948547363\n",
            "Epoch 3061: train loss: 0.43963301181793213\n",
            "Epoch 3062: train loss: 0.39323732256889343\n",
            "Epoch 3063: train loss: 0.4362933337688446\n",
            "Epoch 3064: train loss: 0.3922075927257538\n",
            "Epoch 3065: train loss: 0.4387897551059723\n",
            "Epoch 3066: train loss: 0.3952329158782959\n",
            "Epoch 3067: train loss: 0.4411844313144684\n",
            "Epoch 3068: train loss: 0.39630037546157837\n",
            "Epoch 3069: train loss: 0.4438375234603882\n",
            "Epoch 3070: train loss: 0.3959980010986328\n",
            "Epoch 3071: train loss: 0.439456045627594\n",
            "Epoch 3072: train loss: 0.3926420509815216\n",
            "Epoch 3073: train loss: 0.43732115626335144\n",
            "Epoch 3074: train loss: 0.3922652304172516\n",
            "Epoch 3075: train loss: 0.4362458288669586\n",
            "Epoch 3076: train loss: 0.39339616894721985\n",
            "Epoch 3077: train loss: 0.44126078486442566\n",
            "Epoch 3078: train loss: 0.3968643844127655\n",
            "Epoch 3079: train loss: 0.4428987503051758\n",
            "Epoch 3080: train loss: 0.39616358280181885\n",
            "Epoch 3081: train loss: 0.44267532229423523\n",
            "Epoch 3082: train loss: 0.3944229185581207\n",
            "Epoch 3083: train loss: 0.43671441078186035\n",
            "Epoch 3084: train loss: 0.39168834686279297\n",
            "Epoch 3085: train loss: 0.4365909695625305\n",
            "Epoch 3086: train loss: 0.3927057087421417\n",
            "Epoch 3087: train loss: 0.43782737851142883\n",
            "Epoch 3088: train loss: 0.3949889540672302\n",
            "Epoch 3089: train loss: 0.4433359205722809\n",
            "Epoch 3090: train loss: 0.3971501886844635\n",
            "Epoch 3091: train loss: 0.442252516746521\n",
            "Epoch 3092: train loss: 0.3948841691017151\n",
            "Epoch 3093: train loss: 0.44024693965911865\n",
            "Epoch 3094: train loss: 0.3929632902145386\n",
            "Epoch 3095: train loss: 0.43533244729042053\n",
            "Epoch 3096: train loss: 0.3921011686325073\n",
            "Epoch 3097: train loss: 0.4386456608772278\n",
            "Epoch 3098: train loss: 0.3948378264904022\n",
            "Epoch 3099: train loss: 0.4404149651527405\n",
            "Epoch 3100: train loss: 0.39567235112190247\n",
            "Epoch 3101: train loss: 0.4429503083229065\n",
            "Epoch 3102: train loss: 0.39567235112190247\n",
            "Epoch 3103: train loss: 0.439376562833786\n",
            "Epoch 3104: train loss: 0.3929435610771179\n",
            "Epoch 3105: train loss: 0.4382833242416382\n",
            "Epoch 3106: train loss: 0.392854243516922\n",
            "Epoch 3107: train loss: 0.4366385340690613\n",
            "Epoch 3108: train loss: 0.39314571022987366\n",
            "Epoch 3109: train loss: 0.4404131770133972\n",
            "Epoch 3110: train loss: 0.39603450894355774\n",
            "Epoch 3111: train loss: 0.44183650612831116\n",
            "Epoch 3112: train loss: 0.39574289321899414\n",
            "Epoch 3113: train loss: 0.44210338592529297\n",
            "Epoch 3114: train loss: 0.39449214935302734\n",
            "Epoch 3115: train loss: 0.4373321235179901\n",
            "Epoch 3116: train loss: 0.39174237847328186\n",
            "Epoch 3117: train loss: 0.437349408864975\n",
            "Epoch 3118: train loss: 0.39322924613952637\n",
            "Epoch 3119: train loss: 0.43808209896087646\n",
            "Epoch 3120: train loss: 0.39468225836753845\n",
            "Epoch 3121: train loss: 0.4424230754375458\n",
            "Epoch 3122: train loss: 0.39649128913879395\n",
            "Epoch 3123: train loss: 0.44168147444725037\n",
            "Epoch 3124: train loss: 0.3947076201438904\n",
            "Epoch 3125: train loss: 0.4401260018348694\n",
            "Epoch 3126: train loss: 0.39309579133987427\n",
            "Epoch 3127: train loss: 0.4358513653278351\n",
            "Epoch 3128: train loss: 0.39233115315437317\n",
            "Epoch 3129: train loss: 0.4390658438205719\n",
            "Epoch 3130: train loss: 0.3948073387145996\n",
            "Epoch 3131: train loss: 0.4399053454399109\n",
            "Epoch 3132: train loss: 0.3949040174484253\n",
            "Epoch 3133: train loss: 0.44172221422195435\n",
            "Epoch 3134: train loss: 0.3951106369495392\n",
            "Epoch 3135: train loss: 0.43898338079452515\n",
            "Epoch 3136: train loss: 0.39317309856414795\n",
            "Epoch 3137: train loss: 0.43883422017097473\n",
            "Epoch 3138: train loss: 0.3934047222137451\n",
            "Epoch 3139: train loss: 0.43739622831344604\n",
            "Epoch 3140: train loss: 0.39337584376335144\n",
            "Epoch 3141: train loss: 0.44060418009757996\n",
            "Epoch 3142: train loss: 0.3954915404319763\n",
            "Epoch 3143: train loss: 0.44042161107063293\n",
            "Epoch 3144: train loss: 0.3946351706981659\n",
            "Epoch 3145: train loss: 0.4407978355884552\n",
            "Epoch 3146: train loss: 0.3941243588924408\n",
            "Epoch 3147: train loss: 0.43769243359565735\n",
            "Epoch 3148: train loss: 0.39263391494750977\n",
            "Epoch 3149: train loss: 0.4386584162712097\n",
            "Epoch 3150: train loss: 0.3940204977989197\n",
            "Epoch 3151: train loss: 0.4387431740760803\n",
            "Epoch 3152: train loss: 0.39439308643341064\n",
            "Epoch 3153: train loss: 0.44179967045783997\n",
            "Epoch 3154: train loss: 0.3955990970134735\n",
            "Epoch 3155: train loss: 0.43981245160102844\n",
            "Epoch 3156: train loss: 0.3935226798057556\n",
            "Epoch 3157: train loss: 0.4388628900051117\n",
            "Epoch 3158: train loss: 0.3931214213371277\n",
            "Epoch 3159: train loss: 0.4370673596858978\n",
            "Epoch 3160: train loss: 0.3929932415485382\n",
            "Epoch 3161: train loss: 0.4398774802684784\n",
            "Epoch 3162: train loss: 0.3951103687286377\n",
            "Epoch 3163: train loss: 0.4401486814022064\n",
            "Epoch 3164: train loss: 0.394786536693573\n",
            "Epoch 3165: train loss: 0.4416428804397583\n",
            "Epoch 3166: train loss: 0.39479780197143555\n",
            "Epoch 3167: train loss: 0.43819960951805115\n",
            "Epoch 3168: train loss: 0.39242106676101685\n",
            "Epoch 3169: train loss: 0.437867671251297\n",
            "Epoch 3170: train loss: 0.3931374251842499\n",
            "Epoch 3171: train loss: 0.4376578629016876\n",
            "Epoch 3172: train loss: 0.3939343988895416\n",
            "Epoch 3173: train loss: 0.4413643479347229\n",
            "Epoch 3174: train loss: 0.39581090211868286\n",
            "Epoch 3175: train loss: 0.44061121344566345\n",
            "Epoch 3176: train loss: 0.3943747282028198\n",
            "Epoch 3177: train loss: 0.44043269753456116\n",
            "Epoch 3178: train loss: 0.39360639452934265\n",
            "Epoch 3179: train loss: 0.436572402715683\n",
            "Epoch 3180: train loss: 0.39257630705833435\n",
            "Epoch 3181: train loss: 0.43881016969680786\n",
            "Epoch 3182: train loss: 0.3940645158290863\n",
            "Epoch 3183: train loss: 0.4387975335121155\n",
            "Epoch 3184: train loss: 0.39419496059417725\n",
            "Epoch 3185: train loss: 0.4412117898464203\n",
            "Epoch 3186: train loss: 0.3951089382171631\n",
            "Epoch 3187: train loss: 0.4392741024494171\n",
            "Epoch 3188: train loss: 0.3934227526187897\n",
            "Epoch 3189: train loss: 0.43920624256134033\n",
            "Epoch 3190: train loss: 0.3935231566429138\n",
            "Epoch 3191: train loss: 0.43738582730293274\n",
            "Epoch 3192: train loss: 0.39300382137298584\n",
            "Epoch 3193: train loss: 0.43988877534866333\n",
            "Epoch 3194: train loss: 0.3948322832584381\n",
            "Epoch 3195: train loss: 0.4395442605018616\n",
            "Epoch 3196: train loss: 0.39415109157562256\n",
            "Epoch 3197: train loss: 0.4405116140842438\n",
            "Epoch 3198: train loss: 0.39429929852485657\n",
            "Epoch 3199: train loss: 0.43816912174224854\n",
            "Epoch 3200: train loss: 0.39292672276496887\n",
            "Epoch 3201: train loss: 0.43908625841140747\n",
            "Epoch 3202: train loss: 0.3939054608345032\n",
            "Epoch 3203: train loss: 0.4382397532463074\n",
            "Epoch 3204: train loss: 0.3936884105205536\n",
            "Epoch 3205: train loss: 0.440456360578537\n",
            "Epoch 3206: train loss: 0.3947369456291199\n",
            "Epoch 3207: train loss: 0.4390493929386139\n",
            "Epoch 3208: train loss: 0.39361611008644104\n",
            "Epoch 3209: train loss: 0.43996545672416687\n",
            "Epoch 3210: train loss: 0.3939959704875946\n",
            "Epoch 3211: train loss: 0.4378131330013275\n",
            "Epoch 3212: train loss: 0.39290040731430054\n",
            "Epoch 3213: train loss: 0.43907687067985535\n",
            "Epoch 3214: train loss: 0.3940436542034149\n",
            "Epoch 3215: train loss: 0.43875136971473694\n",
            "Epoch 3216: train loss: 0.39394670724868774\n",
            "Epoch 3217: train loss: 0.4406500458717346\n",
            "Epoch 3218: train loss: 0.394636869430542\n",
            "Epoch 3219: train loss: 0.4387023448944092\n",
            "Epoch 3220: train loss: 0.39328062534332275\n",
            "Epoch 3221: train loss: 0.4394434690475464\n",
            "Epoch 3222: train loss: 0.3937355875968933\n",
            "Epoch 3223: train loss: 0.437662273645401\n",
            "Epoch 3224: train loss: 0.3930169641971588\n",
            "Epoch 3225: train loss: 0.4395078122615814\n",
            "Epoch 3226: train loss: 0.39439332485198975\n",
            "Epoch 3227: train loss: 0.43904659152030945\n",
            "Epoch 3228: train loss: 0.3939417600631714\n",
            "Epoch 3229: train loss: 0.4405094385147095\n",
            "Epoch 3230: train loss: 0.3943102955818176\n",
            "Epoch 3231: train loss: 0.43816298246383667\n",
            "Epoch 3232: train loss: 0.392862468957901\n",
            "Epoch 3233: train loss: 0.43892452120780945\n",
            "Epoch 3234: train loss: 0.3936716616153717\n",
            "Epoch 3235: train loss: 0.4378460943698883\n",
            "Epoch 3236: train loss: 0.3934633731842041\n",
            "Epoch 3237: train loss: 0.44036465883255005\n",
            "Epoch 3238: train loss: 0.3947414457798004\n",
            "Epoch 3239: train loss: 0.4390968084335327\n",
            "Epoch 3240: train loss: 0.3934924006462097\n",
            "Epoch 3241: train loss: 0.43954363465309143\n",
            "Epoch 3242: train loss: 0.3936530649662018\n",
            "Epoch 3243: train loss: 0.4374381899833679\n",
            "Epoch 3244: train loss: 0.39283448457717896\n",
            "Epoch 3245: train loss: 0.439512699842453\n",
            "Epoch 3246: train loss: 0.39437225461006165\n",
            "Epoch 3247: train loss: 0.4389020502567291\n",
            "Epoch 3248: train loss: 0.3937598168849945\n",
            "Epoch 3249: train loss: 0.4401399791240692\n",
            "Epoch 3250: train loss: 0.3941483795642853\n",
            "Epoch 3251: train loss: 0.43813425302505493\n",
            "Epoch 3252: train loss: 0.39295583963394165\n",
            "Epoch 3253: train loss: 0.4391593039035797\n",
            "Epoch 3254: train loss: 0.3937799632549286\n",
            "Epoch 3255: train loss: 0.4378969371318817\n",
            "Epoch 3256: train loss: 0.3932829201221466\n",
            "Epoch 3257: train loss: 0.4399340748786926\n",
            "Epoch 3258: train loss: 0.39436018466949463\n",
            "Epoch 3259: train loss: 0.43863585591316223\n",
            "Epoch 3260: train loss: 0.3933655023574829\n",
            "Epoch 3261: train loss: 0.43965399265289307\n",
            "Epoch 3262: train loss: 0.39386892318725586\n",
            "Epoch 3263: train loss: 0.43776383996009827\n",
            "Epoch 3264: train loss: 0.39287978410720825\n",
            "Epoch 3265: train loss: 0.43928736448287964\n",
            "Epoch 3266: train loss: 0.39404067397117615\n",
            "Epoch 3267: train loss: 0.4383706748485565\n",
            "Epoch 3268: train loss: 0.3934780955314636\n",
            "Epoch 3269: train loss: 0.4400791525840759\n",
            "Epoch 3270: train loss: 0.3942292034626007\n",
            "Epoch 3271: train loss: 0.43831169605255127\n",
            "Epoch 3272: train loss: 0.39311152696609497\n",
            "Epoch 3273: train loss: 0.43934473395347595\n",
            "Epoch 3274: train loss: 0.3937605023384094\n",
            "Epoch 3275: train loss: 0.43777409195899963\n",
            "Epoch 3276: train loss: 0.39300140738487244\n",
            "Epoch 3277: train loss: 0.4395314157009125\n",
            "Epoch 3278: train loss: 0.39416638016700745\n",
            "Epoch 3279: train loss: 0.4384571313858032\n",
            "Epoch 3280: train loss: 0.39335837960243225\n",
            "Epoch 3281: train loss: 0.4398007094860077\n",
            "Epoch 3282: train loss: 0.39401963353157043\n",
            "Epoch 3283: train loss: 0.43798136711120605\n",
            "Epoch 3284: train loss: 0.3929138779640198\n",
            "Epoch 3285: train loss: 0.4392186999320984\n",
            "Epoch 3286: train loss: 0.3938254117965698\n",
            "Epoch 3287: train loss: 0.4379862844944\n",
            "Epoch 3288: train loss: 0.39318951964378357\n",
            "Epoch 3289: train loss: 0.43973487615585327\n",
            "Epoch 3290: train loss: 0.39412859082221985\n",
            "Epoch 3291: train loss: 0.43826812505722046\n",
            "Epoch 3292: train loss: 0.3931279182434082\n",
            "Epoch 3293: train loss: 0.4394378364086151\n",
            "Epoch 3294: train loss: 0.39377978444099426\n",
            "Epoch 3295: train loss: 0.4377988278865814\n",
            "Epoch 3296: train loss: 0.39294663071632385\n",
            "Epoch 3297: train loss: 0.4393896460533142\n",
            "Epoch 3298: train loss: 0.3939933776855469\n",
            "Epoch 3299: train loss: 0.4381959140300751\n",
            "Epoch 3300: train loss: 0.3931807279586792\n",
            "Epoch 3301: train loss: 0.4396032691001892\n",
            "Epoch 3302: train loss: 0.3939351737499237\n",
            "Epoch 3303: train loss: 0.4379517734050751\n",
            "Epoch 3304: train loss: 0.39295831322669983\n",
            "Epoch 3305: train loss: 0.4393099844455719\n",
            "Epoch 3306: train loss: 0.39378300309181213\n",
            "Epoch 3307: train loss: 0.43787676095962524\n",
            "Epoch 3308: train loss: 0.39304810762405396\n",
            "Epoch 3309: train loss: 0.4395599961280823\n",
            "Epoch 3310: train loss: 0.3940109610557556\n",
            "Epoch 3311: train loss: 0.43816065788269043\n",
            "Epoch 3312: train loss: 0.3930775821208954\n",
            "Epoch 3313: train loss: 0.4394507110118866\n",
            "Epoch 3314: train loss: 0.39382681250572205\n",
            "Epoch 3315: train loss: 0.4378470778465271\n",
            "Epoch 3316: train loss: 0.39286163449287415\n",
            "Epoch 3317: train loss: 0.4392322301864624\n",
            "Epoch 3318: train loss: 0.3937743902206421\n",
            "Epoch 3319: train loss: 0.43795326352119446\n",
            "Epoch 3320: train loss: 0.3931540548801422\n",
            "Epoch 3321: train loss: 0.43969276547431946\n",
            "Epoch 3322: train loss: 0.39395517110824585\n",
            "Epoch 3323: train loss: 0.43796417117118835\n",
            "Epoch 3324: train loss: 0.392869234085083\n",
            "Epoch 3325: train loss: 0.43913567066192627\n",
            "Epoch 3326: train loss: 0.3936203718185425\n",
            "Epoch 3327: train loss: 0.4376658499240875\n",
            "Epoch 3328: train loss: 0.3928644359111786\n",
            "Epoch 3329: train loss: 0.4393712878227234\n",
            "Epoch 3330: train loss: 0.39394575357437134\n",
            "Epoch 3331: train loss: 0.438174843788147\n",
            "Epoch 3332: train loss: 0.3931407928466797\n",
            "Epoch 3333: train loss: 0.4395613968372345\n",
            "Epoch 3334: train loss: 0.3937821686267853\n",
            "Epoch 3335: train loss: 0.43774715065956116\n",
            "Epoch 3336: train loss: 0.39276954531669617\n",
            "Epoch 3337: train loss: 0.4390712380409241\n",
            "Epoch 3338: train loss: 0.3936111629009247\n",
            "Epoch 3339: train loss: 0.4377026855945587\n",
            "Epoch 3340: train loss: 0.39286911487579346\n",
            "Epoch 3341: train loss: 0.43936872482299805\n",
            "Epoch 3342: train loss: 0.39389437437057495\n",
            "Epoch 3343: train loss: 0.43807849287986755\n",
            "Epoch 3344: train loss: 0.3930443823337555\n",
            "Epoch 3345: train loss: 0.43942466378211975\n",
            "Epoch 3346: train loss: 0.39370042085647583\n",
            "Epoch 3347: train loss: 0.4376533031463623\n",
            "Epoch 3348: train loss: 0.39344897866249084\n",
            "Epoch 3349: train loss: 0.4400125741958618\n",
            "Epoch 3350: train loss: 0.3937380015850067\n",
            "Epoch 3351: train loss: 0.4373478591442108\n",
            "Epoch 3352: train loss: 0.3930262625217438\n",
            "Epoch 3353: train loss: 0.4392932951450348\n",
            "Epoch 3354: train loss: 0.3933010697364807\n",
            "Epoch 3355: train loss: 0.43693807721138\n",
            "Epoch 3356: train loss: 0.3930422067642212\n",
            "Epoch 3357: train loss: 0.4396456480026245\n",
            "Epoch 3358: train loss: 0.3937498927116394\n",
            "Epoch 3359: train loss: 0.43762993812561035\n",
            "Epoch 3360: train loss: 0.3932732045650482\n",
            "Epoch 3361: train loss: 0.4397152066230774\n",
            "Epoch 3362: train loss: 0.39354050159454346\n",
            "Epoch 3363: train loss: 0.437168151140213\n",
            "Epoch 3364: train loss: 0.39297616481781006\n",
            "Epoch 3365: train loss: 0.4393969774246216\n",
            "Epoch 3366: train loss: 0.39351263642311096\n",
            "Epoch 3367: train loss: 0.43733301758766174\n",
            "Epoch 3368: train loss: 0.3931995928287506\n",
            "Epoch 3369: train loss: 0.439738929271698\n",
            "Epoch 3370: train loss: 0.3936322033405304\n",
            "Epoch 3371: train loss: 0.4373621344566345\n",
            "Epoch 3372: train loss: 0.393065482378006\n",
            "Epoch 3373: train loss: 0.43950212001800537\n",
            "Epoch 3374: train loss: 0.39350980520248413\n",
            "Epoch 3375: train loss: 0.43724414706230164\n",
            "Epoch 3376: train loss: 0.3930473029613495\n",
            "Epoch 3377: train loss: 0.4395374059677124\n",
            "Epoch 3378: train loss: 0.39356279373168945\n",
            "Epoch 3379: train loss: 0.4373426139354706\n",
            "Epoch 3380: train loss: 0.3930818736553192\n",
            "Epoch 3381: train loss: 0.43955016136169434\n",
            "Epoch 3382: train loss: 0.39352497458457947\n",
            "Epoch 3383: train loss: 0.4372611939907074\n",
            "Epoch 3384: train loss: 0.39301589131355286\n",
            "Epoch 3385: train loss: 0.43942007422447205\n",
            "Epoch 3386: train loss: 0.39346158504486084\n",
            "Epoch 3387: train loss: 0.43724891543388367\n",
            "Epoch 3388: train loss: 0.3931035101413727\n",
            "Epoch 3389: train loss: 0.43979352712631226\n",
            "Epoch 3390: train loss: 0.39361947774887085\n",
            "Epoch 3391: train loss: 0.4372910261154175\n",
            "Epoch 3392: train loss: 0.39292415976524353\n",
            "Epoch 3393: train loss: 0.43907803297042847\n",
            "Epoch 3394: train loss: 0.39312198758125305\n",
            "Epoch 3395: train loss: 0.436954528093338\n",
            "Epoch 3396: train loss: 0.3931031823158264\n",
            "Epoch 3397: train loss: 0.43984678387641907\n",
            "Epoch 3398: train loss: 0.39379382133483887\n",
            "Epoch 3399: train loss: 0.4376516044139862\n",
            "Epoch 3400: train loss: 0.39309558272361755\n",
            "Epoch 3401: train loss: 0.43933790922164917\n",
            "Epoch 3402: train loss: 0.3932017982006073\n",
            "Epoch 3403: train loss: 0.43680134415626526\n",
            "Epoch 3404: train loss: 0.39285290241241455\n",
            "Epoch 3405: train loss: 0.43956121802330017\n",
            "Epoch 3406: train loss: 0.3935842216014862\n",
            "Epoch 3407: train loss: 0.43738117814064026\n",
            "Epoch 3408: train loss: 0.3930083215236664\n",
            "Epoch 3409: train loss: 0.43926817178726196\n",
            "Epoch 3410: train loss: 0.3932633101940155\n",
            "Epoch 3411: train loss: 0.43700382113456726\n",
            "Epoch 3412: train loss: 0.39298519492149353\n",
            "Epoch 3413: train loss: 0.43973302841186523\n",
            "Epoch 3414: train loss: 0.3936467170715332\n",
            "Epoch 3415: train loss: 0.43737536668777466\n",
            "Epoch 3416: train loss: 0.3928879201412201\n",
            "Epoch 3417: train loss: 0.4390222430229187\n",
            "Epoch 3418: train loss: 0.3929881751537323\n",
            "Epoch 3419: train loss: 0.43667304515838623\n",
            "Epoch 3420: train loss: 0.39292389154434204\n",
            "Epoch 3421: train loss: 0.43983250856399536\n",
            "Epoch 3422: train loss: 0.3938031494617462\n",
            "Epoch 3423: train loss: 0.43765485286712646\n",
            "Epoch 3424: train loss: 0.3929903507232666\n",
            "Epoch 3425: train loss: 0.43907758593559265\n",
            "Epoch 3426: train loss: 0.3929542601108551\n",
            "Epoch 3427: train loss: 0.4365365505218506\n",
            "Epoch 3428: train loss: 0.39275962114334106\n",
            "Epoch 3429: train loss: 0.4395883083343506\n",
            "Epoch 3430: train loss: 0.3936876356601715\n",
            "Epoch 3431: train loss: 0.43757835030555725\n",
            "Epoch 3432: train loss: 0.3930000960826874\n",
            "Epoch 3433: train loss: 0.43910014629364014\n",
            "Epoch 3434: train loss: 0.39303895831108093\n",
            "Epoch 3435: train loss: 0.4368169605731964\n",
            "Epoch 3436: train loss: 0.39277592301368713\n",
            "Epoch 3437: train loss: 0.43923890590667725\n",
            "Epoch 3438: train loss: 0.39333376288414\n",
            "Epoch 3439: train loss: 0.4372023940086365\n",
            "Epoch 3440: train loss: 0.3930145502090454\n",
            "Epoch 3441: train loss: 0.43968307971954346\n",
            "Epoch 3442: train loss: 0.3934662640094757\n",
            "Epoch 3443: train loss: 0.4370773434638977\n",
            "Epoch 3444: train loss: 0.392658531665802\n",
            "Epoch 3445: train loss: 0.4388110935688019\n",
            "Epoch 3446: train loss: 0.393740177154541\n",
            "Epoch 3447: train loss: 0.4378158450126648\n",
            "Epoch 3448: train loss: 0.3930742144584656\n",
            "Epoch 3449: train loss: 0.4393686056137085\n",
            "Epoch 3450: train loss: 0.39370211958885193\n",
            "Epoch 3451: train loss: 0.4372597336769104\n",
            "Epoch 3452: train loss: 0.3922872841358185\n",
            "Epoch 3453: train loss: 0.43779680132865906\n",
            "Epoch 3454: train loss: 0.3928934335708618\n",
            "Epoch 3455: train loss: 0.4370202422142029\n",
            "Epoch 3456: train loss: 0.3929048180580139\n",
            "Epoch 3457: train loss: 0.4393850266933441\n",
            "Epoch 3458: train loss: 0.39328983426094055\n",
            "Epoch 3459: train loss: 0.4370635747909546\n",
            "Epoch 3460: train loss: 0.3928357660770416\n",
            "Epoch 3461: train loss: 0.43944376707077026\n",
            "Epoch 3462: train loss: 0.3932872712612152\n",
            "Epoch 3463: train loss: 0.4369198977947235\n",
            "Epoch 3464: train loss: 0.392621248960495\n",
            "Epoch 3465: train loss: 0.4389009177684784\n",
            "Epoch 3466: train loss: 0.39389169216156006\n",
            "Epoch 3467: train loss: 0.4380419850349426\n",
            "Epoch 3468: train loss: 0.3930290639400482\n",
            "Epoch 3469: train loss: 0.4392002820968628\n",
            "Epoch 3470: train loss: 0.39347144961357117\n",
            "Epoch 3471: train loss: 0.43694180250167847\n",
            "Epoch 3472: train loss: 0.3920590877532959\n",
            "Epoch 3473: train loss: 0.43751659989356995\n",
            "Epoch 3474: train loss: 0.39285969734191895\n",
            "Epoch 3475: train loss: 0.4371411204338074\n",
            "Epoch 3476: train loss: 0.3930383324623108\n",
            "Epoch 3477: train loss: 0.43962520360946655\n",
            "Epoch 3478: train loss: 0.39339134097099304\n",
            "Epoch 3479: train loss: 0.4370775818824768\n",
            "Epoch 3480: train loss: 0.39260125160217285\n",
            "Epoch 3481: train loss: 0.43890634179115295\n",
            "Epoch 3482: train loss: 0.3937106728553772\n",
            "Epoch 3483: train loss: 0.4377055764198303\n",
            "Epoch 3484: train loss: 0.39278414845466614\n",
            "Epoch 3485: train loss: 0.4387210011482239\n",
            "Epoch 3486: train loss: 0.3932496905326843\n",
            "Epoch 3487: train loss: 0.43693476915359497\n",
            "Epoch 3488: train loss: 0.3923851251602173\n",
            "Epoch 3489: train loss: 0.43862664699554443\n",
            "Epoch 3490: train loss: 0.3934725224971771\n",
            "Epoch 3491: train loss: 0.43740424513816833\n",
            "Epoch 3492: train loss: 0.3927014470100403\n",
            "Epoch 3493: train loss: 0.4387269616127014\n",
            "Epoch 3494: train loss: 0.3934027850627899\n",
            "Epoch 3495: train loss: 0.4372526705265045\n",
            "Epoch 3496: train loss: 0.3925936222076416\n",
            "Epoch 3497: train loss: 0.4388261139392853\n",
            "Epoch 3498: train loss: 0.393474280834198\n",
            "Epoch 3499: train loss: 0.43723607063293457\n",
            "Epoch 3500: train loss: 0.3923775255680084\n",
            "Epoch 3501: train loss: 0.43824300169944763\n",
            "Epoch 3502: train loss: 0.39314696192741394\n",
            "Epoch 3503: train loss: 0.43701666593551636\n",
            "Epoch 3504: train loss: 0.3926449418067932\n",
            "Epoch 3505: train loss: 0.43912273645401\n",
            "Epoch 3506: train loss: 0.39378201961517334\n",
            "Epoch 3507: train loss: 0.43766355514526367\n",
            "Epoch 3508: train loss: 0.3925153613090515\n",
            "Epoch 3509: train loss: 0.438233882188797\n",
            "Epoch 3510: train loss: 0.39292290806770325\n",
            "Epoch 3511: train loss: 0.4365762174129486\n",
            "Epoch 3512: train loss: 0.3923282027244568\n",
            "Epoch 3513: train loss: 0.4387770891189575\n",
            "Epoch 3514: train loss: 0.3936815857887268\n",
            "Epoch 3515: train loss: 0.43774867057800293\n",
            "Epoch 3516: train loss: 0.39280152320861816\n",
            "Epoch 3517: train loss: 0.43872252106666565\n",
            "Epoch 3518: train loss: 0.3931753933429718\n",
            "Epoch 3519: train loss: 0.436822772026062\n",
            "Epoch 3520: train loss: 0.3922266364097595\n",
            "Epoch 3521: train loss: 0.4384119212627411\n",
            "Epoch 3522: train loss: 0.39324161410331726\n",
            "Epoch 3523: train loss: 0.437156081199646\n",
            "Epoch 3524: train loss: 0.3925879895687103\n",
            "Epoch 3525: train loss: 0.4387045204639435\n",
            "Epoch 3526: train loss: 0.39338991045951843\n",
            "Epoch 3527: train loss: 0.43729597330093384\n",
            "Epoch 3528: train loss: 0.3925423324108124\n",
            "Epoch 3529: train loss: 0.43875056505203247\n",
            "Epoch 3530: train loss: 0.39331525564193726\n",
            "Epoch 3531: train loss: 0.4370242655277252\n",
            "Epoch 3532: train loss: 0.39222365617752075\n",
            "Epoch 3533: train loss: 0.4380759000778198\n",
            "Epoch 3534: train loss: 0.39300692081451416\n",
            "Epoch 3535: train loss: 0.4369139075279236\n",
            "Epoch 3536: train loss: 0.39254289865493774\n",
            "Epoch 3537: train loss: 0.43900781869888306\n",
            "Epoch 3538: train loss: 0.39366772770881653\n",
            "Epoch 3539: train loss: 0.4375566840171814\n",
            "Epoch 3540: train loss: 0.3924499750137329\n",
            "Epoch 3541: train loss: 0.43814679980278015\n",
            "Epoch 3542: train loss: 0.39278218150138855\n",
            "Epoch 3543: train loss: 0.43639349937438965\n",
            "Epoch 3544: train loss: 0.39215269684791565\n",
            "Epoch 3545: train loss: 0.43859952688217163\n",
            "Epoch 3546: train loss: 0.39358997344970703\n",
            "Epoch 3547: train loss: 0.4376870393753052\n",
            "Epoch 3548: train loss: 0.3926798403263092\n",
            "Epoch 3549: train loss: 0.43857628107070923\n",
            "Epoch 3550: train loss: 0.3930667042732239\n",
            "Epoch 3551: train loss: 0.4367431104183197\n",
            "Epoch 3552: train loss: 0.3920822739601135\n",
            "Epoch 3553: train loss: 0.43818357586860657\n",
            "Epoch 3554: train loss: 0.39308595657348633\n",
            "Epoch 3555: train loss: 0.43703493475914\n",
            "Epoch 3556: train loss: 0.3924688398838043\n",
            "Epoch 3557: train loss: 0.4385814666748047\n",
            "Epoch 3558: train loss: 0.39329296350479126\n",
            "Epoch 3559: train loss: 0.4372076392173767\n",
            "Epoch 3560: train loss: 0.39239922165870667\n",
            "Epoch 3561: train loss: 0.4385831952095032\n",
            "Epoch 3562: train loss: 0.39319944381713867\n",
            "Epoch 3563: train loss: 0.43691688776016235\n",
            "Epoch 3564: train loss: 0.39214783906936646\n",
            "Epoch 3565: train loss: 0.4379827082157135\n",
            "Epoch 3566: train loss: 0.39284661412239075\n",
            "Epoch 3567: train loss: 0.4367663860321045\n",
            "Epoch 3568: train loss: 0.3924456834793091\n",
            "Epoch 3569: train loss: 0.4389503002166748\n",
            "Epoch 3570: train loss: 0.39356687664985657\n",
            "Epoch 3571: train loss: 0.43742698431015015\n",
            "Epoch 3572: train loss: 0.3923010528087616\n",
            "Epoch 3573: train loss: 0.4379679560661316\n",
            "Epoch 3574: train loss: 0.39262256026268005\n",
            "Epoch 3575: train loss: 0.4362194240093231\n",
            "Epoch 3576: train loss: 0.392011433839798\n",
            "Epoch 3577: train loss: 0.4384603798389435\n",
            "Epoch 3578: train loss: 0.39347004890441895\n",
            "Epoch 3579: train loss: 0.4375843405723572\n",
            "Epoch 3580: train loss: 0.39257609844207764\n",
            "Epoch 3581: train loss: 0.43847107887268066\n",
            "Epoch 3582: train loss: 0.39294862747192383\n",
            "Epoch 3583: train loss: 0.43659043312072754\n",
            "Epoch 3584: train loss: 0.3920241594314575\n",
            "Epoch 3585: train loss: 0.4382023811340332\n",
            "Epoch 3586: train loss: 0.3930308520793915\n",
            "Epoch 3587: train loss: 0.4369042217731476\n",
            "Epoch 3588: train loss: 0.39227205514907837\n",
            "Epoch 3589: train loss: 0.4382956922054291\n",
            "Epoch 3590: train loss: 0.39307865500450134\n",
            "Epoch 3591: train loss: 0.43697988986968994\n",
            "Epoch 3592: train loss: 0.3922622799873352\n",
            "Epoch 3593: train loss: 0.4384828805923462\n",
            "Epoch 3594: train loss: 0.3931213319301605\n",
            "Epoch 3595: train loss: 0.43686509132385254\n",
            "Epoch 3596: train loss: 0.3920418620109558\n",
            "Epoch 3597: train loss: 0.4378584623336792\n",
            "Epoch 3598: train loss: 0.39273008704185486\n",
            "Epoch 3599: train loss: 0.43662723898887634\n",
            "Epoch 3600: train loss: 0.3922782838344574\n",
            "Epoch 3601: train loss: 0.4387598931789398\n",
            "Epoch 3602: train loss: 0.39342981576919556\n",
            "Epoch 3603: train loss: 0.4373116195201874\n",
            "Epoch 3604: train loss: 0.3922135531902313\n",
            "Epoch 3605: train loss: 0.4378967583179474\n",
            "Epoch 3606: train loss: 0.3924967050552368\n",
            "Epoch 3607: train loss: 0.43607622385025024\n",
            "Epoch 3608: train loss: 0.3918982744216919\n",
            "Epoch 3609: train loss: 0.43834188580513\n",
            "Epoch 3610: train loss: 0.39332497119903564\n",
            "Epoch 3611: train loss: 0.4374101161956787\n",
            "Epoch 3612: train loss: 0.39242371916770935\n",
            "Epoch 3613: train loss: 0.4383106231689453\n",
            "Epoch 3614: train loss: 0.39282047748565674\n",
            "Epoch 3615: train loss: 0.4364725351333618\n",
            "Epoch 3616: train loss: 0.3919053375720978\n",
            "Epoch 3617: train loss: 0.4380752444267273\n",
            "Epoch 3618: train loss: 0.39290449023246765\n",
            "Epoch 3619: train loss: 0.4367709159851074\n",
            "Epoch 3620: train loss: 0.3921321928501129\n",
            "Epoch 3621: train loss: 0.43812480568885803\n",
            "Epoch 3622: train loss: 0.39292848110198975\n",
            "Epoch 3623: train loss: 0.4368290901184082\n",
            "Epoch 3624: train loss: 0.3922073543071747\n",
            "Epoch 3625: train loss: 0.43847736716270447\n",
            "Epoch 3626: train loss: 0.3930334746837616\n",
            "Epoch 3627: train loss: 0.4367363452911377\n",
            "Epoch 3628: train loss: 0.39187631011009216\n",
            "Epoch 3629: train loss: 0.43762072920799255\n",
            "Epoch 3630: train loss: 0.3925102651119232\n",
            "Epoch 3631: train loss: 0.436313271522522\n",
            "Epoch 3632: train loss: 0.3921162784099579\n",
            "Epoch 3633: train loss: 0.438647598028183\n",
            "Epoch 3634: train loss: 0.3934202492237091\n",
            "Epoch 3635: train loss: 0.43736135959625244\n",
            "Epoch 3636: train loss: 0.3921206593513489\n",
            "Epoch 3637: train loss: 0.4377216696739197\n",
            "Epoch 3638: train loss: 0.39229121804237366\n",
            "Epoch 3639: train loss: 0.43579721450805664\n",
            "Epoch 3640: train loss: 0.3917087912559509\n",
            "Epoch 3641: train loss: 0.43807774782180786\n",
            "Epoch 3642: train loss: 0.39318057894706726\n",
            "Epoch 3643: train loss: 0.43732181191444397\n",
            "Epoch 3644: train loss: 0.39242878556251526\n",
            "Epoch 3645: train loss: 0.43839365243911743\n",
            "Epoch 3646: train loss: 0.3927561640739441\n",
            "Epoch 3647: train loss: 0.43633249402046204\n",
            "Epoch 3648: train loss: 0.3916887044906616\n",
            "Epoch 3649: train loss: 0.4377053678035736\n",
            "Epoch 3650: train loss: 0.3926777243614197\n",
            "Epoch 3651: train loss: 0.43655577301979065\n",
            "Epoch 3652: train loss: 0.3920019865036011\n",
            "Epoch 3653: train loss: 0.43806126713752747\n",
            "Epoch 3654: train loss: 0.39289966225624084\n",
            "Epoch 3655: train loss: 0.43685564398765564\n",
            "Epoch 3656: train loss: 0.39215660095214844\n",
            "Epoch 3657: train loss: 0.4383884370326996\n",
            "Epoch 3658: train loss: 0.3928942084312439\n",
            "Epoch 3659: train loss: 0.43653377890586853\n",
            "Epoch 3660: train loss: 0.3916832506656647\n",
            "Epoch 3661: train loss: 0.4373112916946411\n",
            "Epoch 3662: train loss: 0.3923085629940033\n",
            "Epoch 3663: train loss: 0.43611064553260803\n",
            "Epoch 3664: train loss: 0.39198729395866394\n",
            "Epoch 3665: train loss: 0.4384855628013611\n",
            "Epoch 3666: train loss: 0.39332613348960876\n",
            "Epoch 3667: train loss: 0.43732354044914246\n",
            "Epoch 3668: train loss: 0.3921492397785187\n",
            "Epoch 3669: train loss: 0.4378317892551422\n",
            "Epoch 3670: train loss: 0.3922731280326843\n",
            "Epoch 3671: train loss: 0.4357723593711853\n",
            "Epoch 3672: train loss: 0.39150458574295044\n",
            "Epoch 3673: train loss: 0.4377128779888153\n",
            "Epoch 3674: train loss: 0.39281097054481506\n",
            "Epoch 3675: train loss: 0.43689337372779846\n",
            "Epoch 3676: train loss: 0.3922377824783325\n",
            "Epoch 3677: train loss: 0.4382675588130951\n",
            "Epoch 3678: train loss: 0.3928685486316681\n",
            "Epoch 3679: train loss: 0.4366063177585602\n",
            "Epoch 3680: train loss: 0.3917761445045471\n",
            "Epoch 3681: train loss: 0.43769949674606323\n",
            "Epoch 3682: train loss: 0.3924180567264557\n",
            "Epoch 3683: train loss: 0.4360911548137665\n",
            "Epoch 3684: train loss: 0.3917056620121002\n",
            "Epoch 3685: train loss: 0.4376884400844574\n",
            "Epoch 3686: train loss: 0.3926948606967926\n",
            "Epoch 3687: train loss: 0.43672090768814087\n",
            "Epoch 3688: train loss: 0.39211228489875793\n",
            "Epoch 3689: train loss: 0.43835964798927307\n",
            "Epoch 3690: train loss: 0.3929259479045868\n",
            "Epoch 3691: train loss: 0.4366031289100647\n",
            "Epoch 3692: train loss: 0.39157599210739136\n",
            "Epoch 3693: train loss: 0.43713533878326416\n",
            "Epoch 3694: train loss: 0.3921137750148773\n",
            "Epoch 3695: train loss: 0.43586426973342896\n",
            "Epoch 3696: train loss: 0.3918086290359497\n",
            "Epoch 3697: train loss: 0.43830597400665283\n",
            "Epoch 3698: train loss: 0.39318087697029114\n",
            "Epoch 3699: train loss: 0.437203586101532\n",
            "Epoch 3700: train loss: 0.39207321405410767\n",
            "Epoch 3701: train loss: 0.4377187192440033\n",
            "Epoch 3702: train loss: 0.39215245842933655\n",
            "Epoch 3703: train loss: 0.4356456398963928\n",
            "Epoch 3704: train loss: 0.39138004183769226\n",
            "Epoch 3705: train loss: 0.43758028745651245\n",
            "Epoch 3706: train loss: 0.39269939064979553\n",
            "Epoch 3707: train loss: 0.43676602840423584\n",
            "Epoch 3708: train loss: 0.3921322822570801\n",
            "Epoch 3709: train loss: 0.4381530284881592\n",
            "Epoch 3710: train loss: 0.3927312195301056\n",
            "Epoch 3711: train loss: 0.4364413917064667\n",
            "Epoch 3712: train loss: 0.39162179827690125\n",
            "Epoch 3713: train loss: 0.43753039836883545\n",
            "Epoch 3714: train loss: 0.3923034071922302\n",
            "Epoch 3715: train loss: 0.43598076701164246\n",
            "Epoch 3716: train loss: 0.3915550112724304\n",
            "Epoch 3717: train loss: 0.4375302791595459\n",
            "Epoch 3718: train loss: 0.39254140853881836\n",
            "Epoch 3719: train loss: 0.4365776777267456\n",
            "Epoch 3720: train loss: 0.3920711278915405\n",
            "Epoch 3721: train loss: 0.438367635011673\n",
            "Epoch 3722: train loss: 0.3928420841693878\n",
            "Epoch 3723: train loss: 0.43647071719169617\n",
            "Epoch 3724: train loss: 0.3914008140563965\n",
            "Epoch 3725: train loss: 0.436908483505249\n",
            "Epoch 3726: train loss: 0.3919326663017273\n",
            "Epoch 3727: train loss: 0.43559956550598145\n",
            "Epoch 3728: train loss: 0.3915667235851288\n",
            "Epoch 3729: train loss: 0.43807363510131836\n",
            "Epoch 3730: train loss: 0.39312225580215454\n",
            "Epoch 3731: train loss: 0.4372575581073761\n",
            "Epoch 3732: train loss: 0.3920673131942749\n",
            "Epoch 3733: train loss: 0.4377194941043854\n",
            "Epoch 3734: train loss: 0.3920385241508484\n",
            "Epoch 3735: train loss: 0.4353775978088379\n",
            "Epoch 3736: train loss: 0.39112162590026855\n",
            "Epoch 3737: train loss: 0.4372192621231079\n",
            "Epoch 3738: train loss: 0.39247363805770874\n",
            "Epoch 3739: train loss: 0.43656063079833984\n",
            "Epoch 3740: train loss: 0.3920649588108063\n",
            "Epoch 3741: train loss: 0.4381590783596039\n",
            "Epoch 3742: train loss: 0.39267775416374207\n",
            "Epoch 3743: train loss: 0.43639513850212097\n",
            "Epoch 3744: train loss: 0.39152753353118896\n",
            "Epoch 3745: train loss: 0.4373008608818054\n",
            "Epoch 3746: train loss: 0.3920717239379883\n",
            "Epoch 3747: train loss: 0.4356953203678131\n",
            "Epoch 3748: train loss: 0.39138364791870117\n",
            "Epoch 3749: train loss: 0.4375269412994385\n",
            "Epoch 3750: train loss: 0.3925832211971283\n",
            "Epoch 3751: train loss: 0.43659770488739014\n",
            "Epoch 3752: train loss: 0.39194124937057495\n",
            "Epoch 3753: train loss: 0.43799927830696106\n",
            "Epoch 3754: train loss: 0.39250653982162476\n",
            "Epoch 3755: train loss: 0.43605098128318787\n",
            "Epoch 3756: train loss: 0.39123421907424927\n",
            "Epoch 3757: train loss: 0.43685200810432434\n",
            "Epoch 3758: train loss: 0.3919712007045746\n",
            "Epoch 3759: train loss: 0.43574270606040955\n",
            "Epoch 3760: train loss: 0.39162445068359375\n",
            "Epoch 3761: train loss: 0.4380497932434082\n",
            "Epoch 3762: train loss: 0.39288464188575745\n",
            "Epoch 3763: train loss: 0.4368129372596741\n",
            "Epoch 3764: train loss: 0.39172840118408203\n",
            "Epoch 3765: train loss: 0.43731898069381714\n",
            "Epoch 3766: train loss: 0.3918715715408325\n",
            "Epoch 3767: train loss: 0.43525320291519165\n",
            "Epoch 3768: train loss: 0.39109891653060913\n",
            "Epoch 3769: train loss: 0.43728336691856384\n",
            "Epoch 3770: train loss: 0.39247921109199524\n",
            "Epoch 3771: train loss: 0.4365975558757782\n",
            "Epoch 3772: train loss: 0.39199087023735046\n",
            "Epoch 3773: train loss: 0.4381159543991089\n",
            "Epoch 3774: train loss: 0.39252379536628723\n",
            "Epoch 3775: train loss: 0.4360519051551819\n",
            "Epoch 3776: train loss: 0.3911402225494385\n",
            "Epoch 3777: train loss: 0.4366866946220398\n",
            "Epoch 3778: train loss: 0.3918105959892273\n",
            "Epoch 3779: train loss: 0.43553391098976135\n",
            "Epoch 3780: train loss: 0.3914416432380676\n",
            "Epoch 3781: train loss: 0.43785011768341064\n",
            "Epoch 3782: train loss: 0.39282840490341187\n",
            "Epoch 3783: train loss: 0.43685397505760193\n",
            "Epoch 3784: train loss: 0.3917750120162964\n",
            "Epoch 3785: train loss: 0.4374454617500305\n",
            "Epoch 3786: train loss: 0.3918668329715729\n",
            "Epoch 3787: train loss: 0.4352657198905945\n",
            "Epoch 3788: train loss: 0.3909945785999298\n",
            "Epoch 3789: train loss: 0.43705683946609497\n",
            "Epoch 3790: train loss: 0.3922821879386902\n",
            "Epoch 3791: train loss: 0.43635889887809753\n",
            "Epoch 3792: train loss: 0.3918958008289337\n",
            "Epoch 3793: train loss: 0.4381273090839386\n",
            "Epoch 3794: train loss: 0.39261290431022644\n",
            "Epoch 3795: train loss: 0.4362199604511261\n",
            "Epoch 3796: train loss: 0.39119774103164673\n",
            "Epoch 3797: train loss: 0.43677619099617004\n",
            "Epoch 3798: train loss: 0.3916650414466858\n",
            "Epoch 3799: train loss: 0.43521761894226074\n",
            "Epoch 3800: train loss: 0.3911937177181244\n",
            "Epoch 3801: train loss: 0.4373309016227722\n",
            "Epoch 3802: train loss: 0.39253488183021545\n",
            "Epoch 3803: train loss: 0.4366300702095032\n",
            "Epoch 3804: train loss: 0.39181050658226013\n",
            "Epoch 3805: train loss: 0.4378437101840973\n",
            "Epoch 3806: train loss: 0.39230281114578247\n",
            "Epoch 3807: train loss: 0.4357874393463135\n",
            "Epoch 3808: train loss: 0.3909112215042114\n",
            "Epoch 3809: train loss: 0.43660688400268555\n",
            "Epoch 3810: train loss: 0.391855925321579\n",
            "Epoch 3811: train loss: 0.4356399178504944\n",
            "Epoch 3812: train loss: 0.39135193824768066\n",
            "Epoch 3813: train loss: 0.4374733567237854\n",
            "Epoch 3814: train loss: 0.39248088002204895\n",
            "Epoch 3815: train loss: 0.4364832043647766\n",
            "Epoch 3816: train loss: 0.3917343020439148\n",
            "Epoch 3817: train loss: 0.4376991093158722\n",
            "Epoch 3818: train loss: 0.39213085174560547\n",
            "Epoch 3819: train loss: 0.4355396330356598\n",
            "Epoch 3820: train loss: 0.3907841742038727\n",
            "Epoch 3821: train loss: 0.4363577961921692\n",
            "Epoch 3822: train loss: 0.39171501994132996\n",
            "Epoch 3823: train loss: 0.4356061518192291\n",
            "Epoch 3824: train loss: 0.39156851172447205\n",
            "Epoch 3825: train loss: 0.43786370754241943\n",
            "Epoch 3826: train loss: 0.39264893531799316\n",
            "Epoch 3827: train loss: 0.4365273118019104\n",
            "Epoch 3828: train loss: 0.3914411962032318\n",
            "Epoch 3829: train loss: 0.43719133734703064\n",
            "Epoch 3830: train loss: 0.3917955458164215\n",
            "Epoch 3831: train loss: 0.4352380931377411\n",
            "Epoch 3832: train loss: 0.3908066749572754\n",
            "Epoch 3833: train loss: 0.43659549951553345\n",
            "Epoch 3834: train loss: 0.39190155267715454\n",
            "Epoch 3835: train loss: 0.4358615577220917\n",
            "Epoch 3836: train loss: 0.39162081480026245\n",
            "Epoch 3837: train loss: 0.4378299415111542\n",
            "Epoch 3838: train loss: 0.39249566197395325\n",
            "Epoch 3839: train loss: 0.436282753944397\n",
            "Epoch 3840: train loss: 0.3913811147212982\n",
            "Epoch 3841: train loss: 0.4371167719364166\n",
            "Epoch 3842: train loss: 0.3916302025318146\n",
            "Epoch 3843: train loss: 0.4349503815174103\n",
            "Epoch 3844: train loss: 0.3915086090564728\n",
            "Epoch 3845: train loss: 0.4377913177013397\n",
            "Epoch 3846: train loss: 0.3922966718673706\n",
            "Epoch 3847: train loss: 0.4358747601509094\n",
            "Epoch 3848: train loss: 0.39092883467674255\n",
            "Epoch 3849: train loss: 0.43629851937294006\n",
            "Epoch 3850: train loss: 0.3914032578468323\n",
            "Epoch 3851: train loss: 0.4350549578666687\n",
            "Epoch 3852: train loss: 0.3910708725452423\n",
            "Epoch 3853: train loss: 0.4373758137226105\n",
            "Epoch 3854: train loss: 0.39270928502082825\n",
            "Epoch 3855: train loss: 0.4369061589241028\n",
            "Epoch 3856: train loss: 0.39177119731903076\n",
            "Epoch 3857: train loss: 0.4375487267971039\n",
            "Epoch 3858: train loss: 0.39171257615089417\n",
            "Epoch 3859: train loss: 0.43488752841949463\n",
            "Epoch 3860: train loss: 0.3912147581577301\n",
            "Epoch 3861: train loss: 0.43703171610832214\n",
            "Epoch 3862: train loss: 0.39161354303359985\n",
            "Epoch 3863: train loss: 0.43515321612358093\n",
            "Epoch 3864: train loss: 0.39094361662864685\n",
            "Epoch 3865: train loss: 0.43693119287490845\n",
            "Epoch 3866: train loss: 0.3920173645019531\n",
            "Epoch 3867: train loss: 0.43597841262817383\n",
            "Epoch 3868: train loss: 0.391425222158432\n",
            "Epoch 3869: train loss: 0.43744567036628723\n",
            "Epoch 3870: train loss: 0.39223697781562805\n",
            "Epoch 3871: train loss: 0.4359195828437805\n",
            "Epoch 3872: train loss: 0.39101096987724304\n",
            "Epoch 3873: train loss: 0.43647149205207825\n",
            "Epoch 3874: train loss: 0.3914209306240082\n",
            "Epoch 3875: train loss: 0.4349912405014038\n",
            "Epoch 3876: train loss: 0.3908654451370239\n",
            "Epoch 3877: train loss: 0.43702003359794617\n",
            "Epoch 3878: train loss: 0.3924179971218109\n",
            "Epoch 3879: train loss: 0.43658265471458435\n",
            "Epoch 3880: train loss: 0.39158058166503906\n",
            "Epoch 3881: train loss: 0.43732571601867676\n",
            "Epoch 3882: train loss: 0.3916878402233124\n",
            "Epoch 3883: train loss: 0.4350222051143646\n",
            "Epoch 3884: train loss: 0.3913101553916931\n",
            "Epoch 3885: train loss: 0.43720605969429016\n",
            "Epoch 3886: train loss: 0.3916553854942322\n",
            "Epoch 3887: train loss: 0.4351133108139038\n",
            "Epoch 3888: train loss: 0.391556054353714\n",
            "Epoch 3889: train loss: 0.4376929998397827\n",
            "Epoch 3890: train loss: 0.39195987582206726\n",
            "Epoch 3891: train loss: 0.4352642893791199\n",
            "Epoch 3892: train loss: 0.3911854922771454\n",
            "Epoch 3893: train loss: 0.4368456304073334\n",
            "Epoch 3894: train loss: 0.39137235283851624\n",
            "Epoch 3895: train loss: 0.4347584843635559\n",
            "Epoch 3896: train loss: 0.39139848947525024\n",
            "Epoch 3897: train loss: 0.4376097619533539\n",
            "Epoch 3898: train loss: 0.3921240270137787\n",
            "Epoch 3899: train loss: 0.4356827735900879\n",
            "Epoch 3900: train loss: 0.39065831899642944\n",
            "Epoch 3901: train loss: 0.43603289127349854\n",
            "Epoch 3902: train loss: 0.39128977060317993\n",
            "Epoch 3903: train loss: 0.4350442886352539\n",
            "Epoch 3904: train loss: 0.39100363850593567\n",
            "Epoch 3905: train loss: 0.4372899532318115\n",
            "Epoch 3906: train loss: 0.39244821667671204\n",
            "Epoch 3907: train loss: 0.43655261397361755\n",
            "Epoch 3908: train loss: 0.3913824260234833\n",
            "Epoch 3909: train loss: 0.436903178691864\n",
            "Epoch 3910: train loss: 0.3913523554801941\n",
            "Epoch 3911: train loss: 0.4347149431705475\n",
            "Epoch 3912: train loss: 0.39129307866096497\n",
            "Epoch 3913: train loss: 0.4374557137489319\n",
            "Epoch 3914: train loss: 0.39204832911491394\n",
            "Epoch 3915: train loss: 0.4355725347995758\n",
            "Epoch 3916: train loss: 0.3904944956302643\n",
            "Epoch 3917: train loss: 0.4358072876930237\n",
            "Epoch 3918: train loss: 0.39108604192733765\n",
            "Epoch 3919: train loss: 0.43480855226516724\n",
            "Epoch 3920: train loss: 0.39186644554138184\n",
            "Epoch 3921: train loss: 0.4385973811149597\n",
            "Epoch 3922: train loss: 0.39272576570510864\n",
            "Epoch 3923: train loss: 0.43618106842041016\n",
            "Epoch 3924: train loss: 0.39051440358161926\n",
            "Epoch 3925: train loss: 0.43535929918289185\n",
            "Epoch 3926: train loss: 0.3912166655063629\n",
            "Epoch 3927: train loss: 0.4348326027393341\n",
            "Epoch 3928: train loss: 0.3913988769054413\n",
            "Epoch 3929: train loss: 0.43761083483695984\n",
            "Epoch 3930: train loss: 0.39201831817626953\n",
            "Epoch 3931: train loss: 0.4354928731918335\n",
            "Epoch 3932: train loss: 0.3912443220615387\n",
            "Epoch 3933: train loss: 0.4367726743221283\n",
            "Epoch 3934: train loss: 0.39111974835395813\n",
            "Epoch 3935: train loss: 0.4342755973339081\n",
            "Epoch 3936: train loss: 0.3909355401992798\n",
            "Epoch 3937: train loss: 0.4371618330478668\n",
            "Epoch 3938: train loss: 0.39194944500923157\n",
            "Epoch 3939: train loss: 0.43566927313804626\n",
            "Epoch 3940: train loss: 0.39084693789482117\n",
            "Epoch 3941: train loss: 0.4363170862197876\n",
            "Epoch 3942: train loss: 0.391230046749115\n",
            "Epoch 3943: train loss: 0.43492022156715393\n",
            "Epoch 3944: train loss: 0.3915221691131592\n",
            "Epoch 3945: train loss: 0.43775731325149536\n",
            "Epoch 3946: train loss: 0.3920968770980835\n",
            "Epoch 3947: train loss: 0.43557024002075195\n",
            "Epoch 3948: train loss: 0.3911818265914917\n",
            "Epoch 3949: train loss: 0.436662882566452\n",
            "Epoch 3950: train loss: 0.39098039269447327\n",
            "Epoch 3951: train loss: 0.43407028913497925\n",
            "Epoch 3952: train loss: 0.39073485136032104\n",
            "Epoch 3953: train loss: 0.43688252568244934\n",
            "Epoch 3954: train loss: 0.3918125629425049\n",
            "Epoch 3955: train loss: 0.43558427691459656\n",
            "Epoch 3956: train loss: 0.39089053869247437\n",
            "Epoch 3957: train loss: 0.4365924298763275\n",
            "Epoch 3958: train loss: 0.39147457480430603\n",
            "Epoch 3959: train loss: 0.4351261258125305\n",
            "Epoch 3960: train loss: 0.3915267884731293\n",
            "Epoch 3961: train loss: 0.4377304017543793\n",
            "Epoch 3962: train loss: 0.3919251561164856\n",
            "Epoch 3963: train loss: 0.43523430824279785\n",
            "Epoch 3964: train loss: 0.3909226059913635\n",
            "Epoch 3965: train loss: 0.43628767132759094\n",
            "Epoch 3966: train loss: 0.3908575773239136\n",
            "Epoch 3967: train loss: 0.434047669172287\n",
            "Epoch 3968: train loss: 0.3908136785030365\n",
            "Epoch 3969: train loss: 0.4369875490665436\n",
            "Epoch 3970: train loss: 0.3918074369430542\n",
            "Epoch 3971: train loss: 0.43554261326789856\n",
            "Epoch 3972: train loss: 0.39079317450523376\n",
            "Epoch 3973: train loss: 0.436441570520401\n",
            "Epoch 3974: train loss: 0.3913549780845642\n",
            "Epoch 3975: train loss: 0.43499425053596497\n",
            "Epoch 3976: train loss: 0.39144209027290344\n",
            "Epoch 3977: train loss: 0.4376889169216156\n",
            "Epoch 3978: train loss: 0.3919636011123657\n",
            "Epoch 3979: train loss: 0.43522655963897705\n",
            "Epoch 3980: train loss: 0.3907499313354492\n",
            "Epoch 3981: train loss: 0.4358590245246887\n",
            "Epoch 3982: train loss: 0.39133137464523315\n",
            "Epoch 3983: train loss: 0.434998095035553\n",
            "Epoch 3984: train loss: 0.3911489248275757\n",
            "Epoch 3985: train loss: 0.43694007396698\n",
            "Epoch 3986: train loss: 0.3912040889263153\n",
            "Epoch 3987: train loss: 0.4343399107456207\n",
            "Epoch 3988: train loss: 0.3906365931034088\n",
            "Epoch 3989: train loss: 0.4364376366138458\n",
            "Epoch 3990: train loss: 0.39134636521339417\n",
            "Epoch 3991: train loss: 0.43500617146492004\n",
            "Epoch 3992: train loss: 0.3914669454097748\n",
            "Epoch 3993: train loss: 0.4377728998661041\n",
            "Epoch 3994: train loss: 0.3919321298599243\n",
            "Epoch 3995: train loss: 0.4351555109024048\n",
            "Epoch 3996: train loss: 0.39065855741500854\n",
            "Epoch 3997: train loss: 0.4357585608959198\n",
            "Epoch 3998: train loss: 0.3912937045097351\n",
            "Epoch 3999: train loss: 0.4349757134914398\n",
            "Epoch 4000: train loss: 0.390983521938324\n",
            "Epoch 4001: train loss: 0.43676555156707764\n",
            "Epoch 4002: train loss: 0.3912327289581299\n",
            "Epoch 4003: train loss: 0.43451571464538574\n",
            "Epoch 4004: train loss: 0.3907000422477722\n",
            "Epoch 4005: train loss: 0.43632709980010986\n",
            "Epoch 4006: train loss: 0.39119866490364075\n",
            "Epoch 4007: train loss: 0.43490636348724365\n",
            "Epoch 4008: train loss: 0.39132118225097656\n",
            "Epoch 4009: train loss: 0.43747422099113464\n",
            "Epoch 4010: train loss: 0.3917423486709595\n",
            "Epoch 4011: train loss: 0.43502095341682434\n",
            "Epoch 4012: train loss: 0.39064186811447144\n",
            "Epoch 4013: train loss: 0.43588510155677795\n",
            "Epoch 4014: train loss: 0.3914654850959778\n",
            "Epoch 4015: train loss: 0.4351899325847626\n",
            "Epoch 4016: train loss: 0.39101070165634155\n",
            "Epoch 4017: train loss: 0.4366612136363983\n",
            "Epoch 4018: train loss: 0.39092960953712463\n",
            "Epoch 4019: train loss: 0.4339967370033264\n",
            "Epoch 4020: train loss: 0.39124125242233276\n",
            "Epoch 4021: train loss: 0.4371720552444458\n",
            "Epoch 4022: train loss: 0.39129963517189026\n",
            "Epoch 4023: train loss: 0.4346345365047455\n",
            "Epoch 4024: train loss: 0.3906409442424774\n",
            "Epoch 4025: train loss: 0.4363531172275543\n",
            "Epoch 4026: train loss: 0.39102330803871155\n",
            "Epoch 4027: train loss: 0.434369295835495\n",
            "Epoch 4028: train loss: 0.3907674551010132\n",
            "Epoch 4029: train loss: 0.4365992248058319\n",
            "Epoch 4030: train loss: 0.3913927972316742\n",
            "Epoch 4031: train loss: 0.43512576818466187\n",
            "Epoch 4032: train loss: 0.39128974080085754\n",
            "Epoch 4033: train loss: 0.4372921288013458\n",
            "Epoch 4034: train loss: 0.3914075493812561\n",
            "Epoch 4035: train loss: 0.43449530005455017\n",
            "Epoch 4036: train loss: 0.3902839422225952\n",
            "Epoch 4037: train loss: 0.4355303645133972\n",
            "Epoch 4038: train loss: 0.3913547098636627\n",
            "Epoch 4039: train loss: 0.4353536069393158\n",
            "Epoch 4040: train loss: 0.39140328764915466\n",
            "Epoch 4041: train loss: 0.43733566999435425\n",
            "Epoch 4042: train loss: 0.3911920487880707\n",
            "Epoch 4043: train loss: 0.4341041147708893\n",
            "Epoch 4044: train loss: 0.39081501960754395\n",
            "Epoch 4045: train loss: 0.4363142251968384\n",
            "Epoch 4046: train loss: 0.39081451296806335\n",
            "Epoch 4047: train loss: 0.43413081765174866\n",
            "Epoch 4048: train loss: 0.3904452919960022\n",
            "Epoch 4049: train loss: 0.4363629221916199\n",
            "Epoch 4050: train loss: 0.39127475023269653\n",
            "Epoch 4051: train loss: 0.43486443161964417\n",
            "Epoch 4052: train loss: 0.39100977778434753\n",
            "Epoch 4053: train loss: 0.43678900599479675\n",
            "Epoch 4054: train loss: 0.3912670612335205\n",
            "Epoch 4055: train loss: 0.4347059428691864\n",
            "Epoch 4056: train loss: 0.3906492292881012\n",
            "Epoch 4057: train loss: 0.43637755513191223\n",
            "Epoch 4058: train loss: 0.39097100496292114\n",
            "Epoch 4059: train loss: 0.43427547812461853\n",
            "Epoch 4060: train loss: 0.3906303346157074\n",
            "Epoch 4061: train loss: 0.4363311529159546\n",
            "Epoch 4062: train loss: 0.3910682797431946\n",
            "Epoch 4063: train loss: 0.43471387028694153\n",
            "Epoch 4064: train loss: 0.39098528027534485\n",
            "Epoch 4065: train loss: 0.43705978989601135\n",
            "Epoch 4066: train loss: 0.391473650932312\n",
            "Epoch 4067: train loss: 0.43476006388664246\n",
            "Epoch 4068: train loss: 0.39040401577949524\n",
            "Epoch 4069: train loss: 0.4356568157672882\n",
            "Epoch 4070: train loss: 0.3912805914878845\n",
            "Epoch 4071: train loss: 0.4350305199623108\n",
            "Epoch 4072: train loss: 0.39100465178489685\n",
            "Epoch 4073: train loss: 0.4367060363292694\n",
            "Epoch 4074: train loss: 0.39074286818504333\n",
            "Epoch 4075: train loss: 0.43364378809928894\n",
            "Epoch 4076: train loss: 0.3908034563064575\n",
            "Epoch 4077: train loss: 0.4365987181663513\n",
            "Epoch 4078: train loss: 0.39106079936027527\n",
            "Epoch 4079: train loss: 0.4345302879810333\n",
            "Epoch 4080: train loss: 0.39059820771217346\n",
            "Epoch 4081: train loss: 0.4363938271999359\n",
            "Epoch 4082: train loss: 0.3909667134284973\n",
            "Epoch 4083: train loss: 0.43424922227859497\n",
            "Epoch 4084: train loss: 0.3904564678668976\n",
            "Epoch 4085: train loss: 0.43605703115463257\n",
            "Epoch 4086: train loss: 0.3909660279750824\n",
            "Epoch 4087: train loss: 0.4347931146621704\n",
            "Epoch 4088: train loss: 0.3911955654621124\n",
            "Epoch 4089: train loss: 0.43731334805488586\n",
            "Epoch 4090: train loss: 0.39143136143684387\n",
            "Epoch 4091: train loss: 0.434488981962204\n",
            "Epoch 4092: train loss: 0.3908986449241638\n",
            "Epoch 4093: train loss: 0.4362630546092987\n",
            "Epoch 4094: train loss: 0.39111918210983276\n",
            "Epoch 4095: train loss: 0.4343979060649872\n",
            "Epoch 4096: train loss: 0.39090314507484436\n",
            "Epoch 4097: train loss: 0.43654072284698486\n",
            "Epoch 4098: train loss: 0.391207218170166\n",
            "Epoch 4099: train loss: 0.43406563997268677\n",
            "Epoch 4100: train loss: 0.3903979957103729\n",
            "Epoch 4101: train loss: 0.435542494058609\n",
            "Epoch 4102: train loss: 0.3909298777580261\n",
            "Epoch 4103: train loss: 0.43443727493286133\n",
            "Epoch 4104: train loss: 0.39104339480400085\n",
            "Epoch 4105: train loss: 0.436966210603714\n",
            "Epoch 4106: train loss: 0.39080867171287537\n",
            "Epoch 4107: train loss: 0.4336049258708954\n",
            "Epoch 4108: train loss: 0.39060816168785095\n",
            "Epoch 4109: train loss: 0.4361633360385895\n",
            "Epoch 4110: train loss: 0.39138296246528625\n",
            "Epoch 4111: train loss: 0.43501609563827515\n",
            "Epoch 4112: train loss: 0.39054277539253235\n",
            "Epoch 4113: train loss: 0.43597593903541565\n",
            "Epoch 4114: train loss: 0.3911293148994446\n",
            "Epoch 4115: train loss: 0.4343477785587311\n",
            "Epoch 4116: train loss: 0.39082929491996765\n",
            "Epoch 4117: train loss: 0.43633028864860535\n",
            "Epoch 4118: train loss: 0.39132195711135864\n",
            "Epoch 4119: train loss: 0.43470409512519836\n",
            "Epoch 4120: train loss: 0.39080673456192017\n",
            "Epoch 4121: train loss: 0.43632370233535767\n",
            "Epoch 4122: train loss: 0.39095911383628845\n",
            "Epoch 4123: train loss: 0.4338209331035614\n",
            "Epoch 4124: train loss: 0.39043620228767395\n",
            "Epoch 4125: train loss: 0.43570631742477417\n",
            "Epoch 4126: train loss: 0.39085274934768677\n",
            "Epoch 4127: train loss: 0.43435990810394287\n",
            "Epoch 4128: train loss: 0.39107295870780945\n",
            "Epoch 4129: train loss: 0.43708208203315735\n",
            "Epoch 4130: train loss: 0.3908303380012512\n",
            "Epoch 4131: train loss: 0.4335746169090271\n",
            "Epoch 4132: train loss: 0.39037543535232544\n",
            "Epoch 4133: train loss: 0.4358800947666168\n",
            "Epoch 4134: train loss: 0.3912968635559082\n",
            "Epoch 4135: train loss: 0.43499869108200073\n",
            "Epoch 4136: train loss: 0.3905143737792969\n",
            "Epoch 4137: train loss: 0.4359894096851349\n",
            "Epoch 4138: train loss: 0.3910537362098694\n",
            "Epoch 4139: train loss: 0.43427690863609314\n",
            "Epoch 4140: train loss: 0.3907971978187561\n",
            "Epoch 4141: train loss: 0.4362794756889343\n",
            "Epoch 4142: train loss: 0.3912183344364166\n",
            "Epoch 4143: train loss: 0.4346390664577484\n",
            "Epoch 4144: train loss: 0.3909194767475128\n",
            "Epoch 4145: train loss: 0.4366609752178192\n",
            "Epoch 4146: train loss: 0.39111122488975525\n",
            "Epoch 4147: train loss: 0.4338519871234894\n",
            "Epoch 4148: train loss: 0.39015907049179077\n",
            "Epoch 4149: train loss: 0.4351094663143158\n",
            "Epoch 4150: train loss: 0.39113321900367737\n",
            "Epoch 4151: train loss: 0.43504229187965393\n",
            "Epoch 4152: train loss: 0.39123353362083435\n",
            "Epoch 4153: train loss: 0.436601847410202\n",
            "Epoch 4154: train loss: 0.3908523917198181\n",
            "Epoch 4155: train loss: 0.43360018730163574\n",
            "Epoch 4156: train loss: 0.39002254605293274\n",
            "Epoch 4157: train loss: 0.43540745973587036\n",
            "Epoch 4158: train loss: 0.3914211690425873\n",
            "Epoch 4159: train loss: 0.434977650642395\n",
            "Epoch 4160: train loss: 0.3909222483634949\n",
            "Epoch 4161: train loss: 0.43616369366645813\n",
            "Epoch 4162: train loss: 0.39075762033462524\n",
            "Epoch 4163: train loss: 0.4337919056415558\n",
            "Epoch 4164: train loss: 0.3902340829372406\n",
            "Epoch 4165: train loss: 0.43577006459236145\n",
            "Epoch 4166: train loss: 0.3908911645412445\n",
            "Epoch 4167: train loss: 0.43411678075790405\n",
            "Epoch 4168: train loss: 0.3906968832015991\n",
            "Epoch 4169: train loss: 0.4362436532974243\n",
            "Epoch 4170: train loss: 0.39138495922088623\n",
            "Epoch 4171: train loss: 0.4350641369819641\n",
            "Epoch 4172: train loss: 0.39085909724235535\n",
            "Epoch 4173: train loss: 0.435941606760025\n",
            "Epoch 4174: train loss: 0.39040321111679077\n",
            "Epoch 4175: train loss: 0.4332386553287506\n",
            "Epoch 4176: train loss: 0.39010855555534363\n",
            "Epoch 4177: train loss: 0.43581902980804443\n",
            "Epoch 4178: train loss: 0.3909878730773926\n",
            "Epoch 4179: train loss: 0.4343668818473816\n",
            "Epoch 4180: train loss: 0.3909214735031128\n",
            "Epoch 4181: train loss: 0.4364562928676605\n",
            "Epoch 4182: train loss: 0.3912244439125061\n",
            "Epoch 4183: train loss: 0.43471625447273254\n",
            "Epoch 4184: train loss: 0.3906373083591461\n",
            "Epoch 4185: train loss: 0.4357035160064697\n",
            "Epoch 4186: train loss: 0.39108705520629883\n",
            "Epoch 4187: train loss: 0.43431857228279114\n",
            "Epoch 4188: train loss: 0.3903883695602417\n",
            "Epoch 4189: train loss: 0.435672402381897\n",
            "Epoch 4190: train loss: 0.39103710651397705\n",
            "Epoch 4191: train loss: 0.4340249001979828\n",
            "Epoch 4192: train loss: 0.3901464641094208\n",
            "Epoch 4193: train loss: 0.435324102640152\n",
            "Epoch 4194: train loss: 0.39121100306510925\n",
            "Epoch 4195: train loss: 0.43472862243652344\n",
            "Epoch 4196: train loss: 0.3906872868537903\n",
            "Epoch 4197: train loss: 0.4361426830291748\n",
            "Epoch 4198: train loss: 0.39062023162841797\n",
            "Epoch 4199: train loss: 0.4333421289920807\n",
            "Epoch 4200: train loss: 0.38990455865859985\n",
            "Epoch 4201: train loss: 0.4352158010005951\n",
            "Epoch 4202: train loss: 0.3914231061935425\n",
            "Epoch 4203: train loss: 0.4353550672531128\n",
            "Epoch 4204: train loss: 0.3912631869316101\n",
            "Epoch 4205: train loss: 0.4368152618408203\n",
            "Epoch 4206: train loss: 0.39078283309936523\n",
            "Epoch 4207: train loss: 0.43315061926841736\n",
            "Epoch 4208: train loss: 0.38937076926231384\n",
            "Epoch 4209: train loss: 0.43411654233932495\n",
            "Epoch 4210: train loss: 0.39081165194511414\n",
            "Epoch 4211: train loss: 0.4350510239601135\n",
            "Epoch 4212: train loss: 0.39136114716529846\n",
            "Epoch 4213: train loss: 0.43707549571990967\n",
            "Epoch 4214: train loss: 0.391335129737854\n",
            "Epoch 4215: train loss: 0.4341418743133545\n",
            "Epoch 4216: train loss: 0.38982439041137695\n",
            "Epoch 4217: train loss: 0.4347669184207916\n",
            "Epoch 4218: train loss: 0.39054277539253235\n",
            "Epoch 4219: train loss: 0.43371182680130005\n",
            "Epoch 4220: train loss: 0.39033472537994385\n",
            "Epoch 4221: train loss: 0.4358682632446289\n",
            "Epoch 4222: train loss: 0.3907773196697235\n",
            "Epoch 4223: train loss: 0.43412479758262634\n",
            "Epoch 4224: train loss: 0.3906600773334503\n",
            "Epoch 4225: train loss: 0.4364446699619293\n",
            "Epoch 4226: train loss: 0.3910638391971588\n",
            "Epoch 4227: train loss: 0.4340269863605499\n",
            "Epoch 4228: train loss: 0.3902066648006439\n",
            "Epoch 4229: train loss: 0.4353635013103485\n",
            "Epoch 4230: train loss: 0.3909740746021271\n",
            "Epoch 4231: train loss: 0.43433067202568054\n",
            "Epoch 4232: train loss: 0.39045557379722595\n",
            "Epoch 4233: train loss: 0.4359082579612732\n",
            "Epoch 4234: train loss: 0.3911482095718384\n",
            "Epoch 4235: train loss: 0.43412476778030396\n",
            "Epoch 4236: train loss: 0.3900519013404846\n",
            "Epoch 4237: train loss: 0.4349508285522461\n",
            "Epoch 4238: train loss: 0.3906344771385193\n",
            "Epoch 4239: train loss: 0.43406859040260315\n",
            "Epoch 4240: train loss: 0.390548437833786\n",
            "Epoch 4241: train loss: 0.436391144990921\n",
            "Epoch 4242: train loss: 0.3909606635570526\n",
            "Epoch 4243: train loss: 0.4338238537311554\n",
            "Epoch 4244: train loss: 0.3899567127227783\n",
            "Epoch 4245: train loss: 0.43492111563682556\n",
            "Epoch 4246: train loss: 0.3908255398273468\n",
            "Epoch 4247: train loss: 0.43465420603752136\n",
            "Epoch 4248: train loss: 0.39080309867858887\n",
            "Epoch 4249: train loss: 0.43606677651405334\n",
            "Epoch 4250: train loss: 0.3911789357662201\n",
            "Epoch 4251: train loss: 0.4343039393424988\n",
            "Epoch 4252: train loss: 0.3900426924228668\n",
            "Epoch 4253: train loss: 0.4350419342517853\n",
            "Epoch 4254: train loss: 0.3904171884059906\n",
            "Epoch 4255: train loss: 0.433295339345932\n",
            "Epoch 4256: train loss: 0.3898247182369232\n",
            "Epoch 4257: train loss: 0.43491023778915405\n",
            "Epoch 4258: train loss: 0.3909974694252014\n",
            "Epoch 4259: train loss: 0.4350355863571167\n",
            "Epoch 4260: train loss: 0.3910353481769562\n",
            "Epoch 4261: train loss: 0.4362677037715912\n",
            "Epoch 4262: train loss: 0.39118778705596924\n",
            "Epoch 4263: train loss: 0.43425673246383667\n",
            "Epoch 4264: train loss: 0.3898853063583374\n",
            "Epoch 4265: train loss: 0.43473801016807556\n",
            "Epoch 4266: train loss: 0.390201210975647\n",
            "Epoch 4267: train loss: 0.4330546259880066\n",
            "Epoch 4268: train loss: 0.3897587060928345\n",
            "Epoch 4269: train loss: 0.4348652958869934\n",
            "Epoch 4270: train loss: 0.3910260796546936\n",
            "Epoch 4271: train loss: 0.43515950441360474\n",
            "Epoch 4272: train loss: 0.3911691904067993\n",
            "Epoch 4273: train loss: 0.43643197417259216\n",
            "Epoch 4274: train loss: 0.39121606945991516\n",
            "Epoch 4275: train loss: 0.4341781735420227\n",
            "Epoch 4276: train loss: 0.3896044194698334\n",
            "Epoch 4277: train loss: 0.4342207610607147\n",
            "Epoch 4278: train loss: 0.3899135887622833\n",
            "Epoch 4279: train loss: 0.4328751564025879\n",
            "Epoch 4280: train loss: 0.38983529806137085\n",
            "Epoch 4281: train loss: 0.4351838529109955\n",
            "Epoch 4282: train loss: 0.3914281725883484\n",
            "Epoch 4283: train loss: 0.435689240694046\n",
            "Epoch 4284: train loss: 0.3913912773132324\n",
            "Epoch 4285: train loss: 0.4364681839942932\n",
            "Epoch 4286: train loss: 0.39083409309387207\n",
            "Epoch 4287: train loss: 0.4333919882774353\n",
            "Epoch 4288: train loss: 0.38914236426353455\n",
            "Epoch 4289: train loss: 0.4337376058101654\n",
            "Epoch 4290: train loss: 0.3898257911205292\n",
            "Epoch 4291: train loss: 0.43304726481437683\n",
            "Epoch 4292: train loss: 0.3901161849498749\n",
            "Epoch 4293: train loss: 0.4357302486896515\n",
            "Epoch 4294: train loss: 0.3919185400009155\n",
            "Epoch 4295: train loss: 0.43619072437286377\n",
            "Epoch 4296: train loss: 0.391320139169693\n",
            "Epoch 4297: train loss: 0.43601110577583313\n",
            "Epoch 4298: train loss: 0.39028963446617126\n",
            "Epoch 4299: train loss: 0.4326097369194031\n",
            "Epoch 4300: train loss: 0.38858848810195923\n",
            "Epoch 4301: train loss: 0.43300530314445496\n",
            "Epoch 4302: train loss: 0.3898531198501587\n",
            "Epoch 4303: train loss: 0.4335625171661377\n",
            "Epoch 4304: train loss: 0.3907691538333893\n",
            "Epoch 4305: train loss: 0.4372848570346832\n",
            "Epoch 4306: train loss: 0.3919186294078827\n",
            "Epoch 4307: train loss: 0.43520891666412354\n",
            "Epoch 4308: train loss: 0.390255331993103\n",
            "Epoch 4309: train loss: 0.4347256124019623\n",
            "Epoch 4310: train loss: 0.38992032408714294\n",
            "Epoch 4311: train loss: 0.4328545928001404\n",
            "Epoch 4312: train loss: 0.38946259021759033\n",
            "Epoch 4313: train loss: 0.4346749186515808\n",
            "Epoch 4314: train loss: 0.3907428979873657\n",
            "Epoch 4315: train loss: 0.4342728853225708\n",
            "Epoch 4316: train loss: 0.39053279161453247\n",
            "Epoch 4317: train loss: 0.43586400151252747\n",
            "Epoch 4318: train loss: 0.39113903045654297\n",
            "Epoch 4319: train loss: 0.43463489413261414\n",
            "Epoch 4320: train loss: 0.39004698395729065\n",
            "Epoch 4321: train loss: 0.4346284866333008\n",
            "Epoch 4322: train loss: 0.38994473218917847\n",
            "Epoch 4323: train loss: 0.4329928755760193\n",
            "Epoch 4324: train loss: 0.3896719515323639\n",
            "Epoch 4325: train loss: 0.4351636469364166\n",
            "Epoch 4326: train loss: 0.39110279083251953\n",
            "Epoch 4327: train loss: 0.4345643222332001\n",
            "Epoch 4328: train loss: 0.3902968168258667\n",
            "Epoch 4329: train loss: 0.4352450966835022\n",
            "Epoch 4330: train loss: 0.390686959028244\n",
            "Epoch 4331: train loss: 0.43395277857780457\n",
            "Epoch 4332: train loss: 0.38979870080947876\n",
            "Epoch 4333: train loss: 0.4348095655441284\n",
            "Epoch 4334: train loss: 0.39029163122177124\n",
            "Epoch 4335: train loss: 0.4332740306854248\n",
            "Epoch 4336: train loss: 0.3897041976451874\n",
            "Epoch 4337: train loss: 0.43468961119651794\n",
            "Epoch 4338: train loss: 0.390638530254364\n",
            "Epoch 4339: train loss: 0.43439674377441406\n",
            "Epoch 4340: train loss: 0.3905037045478821\n",
            "Epoch 4341: train loss: 0.43609362840652466\n",
            "Epoch 4342: train loss: 0.39116406440734863\n",
            "Epoch 4343: train loss: 0.43398821353912354\n",
            "Epoch 4344: train loss: 0.38942596316337585\n",
            "Epoch 4345: train loss: 0.4336507022380829\n",
            "Epoch 4346: train loss: 0.3896326720714569\n",
            "Epoch 4347: train loss: 0.433022141456604\n",
            "Epoch 4348: train loss: 0.39001351594924927\n",
            "Epoch 4349: train loss: 0.43578803539276123\n",
            "Epoch 4350: train loss: 0.39154455065727234\n",
            "Epoch 4351: train loss: 0.4349767565727234\n",
            "Epoch 4352: train loss: 0.3902169167995453\n",
            "Epoch 4353: train loss: 0.4348495900630951\n",
            "Epoch 4354: train loss: 0.3900487720966339\n",
            "Epoch 4355: train loss: 0.432929128408432\n",
            "Epoch 4356: train loss: 0.3890850841999054\n",
            "Epoch 4357: train loss: 0.4339943826198578\n",
            "Epoch 4358: train loss: 0.3904514014720917\n",
            "Epoch 4359: train loss: 0.4339766800403595\n",
            "Epoch 4360: train loss: 0.3901195228099823\n",
            "Epoch 4361: train loss: 0.4353707730770111\n",
            "Epoch 4362: train loss: 0.39112669229507446\n",
            "Epoch 4363: train loss: 0.43471643328666687\n",
            "Epoch 4364: train loss: 0.3901177942752838\n",
            "Epoch 4365: train loss: 0.4350152611732483\n",
            "Epoch 4366: train loss: 0.39015594124794006\n",
            "Epoch 4367: train loss: 0.43273112177848816\n",
            "Epoch 4368: train loss: 0.38885700702667236\n",
            "Epoch 4369: train loss: 0.43337559700012207\n",
            "Epoch 4370: train loss: 0.3901778757572174\n",
            "Epoch 4371: train loss: 0.43428441882133484\n",
            "Epoch 4372: train loss: 0.3908942639827728\n",
            "Epoch 4373: train loss: 0.43697720766067505\n",
            "Epoch 4374: train loss: 0.3917064666748047\n",
            "Epoch 4375: train loss: 0.4345166087150574\n",
            "Epoch 4376: train loss: 0.3892398178577423\n",
            "Epoch 4377: train loss: 0.43297088146209717\n",
            "Epoch 4378: train loss: 0.38879093527793884\n",
            "Epoch 4379: train loss: 0.4315810799598694\n",
            "Epoch 4380: train loss: 0.38921502232551575\n",
            "Epoch 4381: train loss: 0.4351360499858856\n",
            "Epoch 4382: train loss: 0.3917171061038971\n",
            "Epoch 4383: train loss: 0.43585288524627686\n",
            "Epoch 4384: train loss: 0.39121854305267334\n",
            "Epoch 4385: train loss: 0.43619096279144287\n",
            "Epoch 4386: train loss: 0.39042115211486816\n",
            "Epoch 4387: train loss: 0.43283092975616455\n",
            "Epoch 4388: train loss: 0.38837528228759766\n",
            "Epoch 4389: train loss: 0.4325258731842041\n",
            "Epoch 4390: train loss: 0.389216810464859\n",
            "Epoch 4391: train loss: 0.43233609199523926\n",
            "Epoch 4392: train loss: 0.3897240161895752\n",
            "Epoch 4393: train loss: 0.4356768727302551\n",
            "Epoch 4394: train loss: 0.39205360412597656\n",
            "Epoch 4395: train loss: 0.43618452548980713\n",
            "Epoch 4396: train loss: 0.39085933566093445\n",
            "Epoch 4397: train loss: 0.43556463718414307\n",
            "Epoch 4398: train loss: 0.38991186022758484\n",
            "Epoch 4399: train loss: 0.4319181740283966\n",
            "Epoch 4400: train loss: 0.3878280222415924\n",
            "Epoch 4401: train loss: 0.4321887791156769\n",
            "Epoch 4402: train loss: 0.3895643353462219\n",
            "Epoch 4403: train loss: 0.43323108553886414\n",
            "Epoch 4404: train loss: 0.3903718888759613\n",
            "Epoch 4405: train loss: 0.4363172948360443\n",
            "Epoch 4406: train loss: 0.3921324610710144\n",
            "Epoch 4407: train loss: 0.43607768416404724\n",
            "Epoch 4408: train loss: 0.3904815912246704\n",
            "Epoch 4409: train loss: 0.43491581082344055\n",
            "Epoch 4410: train loss: 0.3893447816371918\n",
            "Epoch 4411: train loss: 0.4311840534210205\n",
            "Epoch 4412: train loss: 0.38759997487068176\n",
            "Epoch 4413: train loss: 0.43194690346717834\n",
            "Epoch 4414: train loss: 0.389880895614624\n",
            "Epoch 4415: train loss: 0.43446892499923706\n",
            "Epoch 4416: train loss: 0.39140504598617554\n",
            "Epoch 4417: train loss: 0.43774130940437317\n",
            "Epoch 4418: train loss: 0.39168742299079895\n",
            "Epoch 4419: train loss: 0.43458089232444763\n",
            "Epoch 4420: train loss: 0.38919898867607117\n",
            "Epoch 4421: train loss: 0.4330807626247406\n",
            "Epoch 4422: train loss: 0.3887542188167572\n",
            "Epoch 4423: train loss: 0.4309714138507843\n",
            "Epoch 4424: train loss: 0.38828128576278687\n",
            "Epoch 4425: train loss: 0.43354788422584534\n",
            "Epoch 4426: train loss: 0.39115405082702637\n",
            "Epoch 4427: train loss: 0.4360496401786804\n",
            "Epoch 4428: train loss: 0.3918716013431549\n",
            "Epoch 4429: train loss: 0.4377796947956085\n",
            "Epoch 4430: train loss: 0.3913654088973999\n",
            "Epoch 4431: train loss: 0.43322446942329407\n",
            "Epoch 4432: train loss: 0.38771647214889526\n",
            "Epoch 4433: train loss: 0.43057677149772644\n",
            "Epoch 4434: train loss: 0.38765478134155273\n",
            "Epoch 4435: train loss: 0.4309443533420563\n",
            "Epoch 4436: train loss: 0.3896770775318146\n",
            "Epoch 4437: train loss: 0.4362088441848755\n",
            "Epoch 4438: train loss: 0.39282092452049255\n",
            "Epoch 4439: train loss: 0.4376133382320404\n",
            "Epoch 4440: train loss: 0.3911842107772827\n",
            "Epoch 4441: train loss: 0.43564486503601074\n",
            "Epoch 4442: train loss: 0.3892439901828766\n",
            "Epoch 4443: train loss: 0.4305992126464844\n",
            "Epoch 4444: train loss: 0.387054443359375\n",
            "Epoch 4445: train loss: 0.4310641288757324\n",
            "Epoch 4446: train loss: 0.3891921043395996\n",
            "Epoch 4447: train loss: 0.4337235987186432\n",
            "Epoch 4448: train loss: 0.39130067825317383\n",
            "Epoch 4449: train loss: 0.4379599392414093\n",
            "Epoch 4450: train loss: 0.3915197551250458\n",
            "Epoch 4451: train loss: 0.4345722496509552\n",
            "Epoch 4452: train loss: 0.3894151449203491\n",
            "Epoch 4453: train loss: 0.43365272879600525\n",
            "Epoch 4454: train loss: 0.3891086280345917\n",
            "Epoch 4455: train loss: 0.4312662184238434\n",
            "Epoch 4456: train loss: 0.3880574703216553\n",
            "Epoch 4457: train loss: 0.4328720271587372\n",
            "Epoch 4458: train loss: 0.39039871096611023\n",
            "Epoch 4459: train loss: 0.4349624812602997\n",
            "Epoch 4460: train loss: 0.3912702798843384\n",
            "Epoch 4461: train loss: 0.4373592436313629\n",
            "Epoch 4462: train loss: 0.39172738790512085\n",
            "Epoch 4463: train loss: 0.4341377317905426\n",
            "Epoch 4464: train loss: 0.3882317841053009\n",
            "Epoch 4465: train loss: 0.4312683939933777\n",
            "Epoch 4466: train loss: 0.387745201587677\n",
            "Epoch 4467: train loss: 0.43041205406188965\n",
            "Epoch 4468: train loss: 0.3886078894138336\n",
            "Epoch 4469: train loss: 0.43484923243522644\n",
            "Epoch 4470: train loss: 0.39213329553604126\n",
            "Epoch 4471: train loss: 0.43686243891716003\n",
            "Epoch 4472: train loss: 0.3918292224407196\n",
            "Epoch 4473: train loss: 0.4368148446083069\n",
            "Epoch 4474: train loss: 0.3901413083076477\n",
            "Epoch 4475: train loss: 0.4317149817943573\n",
            "Epoch 4476: train loss: 0.3868146240711212\n",
            "Epoch 4477: train loss: 0.43027564883232117\n",
            "Epoch 4478: train loss: 0.38814467191696167\n",
            "Epoch 4479: train loss: 0.43158188462257385\n",
            "Epoch 4480: train loss: 0.3899974524974823\n",
            "Epoch 4481: train loss: 0.4370117783546448\n",
            "Epoch 4482: train loss: 0.3925957679748535\n",
            "Epoch 4483: train loss: 0.4366055727005005\n",
            "Epoch 4484: train loss: 0.3907715976238251\n",
            "Epoch 4485: train loss: 0.4350428283214569\n",
            "Epoch 4486: train loss: 0.38910311460494995\n",
            "Epoch 4487: train loss: 0.4305502772331238\n",
            "Epoch 4488: train loss: 0.3868691027164459\n",
            "Epoch 4489: train loss: 0.4311607778072357\n",
            "Epoch 4490: train loss: 0.38911929726600647\n",
            "Epoch 4491: train loss: 0.43318799138069153\n",
            "Epoch 4492: train loss: 0.39084872603416443\n",
            "Epoch 4493: train loss: 0.43757468461990356\n",
            "Epoch 4494: train loss: 0.3921530842781067\n",
            "Epoch 4495: train loss: 0.43530166149139404\n",
            "Epoch 4496: train loss: 0.38940808176994324\n",
            "Epoch 4497: train loss: 0.43293920159339905\n",
            "Epoch 4498: train loss: 0.3882010281085968\n",
            "Epoch 4499: train loss: 0.43025365471839905\n",
            "Epoch 4500: train loss: 0.38763850927352905\n",
            "Epoch 4501: train loss: 0.43294963240623474\n",
            "Epoch 4502: train loss: 0.39067378640174866\n",
            "Epoch 4503: train loss: 0.43519464135169983\n",
            "Epoch 4504: train loss: 0.39142078161239624\n",
            "Epoch 4505: train loss: 0.43729445338249207\n",
            "Epoch 4506: train loss: 0.39137160778045654\n",
            "Epoch 4507: train loss: 0.4338730573654175\n",
            "Epoch 4508: train loss: 0.38823848962783813\n",
            "Epoch 4509: train loss: 0.43143200874328613\n",
            "Epoch 4510: train loss: 0.38756659626960754\n",
            "Epoch 4511: train loss: 0.42991819977760315\n",
            "Epoch 4512: train loss: 0.3881819546222687\n",
            "Epoch 4513: train loss: 0.43443262577056885\n",
            "Epoch 4514: train loss: 0.3918851315975189\n",
            "Epoch 4515: train loss: 0.43669381737709045\n",
            "Epoch 4516: train loss: 0.3918635845184326\n",
            "Epoch 4517: train loss: 0.43695351481437683\n",
            "Epoch 4518: train loss: 0.3903210461139679\n",
            "Epoch 4519: train loss: 0.43197888135910034\n",
            "Epoch 4520: train loss: 0.3867599368095398\n",
            "Epoch 4521: train loss: 0.42999330163002014\n",
            "Epoch 4522: train loss: 0.3876522183418274\n",
            "Epoch 4523: train loss: 0.43083998560905457\n",
            "Epoch 4524: train loss: 0.38945290446281433\n",
            "Epoch 4525: train loss: 0.4361085593700409\n",
            "Epoch 4526: train loss: 0.39283472299575806\n",
            "Epoch 4527: train loss: 0.43764039874076843\n",
            "Epoch 4528: train loss: 0.39172685146331787\n",
            "Epoch 4529: train loss: 0.43609529733657837\n",
            "Epoch 4530: train loss: 0.38879069685935974\n",
            "Epoch 4531: train loss: 0.42919921875\n",
            "Epoch 4532: train loss: 0.38560372591018677\n",
            "Epoch 4533: train loss: 0.4289935529232025\n",
            "Epoch 4534: train loss: 0.38796868920326233\n",
            "Epoch 4535: train loss: 0.43268516659736633\n",
            "Epoch 4536: train loss: 0.3915686309337616\n",
            "Epoch 4537: train loss: 0.4392808675765991\n",
            "Epoch 4538: train loss: 0.39265236258506775\n",
            "Epoch 4539: train loss: 0.43567004799842834\n",
            "Epoch 4540: train loss: 0.38917863368988037\n",
            "Epoch 4541: train loss: 0.4323101043701172\n",
            "Epoch 4542: train loss: 0.38754627108573914\n",
            "Epoch 4543: train loss: 0.4292936623096466\n",
            "Epoch 4544: train loss: 0.3872722387313843\n",
            "Epoch 4545: train loss: 0.43295496702194214\n",
            "Epoch 4546: train loss: 0.39097166061401367\n",
            "Epoch 4547: train loss: 0.4356657564640045\n",
            "Epoch 4548: train loss: 0.39162033796310425\n",
            "Epoch 4549: train loss: 0.4372597932815552\n",
            "Epoch 4550: train loss: 0.39111462235450745\n",
            "Epoch 4551: train loss: 0.433363676071167\n",
            "Epoch 4552: train loss: 0.38750913739204407\n",
            "Epoch 4553: train loss: 0.4304831922054291\n",
            "Epoch 4554: train loss: 0.3871946334838867\n",
            "Epoch 4555: train loss: 0.429577112197876\n",
            "Epoch 4556: train loss: 0.3881176710128784\n",
            "Epoch 4557: train loss: 0.434282124042511\n",
            "Epoch 4558: train loss: 0.39205124974250793\n",
            "Epoch 4559: train loss: 0.43742889165878296\n",
            "Epoch 4560: train loss: 0.3907618224620819\n",
            "Epoch 4561: train loss: 0.43539249897003174\n",
            "Epoch 4562: train loss: 0.389700710773468\n",
            "Epoch 4563: train loss: 0.4313438832759857\n",
            "Epoch 4564: train loss: 0.3869743049144745\n",
            "Epoch 4565: train loss: 0.4305407702922821\n",
            "Epoch 4566: train loss: 0.3883585035800934\n",
            "Epoch 4567: train loss: 0.4323953092098236\n",
            "Epoch 4568: train loss: 0.39018821716308594\n",
            "Epoch 4569: train loss: 0.436474472284317\n",
            "Epoch 4570: train loss: 0.39223718643188477\n",
            "Epoch 4571: train loss: 0.4360875189304352\n",
            "Epoch 4572: train loss: 0.38988229632377625\n",
            "Epoch 4573: train loss: 0.4334696829319\n",
            "Epoch 4574: train loss: 0.3877923786640167\n",
            "Epoch 4575: train loss: 0.42887622117996216\n",
            "Epoch 4576: train loss: 0.3863561153411865\n",
            "Epoch 4577: train loss: 0.4309525191783905\n",
            "Epoch 4578: train loss: 0.38952022790908813\n",
            "Epoch 4579: train loss: 0.43462997674942017\n",
            "Epoch 4580: train loss: 0.3922155499458313\n",
            "Epoch 4581: train loss: 0.4391493499279022\n",
            "Epoch 4582: train loss: 0.39085614681243896\n",
            "Epoch 4583: train loss: 0.4325186312198639\n",
            "Epoch 4584: train loss: 0.387402206659317\n",
            "Epoch 4585: train loss: 0.430476576089859\n",
            "Epoch 4586: train loss: 0.38712185621261597\n",
            "Epoch 4587: train loss: 0.4300449788570404\n",
            "Epoch 4588: train loss: 0.3889044523239136\n",
            "Epoch 4589: train loss: 0.43552204966545105\n",
            "Epoch 4590: train loss: 0.392205148935318\n",
            "Epoch 4591: train loss: 0.43672287464141846\n",
            "Epoch 4592: train loss: 0.3913709819316864\n",
            "Epoch 4593: train loss: 0.43573299050331116\n",
            "Epoch 4594: train loss: 0.388645738363266\n",
            "Epoch 4595: train loss: 0.42948299646377563\n",
            "Epoch 4596: train loss: 0.3860347270965576\n",
            "Epoch 4597: train loss: 0.4298561215400696\n",
            "Epoch 4598: train loss: 0.3878861665725708\n",
            "Epoch 4599: train loss: 0.43179237842559814\n",
            "Epoch 4600: train loss: 0.39052480459213257\n",
            "Epoch 4601: train loss: 0.43750590085983276\n",
            "Epoch 4602: train loss: 0.3914572596549988\n",
            "Epoch 4603: train loss: 0.4350264370441437\n",
            "Epoch 4604: train loss: 0.38985076546669006\n",
            "Epoch 4605: train loss: 0.4341028034687042\n",
            "Epoch 4606: train loss: 0.38853219151496887\n",
            "Epoch 4607: train loss: 0.4300078749656677\n",
            "Epoch 4608: train loss: 0.3868228793144226\n",
            "Epoch 4609: train loss: 0.4310627579689026\n",
            "Epoch 4610: train loss: 0.3889414370059967\n",
            "Epoch 4611: train loss: 0.43341606855392456\n",
            "Epoch 4612: train loss: 0.39088863134384155\n",
            "Epoch 4613: train loss: 0.43715032935142517\n",
            "Epoch 4614: train loss: 0.3919976055622101\n",
            "Epoch 4615: train loss: 0.43513575196266174\n",
            "Epoch 4616: train loss: 0.3888189196586609\n",
            "Epoch 4617: train loss: 0.4319189488887787\n",
            "Epoch 4618: train loss: 0.3868520259857178\n",
            "Epoch 4619: train loss: 0.4281381666660309\n",
            "Epoch 4620: train loss: 0.3866835832595825\n",
            "Epoch 4621: train loss: 0.43195638060569763\n",
            "Epoch 4622: train loss: 0.39035436511039734\n",
            "Epoch 4623: train loss: 0.4356212317943573\n",
            "Epoch 4624: train loss: 0.39240139722824097\n",
            "Epoch 4625: train loss: 0.43875652551651\n",
            "Epoch 4626: train loss: 0.3899884819984436\n",
            "Epoch 4627: train loss: 0.43085238337516785\n",
            "Epoch 4628: train loss: 0.38624611496925354\n",
            "Epoch 4629: train loss: 0.42924368381500244\n",
            "Epoch 4630: train loss: 0.3870125710964203\n",
            "Epoch 4631: train loss: 0.43050146102905273\n",
            "Epoch 4632: train loss: 0.38974887132644653\n",
            "Epoch 4633: train loss: 0.43710950016975403\n",
            "Epoch 4634: train loss: 0.3932626247406006\n",
            "Epoch 4635: train loss: 0.4374775290489197\n",
            "Epoch 4636: train loss: 0.3907187581062317\n",
            "Epoch 4637: train loss: 0.4338228702545166\n",
            "Epoch 4638: train loss: 0.38704273104667664\n",
            "Epoch 4639: train loss: 0.4275268614292145\n",
            "Epoch 4640: train loss: 0.3858587145805359\n",
            "Epoch 4641: train loss: 0.4303846061229706\n",
            "Epoch 4642: train loss: 0.3888948857784271\n",
            "Epoch 4643: train loss: 0.43335840106010437\n",
            "Epoch 4644: train loss: 0.3909367322921753\n",
            "Epoch 4645: train loss: 0.43742114305496216\n",
            "Epoch 4646: train loss: 0.39244261384010315\n",
            "Epoch 4647: train loss: 0.4357636272907257\n",
            "Epoch 4648: train loss: 0.3888225555419922\n",
            "Epoch 4649: train loss: 0.43156081438064575\n",
            "Epoch 4650: train loss: 0.3864172697067261\n",
            "Epoch 4651: train loss: 0.4273388981819153\n",
            "Epoch 4652: train loss: 0.38672035932540894\n",
            "Epoch 4653: train loss: 0.4322226643562317\n",
            "Epoch 4654: train loss: 0.39059117436408997\n",
            "Epoch 4655: train loss: 0.43573394417762756\n",
            "Epoch 4656: train loss: 0.39197200536727905\n",
            "Epoch 4657: train loss: 0.43796080350875854\n",
            "Epoch 4658: train loss: 0.3912087678909302\n",
            "Epoch 4659: train loss: 0.4326918125152588\n",
            "Epoch 4660: train loss: 0.38623711466789246\n",
            "Epoch 4661: train loss: 0.4282242953777313\n",
            "Epoch 4662: train loss: 0.38651931285858154\n",
            "Epoch 4663: train loss: 0.42952290177345276\n",
            "Epoch 4664: train loss: 0.3885485529899597\n",
            "Epoch 4665: train loss: 0.4355001151561737\n",
            "Epoch 4666: train loss: 0.39251765608787537\n",
            "Epoch 4667: train loss: 0.4372510313987732\n",
            "Epoch 4668: train loss: 0.3914075791835785\n",
            "Epoch 4669: train loss: 0.4354684352874756\n",
            "Epoch 4670: train loss: 0.3883005678653717\n",
            "Epoch 4671: train loss: 0.4289347529411316\n",
            "Epoch 4672: train loss: 0.38608667254447937\n",
            "Epoch 4673: train loss: 0.4298466145992279\n",
            "Epoch 4674: train loss: 0.3876568078994751\n",
            "Epoch 4675: train loss: 0.4312921464443207\n",
            "Epoch 4676: train loss: 0.3897762894630432\n",
            "Epoch 4677: train loss: 0.4367120862007141\n",
            "Epoch 4678: train loss: 0.3926010727882385\n",
            "Epoch 4679: train loss: 0.436524361371994\n",
            "Epoch 4680: train loss: 0.3899978697299957\n",
            "Epoch 4681: train loss: 0.4331890940666199\n",
            "Epoch 4682: train loss: 0.3870696723461151\n",
            "Epoch 4683: train loss: 0.4279421269893646\n",
            "Epoch 4684: train loss: 0.3863600492477417\n",
            "Epoch 4685: train loss: 0.43126559257507324\n",
            "Epoch 4686: train loss: 0.3892461657524109\n",
            "Epoch 4687: train loss: 0.4335329830646515\n",
            "Epoch 4688: train loss: 0.39078524708747864\n",
            "Epoch 4689: train loss: 0.43682965636253357\n",
            "Epoch 4690: train loss: 0.3913995325565338\n",
            "Epoch 4691: train loss: 0.4343340992927551\n",
            "Epoch 4692: train loss: 0.38827890157699585\n",
            "Epoch 4693: train loss: 0.43131378293037415\n",
            "Epoch 4694: train loss: 0.386557012796402\n",
            "Epoch 4695: train loss: 0.4280878007411957\n",
            "Epoch 4696: train loss: 0.38687121868133545\n",
            "Epoch 4697: train loss: 0.4324920177459717\n",
            "Epoch 4698: train loss: 0.3905178904533386\n",
            "Epoch 4699: train loss: 0.4354929029941559\n",
            "Epoch 4700: train loss: 0.39199307560920715\n",
            "Epoch 4701: train loss: 0.4380183219909668\n",
            "Epoch 4702: train loss: 0.39078134298324585\n",
            "Epoch 4703: train loss: 0.43210938572883606\n",
            "Epoch 4704: train loss: 0.386129766702652\n",
            "Epoch 4705: train loss: 0.42810365557670593\n",
            "Epoch 4706: train loss: 0.38623693585395813\n",
            "Epoch 4707: train loss: 0.42933571338653564\n",
            "Epoch 4708: train loss: 0.3884795606136322\n",
            "Epoch 4709: train loss: 0.4349454641342163\n",
            "Epoch 4710: train loss: 0.3920149803161621\n",
            "Epoch 4711: train loss: 0.43690186738967896\n",
            "Epoch 4712: train loss: 0.3915809690952301\n",
            "Epoch 4713: train loss: 0.4361198842525482\n",
            "Epoch 4714: train loss: 0.38839632272720337\n",
            "Epoch 4715: train loss: 0.42846083641052246\n",
            "Epoch 4716: train loss: 0.3854532837867737\n",
            "Epoch 4717: train loss: 0.42861154675483704\n",
            "Epoch 4718: train loss: 0.38690832257270813\n",
            "Epoch 4719: train loss: 0.4308640658855438\n",
            "Epoch 4720: train loss: 0.3900734782218933\n",
            "Epoch 4721: train loss: 0.43760839104652405\n",
            "Epoch 4722: train loss: 0.3931727409362793\n",
            "Epoch 4723: train loss: 0.43709179759025574\n",
            "Epoch 4724: train loss: 0.39011791348457336\n",
            "Epoch 4725: train loss: 0.43264859914779663\n",
            "Epoch 4726: train loss: 0.38583892583847046\n",
            "Epoch 4727: train loss: 0.4260062277317047\n",
            "Epoch 4728: train loss: 0.3854992091655731\n",
            "Epoch 4729: train loss: 0.43055203557014465\n",
            "Epoch 4730: train loss: 0.38932251930236816\n",
            "Epoch 4731: train loss: 0.4344150722026825\n",
            "Epoch 4732: train loss: 0.3919266164302826\n",
            "Epoch 4733: train loss: 0.4386659264564514\n",
            "Epoch 4734: train loss: 0.39202091097831726\n",
            "Epoch 4735: train loss: 0.4340532124042511\n",
            "Epoch 4736: train loss: 0.3870205879211426\n",
            "Epoch 4737: train loss: 0.4286769926548004\n",
            "Epoch 4738: train loss: 0.38572049140930176\n",
            "Epoch 4739: train loss: 0.4277859628200531\n",
            "Epoch 4740: train loss: 0.38783904910087585\n",
            "Epoch 4741: train loss: 0.4341350197792053\n",
            "Epoch 4742: train loss: 0.3911990821361542\n",
            "Epoch 4743: train loss: 0.435768723487854\n",
            "Epoch 4744: train loss: 0.39110904932022095\n",
            "Epoch 4745: train loss: 0.43595021963119507\n",
            "Epoch 4746: train loss: 0.3888742923736572\n",
            "Epoch 4747: train loss: 0.42958101630210876\n",
            "Epoch 4748: train loss: 0.3854656517505646\n",
            "Epoch 4749: train loss: 0.42842134833335876\n",
            "Epoch 4750: train loss: 0.3867366909980774\n",
            "Epoch 4751: train loss: 0.43078911304473877\n",
            "Epoch 4752: train loss: 0.3899754285812378\n",
            "Epoch 4753: train loss: 0.4371071457862854\n",
            "Epoch 4754: train loss: 0.3928292393684387\n",
            "Epoch 4755: train loss: 0.43709221482276917\n",
            "Epoch 4756: train loss: 0.39047130942344666\n",
            "Epoch 4757: train loss: 0.43358680605888367\n",
            "Epoch 4758: train loss: 0.3862575888633728\n",
            "Epoch 4759: train loss: 0.42590081691741943\n",
            "Epoch 4760: train loss: 0.3848375678062439\n",
            "Epoch 4761: train loss: 0.4291257858276367\n",
            "Epoch 4762: train loss: 0.3882647454738617\n",
            "Epoch 4763: train loss: 0.43353453278541565\n",
            "Epoch 4764: train loss: 0.39179518818855286\n",
            "Epoch 4765: train loss: 0.43886011838912964\n",
            "Epoch 4766: train loss: 0.3910273313522339\n",
            "Epoch 4767: train loss: 0.4332267642021179\n",
            "Epoch 4768: train loss: 0.3874952495098114\n",
            "Epoch 4769: train loss: 0.4304526150226593\n",
            "Epoch 4770: train loss: 0.3862721920013428\n",
            "Epoch 4771: train loss: 0.4281492531299591\n",
            "Epoch 4772: train loss: 0.3871622681617737\n",
            "Epoch 4773: train loss: 0.43306681513786316\n",
            "Epoch 4774: train loss: 0.39072269201278687\n",
            "Epoch 4775: train loss: 0.43550294637680054\n",
            "Epoch 4776: train loss: 0.39158356189727783\n",
            "Epoch 4777: train loss: 0.43692392110824585\n",
            "Epoch 4778: train loss: 0.3895820081233978\n",
            "Epoch 4779: train loss: 0.4302428364753723\n",
            "Epoch 4780: train loss: 0.38534054160118103\n",
            "Epoch 4781: train loss: 0.4276467263698578\n",
            "Epoch 4782: train loss: 0.38648590445518494\n",
            "Epoch 4783: train loss: 0.4301711320877075\n",
            "Epoch 4784: train loss: 0.3892328441143036\n",
            "Epoch 4785: train loss: 0.43598607182502747\n",
            "Epoch 4786: train loss: 0.3921894133090973\n",
            "Epoch 4787: train loss: 0.43645617365837097\n",
            "Epoch 4788: train loss: 0.3904452323913574\n",
            "Epoch 4789: train loss: 0.43404167890548706\n",
            "Epoch 4790: train loss: 0.3869131803512573\n",
            "Epoch 4791: train loss: 0.4268711805343628\n",
            "Epoch 4792: train loss: 0.38501429557800293\n",
            "Epoch 4793: train loss: 0.4290526211261749\n",
            "Epoch 4794: train loss: 0.38802531361579895\n",
            "Epoch 4795: train loss: 0.4327746033668518\n",
            "Epoch 4796: train loss: 0.39068660140037537\n",
            "Epoch 4797: train loss: 0.4375132918357849\n",
            "Epoch 4798: train loss: 0.39248326420783997\n",
            "Epoch 4799: train loss: 0.4358263313770294\n",
            "Epoch 4800: train loss: 0.3884030282497406\n",
            "Epoch 4801: train loss: 0.4305943548679352\n",
            "Epoch 4802: train loss: 0.38540118932724\n",
            "Epoch 4803: train loss: 0.42595499753952026\n",
            "Epoch 4804: train loss: 0.3856985569000244\n",
            "Epoch 4805: train loss: 0.4314814507961273\n",
            "Epoch 4806: train loss: 0.39032503962516785\n",
            "Epoch 4807: train loss: 0.43545806407928467\n",
            "Epoch 4808: train loss: 0.3917410671710968\n",
            "Epoch 4809: train loss: 0.4373572766780853\n",
            "Epoch 4810: train loss: 0.3906519114971161\n",
            "Epoch 4811: train loss: 0.43223896622657776\n",
            "Epoch 4812: train loss: 0.3857898414134979\n",
            "Epoch 4813: train loss: 0.4277840554714203\n",
            "Epoch 4814: train loss: 0.38592734932899475\n",
            "Epoch 4815: train loss: 0.42833849787712097\n",
            "Epoch 4816: train loss: 0.3873588740825653\n",
            "Epoch 4817: train loss: 0.4335804283618927\n",
            "Epoch 4818: train loss: 0.3914712369441986\n",
            "Epoch 4819: train loss: 0.4365270137786865\n",
            "Epoch 4820: train loss: 0.3913186192512512\n",
            "Epoch 4821: train loss: 0.4361027777194977\n",
            "Epoch 4822: train loss: 0.3888305127620697\n",
            "Epoch 4823: train loss: 0.42912834882736206\n",
            "Epoch 4824: train loss: 0.3851741850376129\n",
            "Epoch 4825: train loss: 0.4278583228588104\n",
            "Epoch 4826: train loss: 0.3869999647140503\n",
            "Epoch 4827: train loss: 0.43083295226097107\n",
            "Epoch 4828: train loss: 0.3889639675617218\n",
            "Epoch 4829: train loss: 0.43534332513809204\n",
            "Epoch 4830: train loss: 0.391752153635025\n",
            "Epoch 4831: train loss: 0.4358484745025635\n",
            "Epoch 4832: train loss: 0.3897550106048584\n",
            "Epoch 4833: train loss: 0.43343600630760193\n",
            "Epoch 4834: train loss: 0.38706210255622864\n",
            "Epoch 4835: train loss: 0.42741724848747253\n",
            "Epoch 4836: train loss: 0.38538214564323425\n",
            "Epoch 4837: train loss: 0.4294688105583191\n",
            "Epoch 4838: train loss: 0.38803353905677795\n",
            "Epoch 4839: train loss: 0.4324429929256439\n",
            "Epoch 4840: train loss: 0.3901814818382263\n",
            "Epoch 4841: train loss: 0.4365270137786865\n",
            "Epoch 4842: train loss: 0.39166706800460815\n",
            "Epoch 4843: train loss: 0.43482908606529236\n",
            "Epoch 4844: train loss: 0.38810741901397705\n",
            "Epoch 4845: train loss: 0.43068554997444153\n",
            "Epoch 4846: train loss: 0.38576704263687134\n",
            "Epoch 4847: train loss: 0.4267347753047943\n",
            "Epoch 4848: train loss: 0.38619837164878845\n",
            "Epoch 4849: train loss: 0.43197548389434814\n",
            "Epoch 4850: train loss: 0.3902773857116699\n",
            "Epoch 4851: train loss: 0.43488895893096924\n",
            "Epoch 4852: train loss: 0.39079731702804565\n",
            "Epoch 4853: train loss: 0.4359574019908905\n",
            "Epoch 4854: train loss: 0.38975435495376587\n",
            "Epoch 4855: train loss: 0.4313437342643738\n",
            "Epoch 4856: train loss: 0.3856990337371826\n",
            "Epoch 4857: train loss: 0.4282611906528473\n",
            "Epoch 4858: train loss: 0.3868343234062195\n",
            "Epoch 4859: train loss: 0.4300234615802765\n",
            "Epoch 4860: train loss: 0.3881258964538574\n",
            "Epoch 4861: train loss: 0.4342396557331085\n",
            "Epoch 4862: train loss: 0.3910534381866455\n",
            "Epoch 4863: train loss: 0.4348694384098053\n",
            "Epoch 4864: train loss: 0.38944247364997864\n",
            "Epoch 4865: train loss: 0.4333217442035675\n",
            "Epoch 4866: train loss: 0.38771888613700867\n",
            "Epoch 4867: train loss: 0.429023802280426\n",
            "Epoch 4868: train loss: 0.3855515122413635\n",
            "Epoch 4869: train loss: 0.42966774106025696\n",
            "Epoch 4870: train loss: 0.3878393769264221\n",
            "Epoch 4871: train loss: 0.4316159784793854\n",
            "Epoch 4872: train loss: 0.38940316438674927\n",
            "Epoch 4873: train loss: 0.43551644682884216\n",
            "Epoch 4874: train loss: 0.3914070129394531\n",
            "Epoch 4875: train loss: 0.434915691614151\n",
            "Epoch 4876: train loss: 0.3884458541870117\n",
            "Epoch 4877: train loss: 0.4314447045326233\n",
            "Epoch 4878: train loss: 0.38627636432647705\n",
            "Epoch 4879: train loss: 0.42725902795791626\n",
            "Epoch 4880: train loss: 0.3860692083835602\n",
            "Epoch 4881: train loss: 0.4312632977962494\n",
            "Epoch 4882: train loss: 0.3893519937992096\n",
            "Epoch 4883: train loss: 0.43370774388313293\n",
            "Epoch 4884: train loss: 0.3901752829551697\n",
            "Epoch 4885: train loss: 0.43576347827911377\n",
            "Epoch 4886: train loss: 0.39013954997062683\n",
            "Epoch 4887: train loss: 0.4318997859954834\n",
            "Epoch 4888: train loss: 0.3861100971698761\n",
            "Epoch 4889: train loss: 0.42857831716537476\n",
            "Epoch 4890: train loss: 0.3867368996143341\n",
            "Epoch 4891: train loss: 0.42967045307159424\n",
            "Epoch 4892: train loss: 0.3875034749507904\n",
            "Epoch 4893: train loss: 0.433145135641098\n",
            "Epoch 4894: train loss: 0.3903137743473053\n",
            "Epoch 4895: train loss: 0.4343429207801819\n",
            "Epoch 4896: train loss: 0.3897721469402313\n",
            "Epoch 4897: train loss: 0.43444371223449707\n",
            "Epoch 4898: train loss: 0.38854584097862244\n",
            "Epoch 4899: train loss: 0.42964252829551697\n",
            "Epoch 4900: train loss: 0.38532641530036926\n",
            "Epoch 4901: train loss: 0.42844057083129883\n",
            "Epoch 4902: train loss: 0.38751763105392456\n",
            "Epoch 4903: train loss: 0.43155279755592346\n",
            "Epoch 4904: train loss: 0.3891844153404236\n",
            "Epoch 4905: train loss: 0.43519333004951477\n",
            "Epoch 4906: train loss: 0.3909837305545807\n",
            "Epoch 4907: train loss: 0.4341588020324707\n",
            "Epoch 4908: train loss: 0.3881024718284607\n",
            "Epoch 4909: train loss: 0.4314248263835907\n",
            "Epoch 4910: train loss: 0.3864315450191498\n",
            "Epoch 4911: train loss: 0.42748498916625977\n",
            "Epoch 4912: train loss: 0.38615721464157104\n",
            "Epoch 4913: train loss: 0.43110761046409607\n",
            "Epoch 4914: train loss: 0.3890955448150635\n",
            "Epoch 4915: train loss: 0.4335341453552246\n",
            "Epoch 4916: train loss: 0.3900258243083954\n",
            "Epoch 4917: train loss: 0.4354749321937561\n",
            "Epoch 4918: train loss: 0.3899727463722229\n",
            "Epoch 4919: train loss: 0.431946724653244\n",
            "Epoch 4920: train loss: 0.3862331807613373\n",
            "Epoch 4921: train loss: 0.4289829730987549\n",
            "Epoch 4922: train loss: 0.3867799639701843\n",
            "Epoch 4923: train loss: 0.42936941981315613\n",
            "Epoch 4924: train loss: 0.38709452748298645\n",
            "Epoch 4925: train loss: 0.4325689673423767\n",
            "Epoch 4926: train loss: 0.3900608718395233\n",
            "Epoch 4927: train loss: 0.4341181814670563\n",
            "Epoch 4928: train loss: 0.3896823823451996\n",
            "Epoch 4929: train loss: 0.43429896235466003\n",
            "Epoch 4930: train loss: 0.3885663151741028\n",
            "Epoch 4931: train loss: 0.42993277311325073\n",
            "Epoch 4932: train loss: 0.3854467272758484\n",
            "Epoch 4933: train loss: 0.42878100275993347\n",
            "Epoch 4934: train loss: 0.38750120997428894\n",
            "Epoch 4935: train loss: 0.4311569929122925\n",
            "Epoch 4936: train loss: 0.3887841999530792\n",
            "Epoch 4937: train loss: 0.43467751145362854\n",
            "Epoch 4938: train loss: 0.39052242040634155\n",
            "Epoch 4939: train loss: 0.43366461992263794\n",
            "Epoch 4940: train loss: 0.3881492018699646\n",
            "Epoch 4941: train loss: 0.43176451325416565\n",
            "Epoch 4942: train loss: 0.3867759108543396\n",
            "Epoch 4943: train loss: 0.4280069172382355\n",
            "Epoch 4944: train loss: 0.38626769185066223\n",
            "Epoch 4945: train loss: 0.4309637248516083\n",
            "Epoch 4946: train loss: 0.3887083828449249\n",
            "Epoch 4947: train loss: 0.4327273964881897\n",
            "Epoch 4948: train loss: 0.3892827332019806\n",
            "Epoch 4949: train loss: 0.43458089232444763\n",
            "Epoch 4950: train loss: 0.3896082937717438\n",
            "Epoch 4951: train loss: 0.43181583285331726\n",
            "Epoch 4952: train loss: 0.386705219745636\n",
            "Epoch 4953: train loss: 0.42996183037757874\n",
            "Epoch 4954: train loss: 0.3864164352416992\n",
            "Epoch 4955: train loss: 0.4284876883029938\n",
            "Epoch 4956: train loss: 0.38662734627723694\n",
            "Epoch 4957: train loss: 0.4320615231990814\n",
            "Epoch 4958: train loss: 0.3900163471698761\n",
            "Epoch 4959: train loss: 0.4345352053642273\n",
            "Epoch 4960: train loss: 0.3901660442352295\n",
            "Epoch 4961: train loss: 0.4351734519004822\n",
            "Epoch 4962: train loss: 0.38896599411964417\n",
            "Epoch 4963: train loss: 0.4298664629459381\n",
            "Epoch 4964: train loss: 0.3855484127998352\n",
            "Epoch 4965: train loss: 0.4282483756542206\n",
            "Epoch 4966: train loss: 0.38659587502479553\n",
            "Epoch 4967: train loss: 0.42970871925354004\n",
            "Epoch 4968: train loss: 0.38769105076789856\n",
            "Epoch 4969: train loss: 0.4336537718772888\n",
            "Epoch 4970: train loss: 0.3904520869255066\n",
            "Epoch 4971: train loss: 0.43395182490348816\n",
            "Epoch 4972: train loss: 0.38872581720352173\n",
            "Epoch 4973: train loss: 0.43241995573043823\n",
            "Epoch 4974: train loss: 0.38724789023399353\n",
            "Epoch 4975: train loss: 0.42880260944366455\n",
            "Epoch 4976: train loss: 0.3862636983394623\n",
            "Epoch 4977: train loss: 0.43058598041534424\n",
            "Epoch 4978: train loss: 0.3878345191478729\n",
            "Epoch 4979: train loss: 0.43122583627700806\n",
            "Epoch 4980: train loss: 0.38854536414146423\n",
            "Epoch 4981: train loss: 0.43423616886138916\n",
            "Epoch 4982: train loss: 0.389829158782959\n",
            "Epoch 4983: train loss: 0.43236562609672546\n",
            "Epoch 4984: train loss: 0.38732749223709106\n",
            "Epoch 4985: train loss: 0.43055444955825806\n",
            "Epoch 4986: train loss: 0.3863542675971985\n",
            "Epoch 4987: train loss: 0.4284282624721527\n",
            "Epoch 4988: train loss: 0.38723310828208923\n",
            "Epoch 4989: train loss: 0.43298619985580444\n",
            "Epoch 4990: train loss: 0.3895796537399292\n",
            "Epoch 4991: train loss: 0.43295103311538696\n",
            "Epoch 4992: train loss: 0.3885793089866638\n",
            "Epoch 4993: train loss: 0.432705283164978\n",
            "Epoch 4994: train loss: 0.3878198564052582\n",
            "Epoch 4995: train loss: 0.42983853816986084\n",
            "Epoch 4996: train loss: 0.38612276315689087\n",
            "Epoch 4997: train loss: 0.43008115887641907\n",
            "Epoch 4998: train loss: 0.3873296082019806\n",
            "Epoch 4999: train loss: 0.430629700422287\n",
            "Accuracy: 0.7543352601156069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "game1 = x[0]\n",
        "wL = y[0]\n",
        "print(x.shape)\n",
        "print(game1)\n",
        "print(wL)\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/MLPROJECT/.pth')\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/MLPROJECT/best.pth'))\n",
        "oupt = model(torch.FloatTensor(game1))\n",
        "print(oupt.item())\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNlqlzpM6WJ5",
        "outputId": "3ad6941d-7b84-468e-e544-b8a1178e4f1c"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5189, 54)\n",
            "[0.09245 0.3325  0.786   0.081   0.241   0.1785  0.2265  0.093   0.305\n",
            " 0.186   0.079   0.51    0.13    0.229   0.09355 0.326   0.7375  0.0885\n",
            " 0.2535  0.195   0.262   0.0765  0.3175  0.187   0.54    0.2105  0.08305\n",
            " 0.3155  0.7665  0.07    0.221   0.1295  0.179   0.0985  0.306   0.1855\n",
            " 0.057   0.505   0.1175  0.16    0.0882  0.339   0.767   0.21    0.135\n",
            " 0.178   0.09    0.3185  0.192   0.53    0.1675  0.069   0.095   0.094  ]\n",
            "1\n",
            "0.7509796619415283\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (fc1): Linear(in_features=54, out_features=20, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (fc2): Linear(in_features=20, out_features=1, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plotcharts(errors):\n",
        "    errors = np.array(errors)\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    graf02 = plt.subplot(1, 2, 1) # nrows, ncols, index\n",
        "    graf02.set_title('Errors')\n",
        "    plt.plot(errors, '-')\n",
        "    plt.xlabel('Epochs')\n",
        "    graf03 = plt.subplot(1, 2, 2)\n",
        "    graf03.set_title('Tests')\n",
        "    a = plt.plot(test_output.numpy(), 'yo', label='Real')\n",
        "    plt.setp(a, markersize=10)\n",
        "    a = plt.plot(y_pred.detach().numpy().round(), 'b+', label='Predicted')\n",
        "    plt.setp(a, markersize=10)\n",
        "    plt.legend(loc=7)\n",
        "    plt.show()\n",
        "\n",
        "plotcharts(errors)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "eNHaHJFP4IvR",
        "outputId": "6791a460-e0a3-436b-e2a8-4e80c392a450"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "y_pred = model(test_input)\n",
        "after_train = criterion(y_pred.squeeze(), test_output)\n",
        "print('Test loss after training' , after_train.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvakIJMh9gU7",
        "outputId": "c38d5b66-578c-4ba1-c6a8-97d9008f5460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss after training 0.6596518158912659\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QK60s_LeAtw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(model, inp, out):\n",
        "  # ds is a iterable Dataset of Tensors\n",
        "  # one item at a time version\n",
        "  n_correct = 0; n_wrong = 0\n",
        "\n",
        "  for i in range(len(inp)):\n",
        "    inpts = inp\n",
        "    target = test_output   # float32  [0.0] or [1.0]\n",
        "    with torch.no_grad():\n",
        "      oupt = model(inpts)\n",
        "\n",
        "    # avoid 'target == 1.0'\n",
        "    if (target < 0.5)  (oupt = 0.5) and (oupt >= 0.5):\n",
        "      n_correct += 1\n",
        "    else:\n",
        "      n_wrong += 1\n",
        "\n",
        "  return (n_correct * 1.0) / (n_correct + n_wrong)\n",
        "\n",
        "  print(accuracy(model))"
      ],
      "metadata": {
        "id": "YM2Zlp2e-9Rr"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = accuracy(model,test_input,test_output)\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "QlkG3OqiCroy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "18899b33-8d5f-4da2-9a68-7d1dd17581cc"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-121-2b8de85c0faf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-120-07a8cc946334>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(model, inp, out)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# avoid 'target == 1.0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0moupt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moupt\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m       \u001b[0mn_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not callable"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "4d2770c334f3778740193ba4b3686745ae5c2e3ee10c8c0d673798cf1c2fcefe"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('tf': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "Copy of MLALG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}